<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-gb" xml:lang="en-gb" style="overflow-x:hidden;" onscroll = "myScroll()"; onresize="myResize()";>
<head>
    <title>Tilo Burghardt | University of Bristol | Dept of Computer Science</title>
    <base target="_top"></base>
    <meta name="description" content="Tilo Burghardt - Homepage"/>
    <meta name="ROBOTS" content="INDEX, FOLLOW"/>
    <meta name="Keywords" content="tilo burghardt, Tilo Burghardt, tilo, Tilo, burghardt, Burghardt, computer vision, animal biometrics, bristol"/>
    <meta HTTP-EQUIV="Keywords" CONTENT="tilo burghardt, tilo, burghardt, computer vision, animal biometrics, bristol"/>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
    <meta content="Tilo Burghardt, University of Bristol" name="DCTERMS.creator" />
    <meta content="en-GB" name="DCTERMS.language" />
    <meta content="2020-02-20" name="DCTERMS.modified" />
    <meta content="2013-07-15" name="DCTERMS.created" />
    <meta content="text/html" name="DCTERMS.format" />
    <meta content="University of Bristol" name="DCTERMS.publisher" />
    <meta content="http://www.bristol.ac.uk/university/web/terms-conditions.html" name="DCTERMS.rights" />
    <meta name="viewport" content="width=device-width, initial-scale=1;user-scalable=0;maximum-scale=1;">
    <link rel="icon" href="http://www.bristol.ac.uk/favicon.gif" type="image/gif" />
    <link rel="stylesheet" type="text/css" media="screen" href="uobcms_corporate.css" />
</head>


<style>

/* Position the image container (needed to position the left and right arrows) */
.container {
  position: relative;
}

/* Hide the images by default */
.mySlides {
  display: none;
  left: -9px;
}

</style>
<script>
 function goToUni()    { window.open("https://www.bristol.ac.uk"); }
 function getTitle()   { return "CamTrap Ecology meets AI"; }
 function getTeach()   { return "Current Teaching in Computer Science"; }
 function getRes()     { return "Computer Vision for the Life Sciences"; }
 function getTQ()      { return "<br/><br/><b>.</b><hr/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td></td></tr></table>"; }
 function getVision()  { return "<br/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td></td></tr></table>"; }
 function getSphere()  { return "<br/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td></td></tr></table>"; }
 function getAB()      { return "<br/><br/><b>.</b><hr/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td></td></tr></table>"; }
 function getAddress() { return "<u>WEDNESDAY WORKSHOPS</u><br/>(Lightweight Online Talks and Tutorials)<br/>14th, 21st, 28th Sept 2022<br/>3pm-5pm BST | 10:00-12:00 EST<br/>&nbsp;<br/>+44 (0) 117 954 5298<br/>tilo (at) cs.bris.ac.uk"}
 function getSummary() { return "<b>About</b><hr/>This is the homepage of a three day series of talks and tutorials on CameraTrap data and related AI and ecology. The event is run as 2h slots per day online. Attendance is free and the events are lightweight intended to bring together the communities of camera trap data producers (e.g. nature parks and reserves), user groups (e.g. ecologists and conservationists), and AI analysis providers (e.g. computer scientists and engineers)."}
 function getTeaching() { return "<b>"}
 function getResearch() {return "<b>"}
</script>
<body onscroll = "myScroll()"; onresize="myResize()"; style="margin: 0; min-width: 1280px; max-width: 100%; background-color: #FFFFFF; background-size: cover; padding: 0px; overflow-x:hidden; width:100%;">

   <!--NAVIGATION-->
<div class="uob-mainnav-container clearfix" id="top">
  <div id="csnav">
  <ul class="width-master uob-mainnav" role="navigation" id="csmenu" style="width:100%; margin-left:0;">
     <li class="u-m-university"> <a class="u-m-link" href="javascript:currentSlide(1)" tabindex="0"> Home </a></li>
  </ul>
  </div>
</div>

<div class="container">
  <img id="topup" src="top.png" onclick="javascript:goToTop()" style="cursor: pointer; width:32px; height:32px; margin-left:7px; position: absolute">

  <div class="mySlides">
    <div class="myFlip">
      <img src="introsimple.gif" style="width:100%; height:320px; position: absolute">
      <div style="position: absolute">
        <a href="https://research-information.bris.ac.uk/en/persons/tilo-burghardt" z-index: 999; target="_blank" title="Go to University Researcher Profile">
           <img src="Tilo_Burghardt.jpg" style="width:132px; margin-top:92px; margin-left:59px">
        </a>
        <div style="width:90%; margin-left:59px; margin-top:-254px; color: #FFFFFF; font-size: 27pt; font-family: 'Didot', serif;">
          <script>document.write(getTitle());</script>
        </div>
        <div style="width:90%; margin-left:220px; margin-top:16px; color: #FFFFFF; font-size: 12pt; font-family: 'Helvetica', sans-serif;">
          <script>document.write(getAddress());</script>
        </div>
      </div>
    </div>

    <div class="myFlip">
      <img src="introsimple.gif" style="width:100%; height:256px; position: absolute">
      <div style="position: absolute">
         <img src="Tilo_Burghardt.jpg" style="width:124px; margin-top:60px; margin-left:50px">
        <div style="position: absolute; margin-left:50px; margin-top:-219px; color: #FFFFFF; font-size: 16pt; font-family: 'Didot', serif;">
          <script>document.write(getTitle());</script>
        </div>
        <div style="margin-left:194px; margin-top:-176px; color: #FFFFFF; font-size: 11pt; font-family: 'Helvetica', sans-serif;">
          <script>document.write(getAddress());</script>
        </div>
      </div>
    </div>

  </div>
  </div>

  <div class="mySlides">

    <div id="myResearch">
      <img src="introsimple.gif" style="width:100%; height:320px; position: absolute"><img src="intro.gif" style="width:100%; height:320px; position: absolute">
      <div style="position: absolute">
        <img src="research.gif" style="width:132px; margin-top:92px; margin-left:59px">
        <div style="width:70%; margin-left:59px; margin-top:-254px; color: #FFFFFF; font-size: 27pt; font-family: 'Didot', serif;">
          <script>document.write(getRes());</script>
        </div>
        <div id="myResearchHook"style="width:70%; margin-left:209px; margin-top:16px; color: #FFFFFF; font-size: 12pt; font-family: 'Helvetica', sans-serif;">
        </div><div style="margin-top:-53px;">&nbsp;</div>
    </div></div>

    <div id="myResearchSmall">
      <img src="introsimple.gif" style="width:100%; height:256px; position: absolute"><img src="intro.gif" style="width:100%; height:256px; position: absolute">
      <div style="position: absolute">
         <img src="research.gif" style="width:124px; margin-top:60px; margin-left:50px">
        <div style="position: absolute; margin-left:50px; margin-top:-219px; color: #FFFFFF; font-size: 16pt; font-family: 'Didot', serif;">
          <script>document.write(getRes());</script>
        </div>
        <div id="myResearchHookSmall" style="margin-left:194px; margin-top:-176px; color: #FFFFFF; font-size: 11pt; font-family: 'Helvetica', sans-serif;">
        </div><div style="margin-top:-50px;">&nbsp;</div>
    </div></div>

</div>
  <div class="mySlides">

    <div id="myTeaching">
      <img src="introsimple.gif" style="width:100%; height:320px; position: absolute"><img src="intro.gif" style="width:100%; height:320px; position: absolute">
      <div style="position: absolute">
        <img src="award.jpg" style="width:132px; margin-top:92px; margin-left:59px">
        <div style="width:100%; margin-left:59px; margin-top:-254px; color: #FFFFFF; font-size: 27pt; font-family: 'Didot', serif;">
          <script>document.write(getTeach());</script>
        </div>
        <div id="myTeachingHook" style="width:70%; margin-left:209px; margin-top:16px; color: #FFFFFF; font-size: 12pt; font-family: 'Helvetica', sans-serif;">
        </div>
    </div></div>

    <div id="myTeachingSmall">
      <img src="introsimple.gif" style="width:100%; height:256px; position: absolute"><img src="intro.gif" style="width:100%; height:256px; position: absolute">
      <div style="position: absolute">
         <img src="award.jpg" style="width:124px; margin-top:60px; margin-left:50px">
        <div style="position: absolute; margin-left:50px; margin-top:-219px; color: #FFFFFF; font-size: 16pt; font-family: 'Didot', serif;">
          <script>document.write(getTeach());</script>
        </div>
        <div id="myTeachingHookSmall" style="margin-left:194px; margin-top:-176px; color: #FFFFFF; font-size: 11pt; font-family: 'Helvetica', sans-serif;">
        </div><div style="margin-top:-24px;">&nbsp;</div>
    </div></div>

</div>

<div class="myBlock" id="myPC" style="margin-left:49px; margin-top:345px; text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
  <table width="100%" border="0" cellspacing="14">
    <tr>
      <td valign="top" style="background-color: #FFFFFF;">
        <div id="myLeft" width="70%" style="text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
          <script> document.write(getSummary()); </script>
          <a id="team"></a>
          <div id="myTeamHook">
          </div>
          <a id="AB"></a>
          <script> document.write(getAB()); </script>
          <div id="myABProjectsHook">
          </div>
          <a id="sphere"></a>
          <script> document.write(getSphere()); </script>
          <div id="mySPProjectsHook">
          </div>
          <a id="research"></a>
          <script> document.write(getResearch()); </script>
          <a id="teaching"></a>
          <script> document.write(getTQ()); </script>
          <script> document.write(getTeaching()); </script>
          <div id="myFinalHook">
          </div>
        </div>
      </td>
      <td valign="top">
        <div id="myMiddle"><font color="#FFFFFF">.......</font>
        </div>
      </td>
      <td valign="top" style="background-color: #FFFFFF;">
        <div id="myRight" width="30%" style="margin-top:-33px; text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
        </div>
      </td>
    </tr>
  </table>
</div>

<div class="myBlock" id="myMobile" style="margin-left:49px; margin-top:288px; text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
  <script> document.write(getSummary()); </script>
  <a id="teamMobile"></a>
  <div id="myTeamHookSmall">
  </div>
  <a id="ABMobile"></a>
  <script> document.write(getAB()); </script>
  <div id="myABProjectsHookSmall">
  </div>
  <a id="sphereMobile"></a>
  <script> document.write(getSphere()); </script>
  <div id="mySPProjectsHookSmall">
  </div>
  <a id="researchMobile"></a>
  <script> document.write(getResearch()); </script>
  <a id="teachingMobile"></a>
  <script> document.write(getTQ()); </script>
  <script> document.write(getTeaching()); </script>
  <div id="myFinalHookSmall">
  </div>
  <a id="papers"></a>
</div>

<script>
var slideIndex = 1;
var stopRotation = 1;
var firstRun = 1;
showSlides(slideIndex);
function getNWidth() { return window.innerWidth * 0.57 - 160;}
function getScroll() { return (document.documentElement.scrollTop || document.body.scrollTop) - 30;}
function getSMWidth() { return window.innerWidth - 92;}
function getRWidth() { return window.innerWidth - 122;}
function getWidth() { return window.innerWidth - 110;}
function getLWidth() { return window.innerWidth * 0.57 - 160;}
function getReducedWidth() { return (window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth) -110;}

function goToTop() {
  document.documentElement.scrollTop = "0px";
  document.body.scrollTop = "0px";
}

function myScroll() {
    document.getElementById("topup").style.marginTop = getScroll()+"px";
}

function myResize() {
  var width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
  var i;
  var slides = document.getElementsByClassName("myFlip");
  var blocks = document.getElementsByClassName("myBlock");
  for (i = 0; i < slides.length; i++) { slides[i].style.display = "none"; }
  for (i = 0; i < blocks.length; i++) { blocks[i].style.display = "none"; }
  document.getElementById("topicSearch").style.display = "none";

  myScroll();

  if (width<1280) {
    slides[1].style.display = "block";
    blocks[1].style.display = "block";
    document.getElementById("topup").style.marginLeft = "7px";
    document.getElementById("pubstab").style.fontSize = "10pt";
    document.getElementById("linkTeam").href = "#teamMobile";
    document.getElementById("linkAB2").href = "#ABMobile";
    document.getElementById("linkSphere2").href = "#sphereMobile";
    document.getElementById("linkRes").href = "#researchMobile";
    document.getElementById("myResearchSmall").style.display = "block";
    document.getElementById("myResearch").style.display = "none";
    document.getElementById("linkTeach").href = "#teachingMobile";
    document.getElementById("myTeachingSmall").style.display = "block";
    document.getElementById("myTeaching").style.display = "none";
    document.getElementById("menupap").style.display = "block";
    document.getElementById("topicSearch").style.display = "none";
    document.getElementById("myMobile").style.width = getWidth()+"px";
    document.getElementById("myScholarSmall").style.marginLeft = getSMWidth()+"px";
    document.getElementById("myDBLPSmall").style.marginLeft = getSMWidth()+"px";
    document.getElementById("myPWCSmall").style.marginLeft = getSMWidth()+"px";
  }
  else {
    slides[0].style.display = "block";
    blocks[0].style.display = "block";
    document.getElementById("topup").style.marginLeft = "12px";
    document.getElementById("pubstab").style.fontSize = "12pt";
    document.getElementById("linkTeam").href = "#team";
    document.getElementById("linkAB2").href = "#AB";
    document.getElementById("linkSphere2").href = "#sphere";
    document.getElementById("linkRes").href = "#research";
    document.getElementById("myResearchSmall").style.display = "none";
    document.getElementById("topicSearch").style.display = "block";
    document.getElementById("myResearch").style.display = "block";
    document.getElementById("linkTeach").href = "#teaching";
    document.getElementById("myTeachingSmall").style.display = "none";
    document.getElementById("myTeaching").style.display = "block";
    document.getElementById("menupap").style.display = "none";
    document.getElementById("myPC").style.width = getWidth()+"px";
    document.getElementById("myLeft").style.width = getLWidth()+"px";
    document.getElementById("myScholar").style.marginLeft = getRWidth()+"px";
    document.getElementById("myDBLP").style.marginLeft = getRWidth()+"px";
    document.getElementById("myPWC").style.marginLeft = getRWidth()+"px";
  }

  if (firstRun>0) {
    firstRun--;
    var itm = document.getElementById("pubs");
    itm.style.display = "block";
    var cln = itm.cloneNode(true);
    document.getElementById("myMobile").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myRight").appendChild(cln);
    itm.remove();
    itm = document.getElementById("topteach");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myTeachingHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myTeachingHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("topres");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myResearchHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myResearchHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("ABprojects");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myABProjectsHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myABProjectsHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("SPprojects");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("mySPProjectsHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("mySPProjectsHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("Team");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myTeamHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myTeamHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("FinalYear");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myFinalHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myFinalHookSmall").appendChild(cln);
    itm.remove();
  }
}

function currentSlide(n) {
  stopRotation = 1;
  showSlides(slideIndex = n);
}

function showSlides(n) {
  var i;
  var slides = document.getElementsByClassName("mySlides");
  if (n > slides.length) {slideIndex = 1}
  if (n < 1) {slideIndex = slides.length}
  for (i = 0; i < slides.length; i++) {
      slides[i].style.display = "none";
  }
  slides[slideIndex-1].style.display = "block";
  if (!stopRotation) slideIndex++;
  if (slideIndex > slides.length) {slideIndex = 1}
  if (slideIndex == 2) setTimeout(showSlides, 15000);
  else setTimeout(showSlides, 10000);
}
</script>

<div id="topteach" style="display: none; margin-left: 12px;color: #FFFFFF; font-family: 'Helvetica', sans-serif;">
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMS10016" title="Go to COMS10016 Unit Description" target="_blank"><font color="#FFFFFF"><u>COMS10016</u></font></a>: Imp &amp; Func Programming<br />
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMS10017" title="Go to COMS10017 Unit Description" target="_blank"><font color="#FFFFFF"><u>COMS10017</u></font></a>: OOP and Algorithms I<br />
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMS30043" title="Go to COMS30043 Unit Description" target="_blank"><font color="#FFFFFF"><u>COMS30043</u></font></a>: Team Project<br />
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMSM0045" title="Go to COMSM0045 Unit Description"target="_blank"><font color="#FFFFFF"><u>COMSM0045</u></font></a>: Applied Deep Learning<br /><br />
  <a id="linkTeach" href="#teaching"><font color="#FFFFFF"><u>Go to Teaching Profile</u></font></a><br/>
  <a href=""><font color="#FFFFFF"><u>Go to Previous Teaching</u></font></a>      <!--      <a href=""><font color="#FFFFFF"><b>How to program well across languages?</b></font></a><br/>
  <a href=""><font color="#FFFFFF"><b>When team development meets computer games!</b></font></a><br/>
  <a href=""><font color="#FFFFFF"><b>Preparing for a career in Deep Learning?</b></font></a><br/> -->
</div>

<div id="topres" style="display: none; margin-left: 12px; color: #FFFFFF; font-family: 'Helvetica', sans-serif;">
  <table width="1009px"  border="0" cellspacing="0">
    <tr>
      <td align="left" valign="top" width="254px">
        <b>Recent Work Domains</b>:<br/>
        <a id="linkAB2" href="#AB" title="Go to Animal Biometrics" ><font color="#FFFFFF"><u>Animal Biometrics</u></font></a><br/>
        <a id="linkSphere2" href="#sphere" title="Go to the SPHERE Project" ><font color="#FFFFFF"><u>SPHERE Health Technology</u></font></a><br/>
        <a id="linkCV" href="#CV" title="Go to Computer Vision" ><font color="#FFFFFF"><u>Video Representation Learning</u></font></a><br/><br/>
        <a id="linkRes" href="#research"><font color="#FFFFFF"><u>Go to Research Profile</u></font></a><br/>
        <a href="#papers"><font color="#FFFFFF"><u>Go to Research Papers</u></font></a>
      </td>
      <td align="left" valign="top" width="600px">
      <div id="topicSearch"><font size="2pt">
        <table border="0" cellspacing="0"><tr>
        <td valign="top" width="100pt">
          BY ANIMAL<br/>
          <a href="#cattle" title="Go to Individual Friesian Cattle Identification"><font color="#FFFFFF"><u>Cattle</u></font></a><br/>
          <a href="#apes" title="Go to Great Ape Detection, Identification, and Behaviour Recognition" ><font color="#FFFFFF"><u>Chimpanzee</u></font></a><br/>
          <a href="#corals" title="Go to Deep Learning for Coral Analysis" ><font color="#FFFFFF"><u>Coral</u></font></a><br/>
          <a href="#tiny" title="Go to Enhanced Aerial Animal Detection of Elephants and other African Mammals" ><font color="#FFFFFF"><u>Elephant</u></font></a><br/>
          <a href="#forams" title="Go to Taxonomic Deep Learning for Microfossils" ><font color="#FFFFFF"><u>Foraminifer</u></font></a><br/>
          <a href="#apes" title="Go to Great Ape Detection, Identification, and Behaviour Recognition" ><font color="#FFFFFF"><u>Gorilla</u></font></a><br/>
          <a href="#sharks" title="Go to Visual Fin Identification of Great White Sharks" ><font color="#FFFFFF"><u>Shark</u></font></a>
        </td>
        <td valign="top" width="155pt">
          BY TOPIC<br/>
            <a href="#drone" title="Go to Autonomous Drones for Ecology and Farming" ><font color="#FFFFFF"><u>Autonomous Drones</u></font></a><br/>
            <a href="#tiny" title="Go to Enhanced Aerial Animal Detection of Elephants and other African Mammals" ><font color="#FFFFFF"><u>Aerial Detection</u></font></a><br/>
            <a href="#drone" title="Go to Autonomous Drones for Ecology and Farming"><font color="#FFFFFF"><u>Aerial Navigation</u></font></a><br/>
            <a href="#calorie" title="Go to Calorific Expenditure Estimation from Video" ><font color="#FFFFFF"><u>Calorie Estimation</u></font></a><br/>
            <a href="#cep" title="Go to Cycle Encoding Prediction (CEP) for Video Representation Learning" ><font color="#FFFFFF"><u>CEP Cycle Encoding</u></font></a><br/>            <a href="#cattle" title="Go to Individual Friesian Cattle Identification"><font color="#FFFFFF"><u>Individual Identification</u></font></a><br/>
            <a href="#video" title="Go to Context-Agnostic Meta-Learning"><font color="#FFFFFF"><u>Meta-Learning</u></font></a><br/>
        <td valign="top"><br/>
            <a href="#cattle" title="Go to Individual Friesian Cattle Identification"><font color="#FFFFFF"><u>Metric Deep Learning</u></font></a><br/>
            <a href="#occlusion" title="Go to Animal Detection in Challenging Scenarios" ><font color="#FFFFFF"><u>Occlusion Robustness</u></font></a><br/>
            <a href="#sharks" title="Go to Visual Fin Identification of Great White Sharks"><font color="#FFFFFF"><u>Outline Re-Identification</u></font></a><br/>
            <a href="#TRX" title="Go to Person Re-Identification"><font color="#FFFFFF"><u>Person Recognition</u></font></a><br/>
            <a href="#forams" title="Go to Taxonomic Deep Learning for Microfossils" ><font color="#FFFFFF"><u>Taxonomic Deep Learning</u></font></a><br/>
            <a href="#TRX" title="Go to Temporal-Relational CrossTransformers (TRX) for Video Representation Learning" ><font color="#FFFFFF"><u>TRX CrossTransformer</u></font></a><br/>
            <a href="#community" title="Go to Community Building in Interdisciplinary AI for Ecology, Sustainability, and Wildlife Conservation" ><font color="#FFFFFF"><u>Wildlife AI Community</u></font><br/>
        </td>
      </tr></table>
      </font></div>
</td>
</tr></table>
</div>

<div id="Team" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">
  <b>Current Research Team</b><hr/>
        <ul>
          <li><strong>O Brookes</strong> <em>(PhD student since 2020, Co-supervisors M Mirmehdi &amp; H Kuehl)</em></li>
          <li><strong>M Xue</strong> <em>(PhD student since 2020, Co-supervisor M Mirmehdi)</em></li>
          <li><strong>J Gao</strong> <em>(PhD student since 2019, Co-supervisor N Campbell)</em></li>
          <li><strong>T Karaderi</strong> <em>(PhD student since 2019, Co-supervisor D Schmidt)</em></li>
          <li><strong>A Montout</strong> <em>(PhD student since 2018, Co-supervisor A Dowsey)</em></li>
          <li><strong>A Sharma</strong> <em>(PhD student since 2021, Co-supervisors A Dowsey et al.)</em></li>
          <li><strong>S Zeng</strong> <em>(PhD student since 2021, Co-supervisor A Gambaruto)</em></li>
          <li><strong>L Bertini</strong> <em>(PhD student since 2020, Co-supervisors E Hendy &amp; K Johnson)</em></li>
          <li><strong>X Yang</strong> <em>(PhD student since 2018, Co-supervisor M Mirmehdi)</em></li>
          <li><strong>A Sikdar</strong> <em>(SPHERE RA since 2022, with A Masullo and M Mirmehdi)</em></li>
      </ul>
      <b>Research Team Alumni</b><hr/>
      <ul>
          <li><strong>T Perrett</strong> <em>(SPHERE RA, 2018-2022)</em></li>
          <li><strong>A Masullo</strong> <em>(SPHERE RA, 2017-2021)</em>, at <a href="https://research-information.bris.ac.uk/en/persons/alessandro-masullo" target="_blank">UoB</a></li>
          <li><strong>W Andrew</strong> <em>(PhD/RA 2014-2020)</em>, at <a href="https://www.condensereality.com/company/team" target="_blank">CondenseReality</a></li>
          <li><strong>P Hill</strong> <em>(RA, 2016-2018)</em></li>
          <li><strong>P Anantrasirichai</strong> (RA, 2018)</em></li>
          <li><strong>M Camplani</strong> <em>(SPHERE RA 2013-2017)</em></li>
          <li><strong>S Hannuna</strong> <em>(SPHERE RA 2013-2017)</em></li>
          <li><strong>L Tao</strong> <em>(SPHERE RA 2013-2017)</em></li>
          <li><strong>A Paiment</strong> <em>(SPHERE RA 2013-2017)</em></li>
          <li><strong>M Devereux</strong> <em>(RA, 2015-2016)</em></li>
          <li><strong>B Hughes</strong> <em>(PhD 2010-2015)</em>, at <a href="https://saveourseas.com/project-leader/benjamin-hughes" target="_blank">SaveOurSeas</a></li>
</li>
          <li><strong>R Sandwell</strong> <em>(PhD 2010-2015)</em></li>
          <li><strong>L Palmer</strong><em> (MSc by Research 2011-2014) </em></li>
      </ul>
</div>

<div id="FinalYear" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">
<b>Current Final Year Student Projects</b><hr/>
<ul>
        <li>Deep Learning for the Recognition of Individual Zebras and Giraffes via Deep 3D Fitting (MEng 2022, M Stennett)</li>
        <li>Real-world Smart Cattle Tracking Proof-of-Concept via Deep Learning (MEng 2022, B Dumitrescu)</li>
        <li>Deep Learning for the Accurate Video Detection of European Wildlife from Camera Traps (MEng 2022, O Suleiman)</li>
        <li>Pose Modelling of Great Apes using Deep Learning (MEng 2022, R George)</li>
        <li>Deep Learning for the Accurate Modelling of 3D Growth Bands from 3D X-Rays (MEng 2022, M Mohammed)</li>
        <li>Explainable GAN-based Synthesis of Microfossils (MEng 2022, D Salter)</li>
        <li>Individual Identification of Giraffes using Animal Biometrics (MEng 2022, H Kountouris)</li>
        <li>Coralite 3D Reconstruction using Deep Learning (BSc 2022, B Karaman)</li>
        <li>Computer Vision to Understand Predator-Prey Interactions at Sea (MSc 2023, A Gunning)</li>
        <li>Reliable Animal Detection in Underwater Footage (MSc 2023, M Yang)</li>
  </ul>
<!--
<br/><b>Previeous Student Projects</b><hr/>
<font size="2">
        <ul>Visual Recognition of Individual Giraffes via Deep Learning<br/>(BSc 2021, J Allen)</ul>
        <ul>Automatic Sky Replacement for Landscape Photographs<br/>(MSc 2021, X Huang)</ul>
        <ul>Deep Learning to Understand Coral Growth from 3D X-Rays<br/>(MEng 2021, C Uthaya-Shankar)</ul>
        <ul>Deep Learning for the Reconstruction of Coral from 3D X-Rays<br/>(MEng 2021, S Varcoe)</ul>
        <ul>Deep Learning for the Detection of European Wildlife from Camera Traps<br/>(MEng 2021, G Ioannou)</ul>
        <ul>Deep Learning for the High Quality Recognition of African Wildlife from Drone Imagery<br/>(MEng 2021, T Greenslade)</ul>
        <ul>Image Quality Classification of Manta Ray Imagery<br/>(MEng 2020L, B Fossett)</ul>
        <ul>Deep Volumetric Image Processing for Coral Analysis<br/>(MEng 2020L, A Rutterford)</ul>
        <ul>Great Ape Activity and Behaviour Recognition via Deep Learning<br/>(MEng 2020L, F Sakib)</ul>
        <ul>Deep Open Set ID of Individual Manta Rays<br/>(MEng 2020, D Scott)</ul>
        <ul>Deep End-to-end Great White Shark Fin Identification<br/>(MEng 2020, J Arneil)</ul>
        <ul>Spider Web Reconstruction using Deep Segmentation<br/>(MSc 2020, O Skeates)</ul>
        <ul>Identification of Animals in Drone Imagery<br/>(MSc, 2020, R Mapperson)</ul>
        <ul>Classifying Endless Forams<br/>(BSc 2020, J Ramaer, Reviewer M Mirmehdi)</ul>
        <ul>Auto-marking of Handwritten Linear Algebra Papers<br/>(MSc 2019, N Drake)</ul>
        <ul>Forest Canape Segmentation and Classification using RNNs<br/>(MSc 2019, I Myttas)</ul>
        <ul>Individual Gorilla Face Recognition using Deep Learning Techniques<br/>(MSc 2019, O Brookes)</ul>
        <ul>Cost Functions and Multi-Sample Learning for Deep Animal Identification<br/>(MSc 2019, X Zhang)</ul>
        <ul>Supervised vs Reward-based Learning for Search and Rescue<br/>(BSc 2019, JQ Ovalle)</ul>
        <ul>Shark Fin Segmentation and ID via Deconvolutional Networks<br/>(BSc 2019, L Zhang)</ul>
        <ul>Black and White Sport Footage Recolouration with Cycle GANs<br/>(BSc 2019, J Atton)</ul><hr/>
        <ul>Deep Learning for the Identification of Individual Manta Rays</ul>
        <ul>Deep Learning for Classifying Marine Calcareous Microfossils</ul>
        <ul>Individual Elephant Identification using DNN Biometrics</ul>
        <ul>Style Transfer via Cycle Loss using Deep Neural Networks</ul>
        <ul>Whiteboard2Website: Interactive Digital Visual Content Generation Tool</ul>
        <ul>Great Ape Detection by Motion Signature using CNNs</ul>
        <ul>CNN-based 3D Key Reconstruction from Photography</ul>
        <ul>From Spider Web Photos to Spider Web Simulations</ul>
        <ul>Ecological Prediction using Deconvolutional and Convolutional Neural Nets</ul>
        <ul>Iris Recognition using Low-cost Portable Devices</ul>
        <ul>Elephant Facial Biometrics using Convolutional Neural Networks</ul>
        <ul>Automatic Visual Detection of Speech and Pausing in Testimony Videos</ul>
        <ul>Classifying Marine Calcareous Microfossils</ul>
        <ul>Evaluating CNN Models for Animal Detection in Mobile Vision Applications</ul>
        <ul>Detecting Gorillas in Natural Images using CNNs</ul>
        <ul>LoD for Elephant-part Recognition using Hierarchical DPBMs</ul>
        <ul>CatIdentifier: Which cat is in the Photo?</ul>
        <ul>Music Score Interpretation from Sketches using a Native Mobile Platform</ul>
        <ul>Mammography Interpretation Tool using Computer Vision</ul>
        <ul>Insect Species Recognition via Combined Local and Global Features</ul>
        <ul>Enhancing AR Museum Guides Using Markerless Tracking and 3D Model Generation in a Web-Browser</ul>
        <ul>Visual Identification and Comparison of Bristol graffiti</ul>
        <ul>Object Specific Haar-like Features for Fast and Accurate Shark Fin Detection</ul>
        <ul>Salient Object Detection for Navigation of Mars-like Environments</ul>
</font>-->
</div>

<div id="ABprojects" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">

  <a id="cattle"></a>
  <b>Project: Individual Friesian Cattle Identification</b><br/><font size="2pt">(various pieces of work with J Gao, A Sharma, W Andrew, N Campbell, A Dowsey, C Greatwood, S Hannuna, S Mullan, and others in collaboration with the <a href="http://farscope.bris.ac.uk" target="_blank">Farscope CDT</a>, <a href="https://vilab.blogs.bristol.ac.uk" target="_blank">VILab</a>, and <a href="http://www.bristol.ac.uk/vetscience" target="_blank">BVS</a>)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="friesian.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> This line of work has been the first to apply visual deep learning to the task of automated individual bovine identification. Friesian cattle carry a unique black and white coat pattern. We have produced a highly accurate state-of-the-art recognition framework for this biometric entity in real-world farm environments. Methods such as metric learning and self-supervision can be utilised to improve performance and flexibility of application.<br/>
    <b><i>Keywords:</i></b> Sustainable Farming, Friesian Cattle, Metric Learning, Individual Identification, Coat Pattern Analysis, Self-Supervision, Long-term
Recurrent Convolutional Networks<br/>
    <b><i>Papers:</i></b>
         <a href="https://doi.org/10.1016/j.compag.2021.106133" target="_blank">Deep Metric Learning (CEA2021)</a>,
         <a href="https://arxiv.org/abs/2105.01938" target="_blank">Self-Supervision (CV4A2021)</a>,
         <a href="http://dx.doi.org/10.1109/IROS40897.2019.8968555" target="_blank">Autonomous Drones (IROS2019)</a>,
         <a href="https://doi.org/10.1109/ICCVW.2017.336" target="_blank">LRCN Deep Learning (ICCVW2017)</a>,
         <a href="http://dx.doi.org/10.1109/ICIP.2016.7532404" target="_blank">SIFT ID via RGB-D (ICIP2016)</a><br/>
    <b><i>Thesis:</i></b>
        <a href="https://research-information.bris.ac.uk/admin/files/210500057/Final_Copy_2019_05_07_Andrew_W_PhD_Redacted.pdf" target="_blank">Visual Biometric Processes for Collective Identification of Individual Friesian Cattle</a><br/>
    <b><i>Datasets:</i></b>
        <a href="https://doi.org/10.5523/bris.4vnrca7qw1642qlwxjadp87h7" target="_blank">Cows2021</a>,
        <a href="https://doi.org/10.5523/bris.10m32xl88x2b61zlkkgz3fml17" target="_blank">OpenCows2020</a>,
        <a href="https://doi.org/10.5523/bris.3owflku95bxsx24643cybxu3qh" target="_blank">AerialCattle2017</a>,
        <a href="https://doi.org/10.5523/bris.2yizcfbkuv4352pzc32n54371r" target="_blank">FriesianCattle2017</a>,
        <a href="https://doi.org/10.5523/bris.wurzq71kfm561ljahbwjhx9n3" target="_blank">FriesianCattle2015</a><br/>
    <b><i>Code:</i></b>
        <a href="https://github.com/CWOA/MetricLearningIdentification" target="_blank">Metric Learning for Friesian ID (GitHub)</a>,
        <a href="https://github.com/Wormgit/Cows2021" target="_blank">Self-Supervision for Friesian ID (GitHub)</a>
  </td>
  </tr></table>

  <a id="drone"></a>
  <b>Project: Autonomous Drones for Ecology and Farming</b><br/>
  <font size="2pt">(with W Andrew, C Greatwood, and others in collaboration with the <a href="http://farscope.bris.ac.uk" target="_blank">Farscope CDT</a>, <a href="https://vilab.blogs.bristol.ac.uk" target="_blank">VILab</a>, and <a href="http://www.bristol.ac.uk/vetscience" target="_blank">BVS</a>)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="drone.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> This project has delivered the first system to achieve autonomous aerial animal biometrics. We built a computationally-enhanced M100 UAV platform with an onboard deep learning inference system for integrated computer vision and navigation able to autonomously find and visually identify by coat pattern individual Holstein Friesian cattle in freely moving herds. It has been tested on a small herd in a real-world pasture setting and produced error-free identification. Our proof-of-concept system is a successful step towards autonomous biometric identification of individual animals from the air in open pasture environments for tag-less AI support in farming and ecology. <br/>
    <b><i>Keywords:</i></b> UAV Navigation, Autonomous Robotics, On-board Inference, Multi-Stream Architecture, Sustainable Farming, Friesian Cattle<br/>
    <b><i>Summary Video:</i></b>
        <a href="videodrone.html" target="_blank">IROS 2019 Video Summary</a>, <a href="videonav.html" target="_blank">IROS 2018 Video Summary</a><br/>
    <b><i>Papers:</i></b>
         <a href="http://dx.doi.org/10.1109/IROS40897.2019.8968555" target="_blank">Autonomous Drones (IROS2019)</a>,
         <a href="https://doi.org/10.1109/IROS.2018.8593751" target="_blank">Dynamic Aerial Navigation (IROS2018)</a>,
         <a href="https://doi.org/10.1109/ICCVW.2017.336" target="_blank">Deep Learning (ICCVW2017)</a><br/>
    <b><i>Datasets:</i></b>
        <a href="https://doi.org/10.5523/bris.10m32xl88x2b61zlkkgz3fml17" target="_blank">OpenCows2020</a>,
        <a href="https://doi.org/10.5523/bris.2zot65rxlmgqq23au92qwkaa3x" target="_blank">GTRF2018</a>,
        <a href="https://doi.org/10.5523/bris.3owflku95bxsx24643cybxu3qh" target="_blank">AerialCattle2017</a><br/>
    <b><i>Code:</i></b>
        <a href="https://doi.org/10.5523/bris.2zot65rxlmgqq23au92qwkaa3x" target="_blank">Grid-based Target Recovery Framework (GTRF)</a>
  </td>
  </tr></table>

  <a id="apes"></a>
  <b>Project: Great Ape Individual Identification and Behaviour Recognition</b><br/><font size="2pt">(various lines of work with O Brookes, F Sakib, M Mirmehdi, H Kuehl, CA Brust, M Groenenberg, C Kaeding, M Manguette, J Denzler, RC Sandwell, A Loos and others)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="apes.gif" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Automated monitoring of Great Apes from integrated camera trapping is a pillar for answering important research questions relating to their ecology and conservation. Gathering long-term data on the demographics, behaviours, group structure and social interactions are fundumental for understanding these charismatic creatures better. Here in Bristol we have built systems to detect Great Apes in challenging settings and individually identify them based on their facial features. We have also constructed the first systems that can automatically recognise great ape activities in video. Some facial recognition systems have been applied to thousands of camera trap videos in the wild. We are currently scaling up our work and build new international collaborations to further this line of work.<br/>
    <b><i>Keywords:</i></b> Behaviour Classification, Facial Recognition, Individual Identification, Dual-Stream Networks, Long-term
Recurrent Convolutional Networks, Spatio-Temporal Analysis, Wildlife Conservation<br/>
    <b><i>Papers:</i></b>
      <a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20fstb.pdf" target="_blank">Deep Behaviour Recognition (VAIB2021)</a>,
      <a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20obtb.pdf" target="_blank">Deep Face Recognition for Zoos (VAIB2021)</a>,
      <a href="https://doi.org/10.1109/ICCVW.2017.333" target="_blank">Deep Face Recognition in the Wild (ICCVW2017)</a>,
      <a href="http://dx.doi.org/10.1002/ajp.22627" target="_blank">Face Detection in CamTrap Data (AJP2017)</a>,
      <a href="http://www.bmva.org/bmvc/2013/Workshop/0003.pdf" target="_blank">Augmentation for Face Detection (BMVW2013)</a><br/>
    <b><i>Datasets:</i></b>
      <a href="https://doi.org/10.5523/bris.jf0859kboy8k2ufv60dqeb2t8" target="_blank">BristolGorillas2020 Videos and Annotations</a>,
      <a href="https://doi.org/10.5523/bris.jh6hrovynjik2ix2h7m6fdea3" target="_blank">PanAfrican2020 Annotations</a>,
      <a href="mpi2019.txt" target="_blank">PanAfrican2019 Videos</a>,
      <a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">PanAfrican2019 Annotations</a>,
      <a href="gorilla2017.txt" target="_blank">Gorilla2017 Videos</a><br/>
    <b><i>Code:</i></b>
      <a href="https://github.com/fznsakib/great-ape-behaviour-detector" target="_blank">Dual-Stream LRCN Behaviour Recognition (GitHub)</a>,
      <a href="https://github.com/obrookes/BristolGorillas2020" target="_blank">YOLO-based Gorilla Face Recognition (GitHub)</a>
  </td>
  </tr></table>

  <a id="occlusion"></a>
  <b>Project: Occlusion-robust Animal Detection in Challenging Scenarios</b>
  <br/><font size="2pt">(with X Yang and M Mirmehdi)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="jungle.gif" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Camera trap video data showing heavily occluded animals pose significant challenges to current computer vision methods. Yet, the analysis of biodiversity and animal population dynamics such as chimpanzees and gorillas around the globe often depends on such scenarios. We have started to address these problems using deep object detectors with techniques such as spatio-temporal feature blending which is a special form of attention-based learning. We have published our results along with all relevant key code sections.<br/>
    <b><i>Keywords:</i></b> Spatio-Temporal Feature Blending, Attention Model, ResNet-based Feature Pyramid Network<br/>
    <b><i>Paper:</i></b>
      <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVWC/Yang_Great_Ape_Detection_in_Challenging_Jungle_Camera_Trap_Footage_via_ICCVW_2019_paper.pdf" target="_blank">Challenging Great Ape Detection in Video (ICCVW2019)</a><br/>
    <b><i>Datasets:</i></b>
      <a href="mpi2019.txt" target="_blank">PanAfrican2019 Videos</a>,
      <a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">PanAfrican2019 Annotations</a><br/>
    <b><i>Code:</i></b>
      <a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">SCM and TCM Deep Detection Components</a>
  </td>
  </tr></table>

  <a id="tiny"></a>
  <b>Project: Enhanced Aerial Animal Detection of Elephants and other African Mammals</b>
  <br/><font size="2pt">(various lines of work with M Xue, T Greenslade, and M Mirmehdi)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="sr.gif" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Tiny animal visuals (a few dozen pixels) captured by high-flying aerial drones are, despite advances in deep learning, a seriously challenging application for current AI detection methods. However, census operations in wildlife parks and conservation efforts often rely on exactly such data. Our line of work has for the first time combined deep super-resolution networks with contextual metadata such as drone altitude to animal detection in order to address the issue. Our systems are evaluated on large, publically available datasets including <a href="https://zenodo.org/record/3234780" target="_blank">AED</a> and <a href="https://www.epfl.ch/labs/lasig/research/projects/savmap" target="_blank">SAVMAP</a> containing African elephants and African mammals, respectively.<br/>
    <b><i>Keywords:</i></b> Super-Resolution, Altitude Data, Holistic Attention Network<br/>
    <b><i>Paper:</i></b>
      <a href="https://arxiv.org/abs/2111.06830" target="_blank">Aerial Animal Surveillance with Super-Resolution and Altitude Data (WACVW2022)</a>
  </td>
  </tr></table>

  <a id="forams"></a>
  <b>Project: Taxonomic Deep Learning for Microfossils</b>
  <br/><font size="2pt">(with T Karaderi, AY Hsiang, J Ramaer, and DN Schmidt)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="forams.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Identifying a species based on appearance of the phenotype can be challenging (even for seasoned taxonomists) if visual features are very similar - as is the case in some planktic microfossils. We have started to apply deep metric learning for the first time to the problem of classifying these planktic foraminifer shells on microscopic images. This species recognition task is an important information source and scientific pillar for reconstructing past climates. All foraminifer CNN recognition pipelines in the literature produce black-box classifiers that lack visualisation options for human experts and cannot be applied to open set problems. We have benchmarked metric learning against these pipelines, have produced the first scientific visualisation of the phenotypic planktic foraminifer morphology space, and demonstrated that metric learning can be used to cluster species unseen during training. Metric learning proves highly effective for this domain and can serve as an important tool towards expert-in-the-loop automation of microfossil identification. <br/>
    <b><i>Keywords:</i></b> Deep Metric Learning, Clustering, Planktic Foraminifer, Active Learning<br/>
    <b><i>Pre-print:</i></b>
      <a href="https://arxiv.org/abs/2112.09490" target="_blank">Microfossil Identification via Deep Metric Learning</a>
  </td>
  </tr></table>

    <a id="sharks"></a>
    <b>Project: Visual Fin Identification of Great White Sharks</b>
    <br/><font size="2pt"> (with B Hughes and M Scholl at the <a href="https://www.saveourseasmagazine.com/high-tech-fin-recognition" target="_blank">The SaveOurSeas Foundation</a>)</font><hr/>
    <table><tr>
    <td width="70%" style="text-align: justify;" valign="top">
      <img src="shark.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
      <b><i>Summary:</i></b> Dorsal fins of Great Whites as seen in photographs have an individually characteristic outline shape. This poses a fine-grained, multi-instance classification problem for flexible, fairly textureless and possibly partly occluded objects. Our biometric approach was the first to operate fully automatically on shark fins. It utilises classical machine learning and computer vision to identify individual animals. We exploit techniques such as ultrametric contour maps, combinatorial spectral fingerprinting, LNBNN classification, and random forest fin space construction. <a href="https://saveourseas.com/project-leader/benjamin-hughes/" target="_blank">B Hughes</a> has built further on this work and created real-world animal ID applications for the <a href="https://saveourseas.com" target="_blank">SaveOurSeas Foundation</a> and the <a href="https://www.mantatrust.org" target="_blank">Manta Trust</a>.<br/>
      <b><i>Keywords:</i></b> Great White Sharks, Contour Recognition, Individual Identification, Pattern Matching, Segmentation<br/>
      <b><i>Talks:</i></b>
        <a href="http://www.bmva.org/bmvc/2015/papers/paper092/video.m4v" target="_blank">BMVC 2015 Presentation of Early System</a>, 
        <a href="videoshark.html" target="_blank">Shark and Ray Symposium 2015 Presentation of Early System</a><br/>
      <b><i>Papers:</i></b> <a href="http://dx.doi.org/10.1007/s11263-016-0961-y" target="_blank">Full FinSpace Prototype (IJCV2017)</a>,
                         <a href="https://dx.doi.org/10.5244/C.29.92" target="_blank">LNBNN-based System (BMVC2015)</a>,
                         <a href="http://www.bmva.org/bmvc/2015/mvab/papers/paper008/index.html" target="_blank">Fin Segmentation (BMVCW2015)</a><br/>
      <b><i>Datasets:</i></b>
           <a href="finscholl2456.txt" target="_blank">FinsScholl2456</a>
    </td>
    </tr></table>

    <a id="corals"></a>
    <b>Project: Deep Learning for Coral Analysis</b>
    <br/><font size="2pt">(with L Bertini, EJ Hendy, KG Johnson, A Rutterford, and R Summerfield in collaboration with the <a href="https://www.4d-reef.eu/" target="_blank">4DReef</a> Marie Skłodowska-Curie ITN)</font><hr/>
    <table><tr>
    <td width="70%" style="text-align: justify;" valign="top">
      <img src="coral.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
      <b><i>Summary:</i></b> X-ray micro–computed tomography is increasingly used to record the skeletal structure of corals. We are developing artificial intelligence approaches to assist the expert identification in small colonies of massive Porites spanning decades. For example, annual skeletal banding unlocks information about growth and calcification critical for an understanding of ecology and conservation questions. We recommend the development of a community platform to share annotated images for AI and have started to put this into practice.<br/>
      <b><i>Keywords:</i></b> Deep Learning, UNet Architectures, Segmentation, Coral Ecology<br/>
      <b><i>Paper:</i></b>
          <a href="https://doi.org/10.1007/s42452-021-04912-x" target="_blank">Coral Skeletal Density-banding using Deep Learning (SNAS2022)</a><br/>
      <b><i>Code:</i></b>
          <a href="https://github.com/ainsleyrutterford/deep-learning-coral-analysis" target="_blank">UNet-based Banding Recognition (GitHub)</a>
    </td>
    </tr></table>

  <a id="community"></a>
  <b>Community Building: Interdisciplinary AI for Ecology, Sustainability, and Wildlife Conservation</b><br/><font size="2pt">(work across various aspects with international reserachers including H Kuehl, P Bodesheim, J Denzler, D Tuia, B Kellenberger, S Beery, BR Costelloe, S Zuffi, B Risse, A Mathis, MW Mathis, F van Langevelde, R Kays, H Klinck, M Wikelski, ID Couzin, G van Horn, MC Crofoot, CV Stewart, T Berger-Wolf, RB Fisher and others)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="sensors.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> We are part of a growing international community of engineers and computer scientists who believe that deep learning approaches can be integrated into ecological workflows to improve inputs for conservation and behaviour research. Data acquisition in animal ecology is rapidly accelerating building on inexpensive and readily available sensor data. This holds great potential for large-scale environmental monitoring and understanding. This approach will require decades of work and further close collaboration as well as cross-disciplinary education between the computer science and animal ecology communities. We will need to train a new generation of data scientists who work in ecology, conservation, biodiversity and sustainability sectors. Everything starts with coming together to meet this challenge - <a href="https://www.bristol.ac.uk/engineering/departments/computerscience/courses/postgraduate/" target="_blank">join us</a>!<br/>
    <b><i>Keywords:</i></b> International Community, Biodiversity, Sustainability, Conservation<br/>
    <b><i>Papers:</i></b>
         <a href="https://arxiv.org/abs/2110.12951" target="_blank">Seeing Biodiversity: Machine Learning for Conservation (2022)</a>,
         <a href="http://dx.doi.org/10.1049/iet-cvi.2018.0019" target="_blank">Computer Vision for Animal Biometrics (IETCV2018)</a>,
         <a href="http://dx.doi.org/10.4230/DagRep.7.2.109" target="_blank">Computer Science meets Ecology (DAGSTUHL2017)</a>,
         <a href="http://dx.doi.org/10.1016/j.tree.2013.02.013" target="_blank">Animal Biometrics (TREE2013)</a>
         <br/>
  </td>
  </tr></table>

</div>

<div id="SPprojects" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">

  <a id="calorie"></a>
  <b>Project: Calorific Expenditure Estimation from Video</b>
  <br/><font size="2pt">(various lines of work with A Masullo, L Tao, T Perrett, B Wang, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock and carried out as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="cal.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> This line of work has shown that the estimation of calorific expenditure from video and inertial sensors can be implemented reliably, even when applied in privacy-enhanced environments where RGB video is replaced with sihouette data only. We have experimented with classical machine learning and deep learning solutions, combining various input modalities from visual, depth, and inertial sensors. The work has been validated with a gold-standard gas exchange calorimeter, that is a portable metabolic measurement system of type COSMED K4b2.<br/>
    <b><i>Keywords:</i></b> Deep Learning, Calorific Expenditure Estimation, Motion Analysis, Inertial Sensors, RGB-D<br/>
    <b><i>Papers:</i></b>
      <a href="http://bmvc2018.org/contents/papers/0383.pdf" target="_blank">Inertial+Silhouette CalorieNet System (BMVC2018)</a>,
      <a href="https://doi.org/10.1109/WACVW.2018.00014" target="_blank">Deep Learning (WACVW2018)</a>,
      <a href="http://dx.doi.org/10.1049/iet-cvi.2017.0112" target="_blank">Inertial+Visual System (IETCV2018)</a>,
      <a href="https://doi.org/10.1007/978-3-319-54407-6" target="_blank">RGB-D Calorie Counter (LNCS2016)</a><br/>
    <b><i>Datasets:</i></b>
      <a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">SPHERE-Calorie</a><br/>
      <b><i>Code:</i></b>
        <a href="https://github.com/ale152/CaloriNet" target="_blank">CalorieNet (GitHub)</a>
  </td>
</tr></table>

<a id="TRX"></a>
<b>Project: Person Re-Identification via Deep Learning</b>
<br/><font size="2pt">(various lines of work with A Masullo, T Perrett, V Ponce-Lopez, Y Sun, D Damen, and M Mirmehdi as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="id.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> The identification of individuals from visual, distance, and inertial sensors is critical for enabling individualised services including health technology and assisted living settings. In these works we explored various deep learning techniques in order to re-identify individuals including more private settings where only silhouette and/or inertial data is available.<br/>
  <b><i>Keywords:</i></b> Deep Learning, Person Identification, Private Environments, Silhouette Data, Inertial Sensors, RGB-D<br/>
  <b><i>Papers:</i></b>
    <a href="https://www.doi.org/10.5220/0010202903280337" target="_blank">Multi-Sensory Fusion in Real-World Settings (VISAPP2021)</a>,
    <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVPM/Masullo_Who_Goes_There_Exploiting_Silhouettes_and_Wearable_Signals_for_Subject_ICCVW_2019_paper.pdf" target="_blank">Silhoutte and Wearable Re-ID (SENSORS2021)</a>,
    <a href="http://dx.doi.org/10.1007/978-3-030-30642-7_44" target="_blank">Guided DC-GANs for Re-ID (ICIAP2019)</a>,
    <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Ponce-Lopez_Semantically_Selective_Augmentation_for_Deep_Compact_Person_Re-Identification_ECCVW_2018_paper.pdf" target="_blank">Selective Augmentation for Re-ID (ICIAP2019)</a>
    <br/>
 </td>
</tr></table>

<a id="CV"></a>
<script> document.write(getVision());</script>

<a id="video"></a>
<b>Project: Context-Agnostic Meta-Learning</b>
<br/><font size="2pt">(with T Perrett, A Masullo, M Mirmehdi, D Damen as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="meta.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> In the literature, meta-learning approaches have addressed few-shot problems by finding initialisations suited for fine-tuning to target tasks. Yet, often there are additional properties within training data (the context), not relevant to the target task, which can act as a distractor to meta-learning. We address this oversight by incorporating a context-adversarial component into the meta-learning process. This produces an initialisation which is both context-agnostic and task-generalised. We evaluate our approach on three commonly used meta-learning algorithms and four case studies including calorie estimation data.</b> .<br/>
  <b><i>Keywords:</i></b> Meta-Learning, Context, Calorific Expenditure Estimation<br/>
  <b><i>Paper:</i></b>
    <a href="https://doi.org/10.1007/978-3-030-69538-5_5" target="_blank">Context-Agnostic Meta-Learning (ACCV2021)</a><br/>
  <b><i>Datasets:</i></b>
    <a href="https://tobyperrett.github.io/contextagnosticweb" target="_blank">Website with Splits for Mini-ImageNet and CUB</a>,
    <a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">SPHERE-Calorie</a><br/>
</td>
</tr></table>

<a id="TRX"></a>
<b>Project: Temporal-Relational CrossTransformers (TRX) for Video Representation Learning</b>
<br/><font size="2pt">(work with T Perrett, A Masullo, M Mirmehdi, D Damen as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="trct.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> Temporal-Relational CrossTransformers (TRX) observe relevant sub-sequences of all support videos, rather than  class averages or single best matches. This approach achieved state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51, and UCF101 at publication. Importantly, the TRX method outperformed prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations.<br/>
  <b><i>Keywords:</i></b> Transformers, Contrastive Learning, Pre-Training, Action Recognition<br/>
  <b><i>Paper:</i></b>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Perrett_Temporal-Relational_CrossTransformers_for_Few-Shot_Action_Recognition_CVPR_2021_paper.pdf" target="_blank">Temporal-Relational Cross-Transformers (TRX) (CVPR2021)</a><br/>
  <b><i>Code:</i></b>
    <a href="https://github.com/tobyperrett/trx" target="_blank">Temporal-Relational CrossTransformers (GitHub)</a>
</td>
</tr></table>

<a id="cep"></a>
<b>Project: Cycle Encoding Prediction (CEP) for Video Representation Learning</b>
<br/><font size="2pt">(with X Yang and M Mirmehdi)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="cep.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> We show that learning video feature spaces in which temporal cycles are maximally predictable benefits action classification. In particular, we propose a novel learning approach termed Cycle Encoding Prediction (CEP) that is able to effectively represent high-level spatio-temporal structure of unlabelled video content. CEP builds a latent space wherein the concept of closed forward-backward as well as backward-forward temporal loops is approximately preserved. As a self-supervision signal, CEP leverages the bi-directional temporal coherence of the video stream and applies loss functions that encourage both temporal cycle closure as well as contrastive feature separation. We also show that using CEP as an add-on may improve the performance of existing architectures.<br/>
  <b><i>Keywords:</i></b> Deep Learning, Temporal Cycle Learning, Self-Supervision, Action Recognition<br/>
  <b><i>Paper:</i></b>
    <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0399.pdf" target="_blank">CEP Self-Supervision (BMVC2021)</a><br/>
  <b><i>Code:</i></b>
    <a href="https://github.com/youshyee/CEP" target="_blank">Cycle Encoding Prediction (GitHub)</a>
</td>
</tr></table>
</div>

<div id="pubs" style="width:100%; display: none; vertical-align: top; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">
<div style="vertical-align: top;">
<b>Latest Papers and Publications</b><hr/>
<table id="pubstab" width="100%" border="0" cellspacing="10" style="text-align: justify; margin-top: -24px; font-family: 'Helvetica', sans-serif; font-size:12pt;" >
  <tr><td></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="mi2022.jpg" width="118px" style="float: left; margin-right:19px;"/> T Karaderi, T Burghardt, AY Hsiang, J Ramaer, DN Schmidt. <strong>Visual Microfossil Identification via Deep Metric Learning.</strong> In Press. <em>3rd International Conference on Pattern Recognition and Artificial Intelligence (ICPRAI), Lecture Notes in Computer Science (LNCS), </em>June 2022. (<a href="https://arxiv.org/abs/2112.09490" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> T Perrett, A Masullo, D Damen, T Burghardt, I Craddock, M Mirmehdi. <strong>Personalised Energy Expenditure Estimation: A Visual Sensing Approach With Deep Learning.</strong> In Press. <em>JMIR Formative Research, </em>2022.</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="lplf22.jpg" width="118px" style="float: left; margin-right:19px;"/> J Gao, T Burghardt, NW Campbell. <strong>Label a Herd in Minutes: Individual Holstein-Friesian Cattle Identification.</strong> In Press. <em>21st International Conference on Image Analysis and Processing Workshop (ICIAPW) on Learning in Precision Livestock Farming (LPLF), Lecture Notes in Computer Science (LNCS), </em>May 2022. (<a href="http://arxiv.org/abs/2204.10905" target="_blank">Arxiv PDF</a>), (<a href="https://doi.org/10.5523/bris.4vnrca7qw1642qlwxjadp87h7" target="_blank">Dataset Cows2021</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vaib2020a.jpg" width="118px" style="float: left; margin-right:19px;"/> O Brookes, SI Gray, P Bennett, KV Burgess, FE Clark, E Roberts, T Burghardt. <strong>Evaluating Cognitive Enrichment for Zoo-Housed Gorillas Using Facial Recognition.</strong> In Press. <em>Frontiers in Veterinary Science, Animal Behavior and Welfare</em>, 2022. (<a href="https://doi.org/10.3389/fvets.2022.886720" target="_blank">DOI:10.3389/fvets.2022.886720</a>), (<a href="https://www.frontiersin.org/articles/10.3389/fvets.2022.886720" target="_blank">Frontiers Online</a>), (<a href="https://github.com/obrookes/BristolGorillas2020" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.jf0859kboy8k2ufv60dqeb2t8" target="_blank">Dataset BristolGorillas2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="nc2022.jpg" width="118px" style="float: left; margin-right:19px;"/> D Tuia, B Kellenberger, S Beery, BR Costelloe, S Zuffi, B Risse, A Mathis, MW Mathis, F van Langevelde, T Burghardt, R Kays, H Klinck, M Wikelski, ID Couzin, G van Horn, MC Crofoot, CV Stewart, T Berger-Wolf. <strong>Perspectives in Machine Learning for Wildlife Conservation.</strong> <em>Nature Communications</em>, Vol 13, Issue 1, No 792. February 2022. (<a href="https://www.nature.com/articles/s41467-022-27980-y" target="_blank">DOI:10.1038/s41467-022-27980-y</a>), (<a href="https://arxiv.org/abs/2110.12951" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="snap22.jpg" width="118px" height="118px" style="float: left; margin-right:19px;"/> A Rutterford, L Bertini, EJ Hendy, KG Johnson, R Summerfield, T Burghardt. <strong>Towards the Analysis of Coral Skeletal Density-banding using Deep Learning.</strong> <em>Springer Nature Applied Sciences</em>, Vol 4, Issue 2, No 38, January 2022. (<a href="https://doi.org/10.1007/s42452-021-04912-x" target="_blank">DOI:10.1007/s42452-021-04912-x</a>), (<a href="https://static-content.springer.com/esm/art%3A10.1007%2Fs42452-021-04912-x/MediaObjects/42452_2021_4912_MOESM1_ESM.docx" target="_blank">Supplementary</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="wacvw22.jpg" width="118px" height="118px" style="float: left; margin-right:19px;"/> M Xue, T Greenslade, M Mirmehdi, T Burghardt. <strong>Small or Far Away? Exploiting Deep Super-Resolution and Altitude Data for Aerial Animal Surveillance.</strong> <em>Real-World Surveillance: Applications and Challenges Workshop (RWS) at IEEE Winter Conference on Applications of Computer Vision (WACVW)</em>, pp. 509-519, January 2022. (<a href="https://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Xue_Small_or_Far_Away_Exploiting_Deep_Super-Resolution_and_Altitude_Data_WACVW_2022_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/2111.06830" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/mowen111/salt" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc21.jpg" width="118px" height="80px" style="float: left; margin-right:19px;"/> X Yang, M Mirmehdi, T Burghardt. <strong>Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning.</strong> <em>Proc. 32nd British Machine Vision Conference (BMVC)</em>, 399, BMVA Press, November 2021. (<a href="https://www.bmvc2021-virtualconference.com/assets/papers/0399.pdf" target="_blank">BMVA Version</a>), (<a href="https://arxiv.org/abs/2010.07217" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/youshyee/CEP" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cvpr.jpg" width="118px" style="float: left; margin-right:19px;"/> T Perrett, A Masullo, T Burghardt, M Mirmehdi, D Damen. <strong>Temporal-Relational CrossTransformers for Few-Shot Action Recognition.</strong> <em>Proc. 34th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 475-484, June 2021. (<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Perrett_Temporal-Relational_CrossTransformers_for_Few-Shot_Action_Recognition_CVPR_2021_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/2101.06184v3" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/tobyperrett/trx" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cvprw.jpg" width="118px" style="float: left; margin-right:19px;"/> J Gao, T Burghardt, W Andrew, AW Dowsey, NW Campbell. <strong>Towards Self-Supervision for Video Identification of Individual Holstein-Friesian Cattle: The Cows2021 Dataset.</strong> <em>34th IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)</em>, June 2021. (<a href="https://arxiv.org/abs/2105.01938" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/Wormgit/Cows2021" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.4vnrca7qw1642qlwxjadp87h7" target="_blank">Dataset Cows2021</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cea.jpg" width="118px" style="float: left; margin-right:19px;"/> W Andrew, J Gao, S Mullan, N Campbell, AW Dowsey, T Burghardt. <strong>Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning.</strong> <em>Computers and Electronics in Agriculture</em>, Vol 185, June 2021. (<a href="https://doi.org/10.1016/j.compag.2021.106133" target="_blank">DOI:10.1016/j.compag.2021.106133</a>), (<a href="https://arxiv.org/abs/2006.09205" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/CWOA/MetricLearningIdentification" target="_blank">GitHub</a>), (<a href="https://data.bris.ac.uk/data/dataset/10m32xl88x2b61zlkkgz3fml17" target="_blank">Dataset OpenCows2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi. <strong>No Need for a Lab: Towards Multi-Sensory Fusion for Ambient Assisted Living in Real-World Living Homes.</strong> <em> 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP)</em>, February 2021. (<a href="https://www.doi.org/10.5220/0010202903280337" target="_blank">DOI:10.5220/0010202903280337</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vaib2020b.jpg" width="118px" style="float: left; margin-right:19px;"/> F Sakib, T Burghardt. <strong>Visual Recognition of Great Ape Behaviours in the Wild.</strong> <em>Proc. 25th IEEE/IAPR International Conference on Pattern Recognition (ICPR) Workshop on Visual Observation and Analysis of Vertebrate And Insect Behavior (VAIB)</em>, January 2021. (<a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20fstb.pdf" target="_blank">Workshop Paper</a>), (<a href="mpi2019.txt" target="_blank">Dataset PanAfrican2019 Video</a>), (<a href="https://arxiv.org/abs/2011.10759" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/fznsakib/great-ape-behaviour-detector" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.jh6hrovynjik2ix2h7m6fdea3" target="_blank">Dataset PanAfrican2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vaib2020a.jpg" width="118px" style="float: left; margin-right:19px;"/> O Brookes, T Burghardt. <strong>A Dataset and Application for Facial Recognition of Individual Gorillas in Zoo Environments.</strong> <em>Proc. 25th IEEE/IAPR International Conference on Pattern Recognition (ICPR) Workshop on Visual Observation and Analysis of Vertebrate And Insect Behavior (VAIB)</em>, January 2021. (<a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20obtb.pdf" target="_blank">Workshop Paper</a>), (<a href="https://arxiv.org/abs/2012.04689" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/obrookes/BristolGorillas2020" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.jf0859kboy8k2ufv60dqeb2t8" target="_blank">Dataset BristolGorillas2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="accv.jpg" width="118px" style="float: left; margin-right:19px;"/> T Perrett, A Masullo, T Burghardt, M Mirmehdi, D Damen. <strong>Meta-Learning with Context-Agnostic Initialisations.</strong> <em>15th Asian Conference on Computer Vision (ACCV)</em>, Lecture Notes in Computer Science (LNCS), Vol 12625, pp. 70-86, November 2020. (<a href="https://doi.org/10.1007/978-3-030-69538-5_5" target="_blank">DOI:10.1007/978-3-030-69538-5_5</a>), (<a href="https://arxiv.org/abs/2007.14658" target="_blank">Arxiv PDF</a>), (<a href="https://openaccess.thecvf.com/content/ACCV2020/html/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.html" target="_blank">CVF Version</a>), (<a href="https://tobyperrett.github.io/contextagnosticweb" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sen2020.jpg" width="118px" style="float: left; margin-right:19px;"/> A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi. <strong>Person Re-ID by Fusion of Video Silhouettes and Wearable Signals for Home Monitoring Applications.</strong> <em> Sensors</em>, 20(9), 2576, May 2020. (<a href="http://dx.doi.org/10.3390/s20092576" target="_blank">DOI:10.3390/s20092576</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iros2019.jpg" width="118px" style="float: left; margin-right:19px;"/> W Andrew, C Greatwood, T Burghardt. <strong>Aerial Animal Biometrics: Individual Friesian Cattle Recovery and Visual Identification via an Autonomous UAV with Onboard Deep Inference.</strong><em> 32nd IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, pp. 237-243, November 2019. (<a href="http://dx.doi.org/10.1109/IROS40897.2019.8968555" target="_blank">DOI:10.1109/IROS40897.2019.8968555</a>), (<a href="https://arxiv.org/abs/1907.05310" target="_blank">Arxiv PDF</a>), (<a href="http://openaccess.thecvf.com/content_WACVW_2020/papers/w2/Andrew_Fusing_Animal_Biometrics_with_Autonomous_Robotics_Drone-based_Search_and_Individual_WACVW_2020_paper.pdf" target="_blank">CVF Extended Abstract at WACVW2020</a>), (<a href="https://vimeo.com/391197619" target="_blank">Video Summary</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cvpm2019.jpg" width="118px" style="float: left; margin-right:19px;"/> A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi.<strong> Who Goes There? Exploiting Silhouettes and Wearable Signals for Subject Identification in Multi-Person Environments.</strong><em> 2nd International Workshop on Computer Vision for Physiological Measurement (CVPM) at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 1599-1607, October 2019. (<a href="http://dx.doi.org/10.1109/ICCVW.2019.00199" target="_blank">DOI:10.1109/ICCVW.2019.00199</a>), (<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVPM/Masullo_Who_Goes_There_Exploiting_Silhouettes_and_Wearable_Signals_for_Subject_ICCVW_2019_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iccv2019.jpg" width="118px" style="float: left; margin-right:19px;"/> X Yang, M Mirmehdi, T Burghardt.<strong> Great Ape Detection in Challenging Jungle Camera Trap Footage via Attention-Based Spatial and Temporal Feature Blending.</strong><em> Computer Vision for Wildlife Conservation (CVWC) Workshop at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 255-262, October 2019. (<a href="http://dx.doi.org/10.1109/ICCVW.2019.00034" target="_blank">DOI:10.1109/ICCVW.2019.00034</a>), (<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVWC/Yang_Great_Ape_Detection_in_Challenging_Jungle_Camera_Trap_Footage_via_ICCVW_2019_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/1908.11240" target="_blank">Arxiv PDF</a>), (<a href="mpi2019.txt" target="_blank">Dataset PanAfrican2019 Video</a>), (<a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">Dataset PanAfrican2019 Annotations and Code</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iciap2019.jpg" width="118px" style="float: left; margin-right:19px;"/> V Ponce-Lopez, T Burghardt, Y Sun, S Hannuna, D Damen, M Mirmehdi.<strong> Deep Compact Person Re-Identification with Distractor Synthesis via Guided DC-GANs.</strong><em> 20th International Conference on Image Analysis and Processing (ICIAP)</em>,  pp. 488-498, September 2019. (<a href="http://dx.doi.org/10.1007/978-3-030-30642-7_44" target="_blank">DOI:10.1007/978-3-030-30642-7_44</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bits.jpg" width="118px" style="float: left; margin-right:19px;"/> J Green, T Burghardt, E Oswald.<strong> Not a Free Lunch but a Cheap Lunch: Experimental Results for Training Many Neural Nets.</strong><em> IACR Cryptology ePrint Archive</em>, paper 1068, September 2019. (<a href="https://eprint.iacr.org/2019/1068.pdf" target="_blank">ePrint PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iciar19.jpg" width="118px" style="float: left; margin-right:19px;"/> A Massulo, T Burghardt, T Perrett, D Damen, M Mirmehdi.<strong> Sit-to-Stand Analysis in the Wild using Silhouettes for Longitudinal Health Monitoring.</strong><em> 16th International Conference on Image Analysis and Recognition (ICIAR)</em>, pp. 175-185, August 2019.(<a href="http://dx.doi.org/10.1007/978-3-030-27272-2_15" target="_blank">DOI:10.1007/978-3-030-27272-2_15</a>), (<a href="https://arxiv.org/abs/1910.01370" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="gcpr.jpg" width="118px" style="float: left; margin-right:19px;"/> R Smith, T Burghardt.<strong> DeepKey: Towards End-to-End Physical Key Replication From a Single Photograph.</strong><em> 40th German Conference on Pattern Recognition (GCPR)</em>, pp. 487-502, October 2018. (<a href="https://doi.org/10.1007/978-3-030-12939-2_34" target="_blank">DOI:10.1007/978-3-030-12939-2_34</a>), (<a href="https://arxiv.org/pdf/1811.01405" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iros18.jpg" width="118px" style="float: left; margin-right:19px;"/> W Andrew, C Greatwood, T Burghardt.<strong> Deep Learning for Exploration and Recovery of Uncharted and Dynamic Targets from UAV-like Vision.</strong><em> 31st IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, pp. 1124-1131, October 2018. (<a href="https://doi.org/10.1109/IROS.2018.8593751" target="_blank">DOI:10.1109/IROS.2018.8593751</a>), (<a href="https://ieeexplore.ieee.org/document/8593751" target="_blank">IEEE Version</a>), (<a href="https://doi.org/10.5523/bris.2zot65rxlmgqq23au92qwkaa3x" target="_blank">Dataset GTRF2018</a>), (<a href="https://vimeo.com/280747562" target="_blank">Video Summary</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc18.jpg" width="118px" style="float: left; margin-right:19px;"/> A Massulo, T Burghardt, D Damen, S Hannuna, V Ponce-Lopez, M Mirmehdi.<strong> CaloriNet: From Silhouettes to Calorie Estimation in Private Environments.</strong><em> 29th British Machine Vision Conference (BMVC)</em>, 108, BMVA Press, September 2018. (<a href="http://bmvc2018.org/contents/papers/0383.pdf" target="_blank">BMVA Version</a>), (<a href="https://arxiv.org/pdf/1806.08152" target="_blank">Arxiv PDF</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>), (<a href="https://github.com/ale152/CaloriNet" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="piceccv.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> V Ponce-Lopez, T Burghardt, S Hannuna, D Damen, A Masullo, M Mirmehdi.<strong> Semantically Selective Augmentation for Deep Compact Person Re-Identification.</strong><em> Person in Context (PIC) Workshop at European Conference of Computer Vision (ECCVW)</em>, pp. 551-561, September 2018. (<a href="https://doi.org/10.1007/978-3-030-11012-3_41" target="_blank">DOI:10.1007/978-3-030-11012-3_41</a>), (<a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Ponce-Lopez_Semantically_Selective_Augmentation_for_Deep_Compact_Person_Re-Identification_ECCVW_2018_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/pdf/1806.04074" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> A Elsts, T Burghardt, D Byrne, D Damen, X Fafoutis, S Hannuna, V Ponce-Lopez, A Masullo, M Mirmehdi, G Oikonomou, R Piechocki, E Tonkin, A Vafeas, P Woznowski, I Craddock.<strong> A Guide to the SPHERE 100 Homes Study Dataset.</strong><em> Technical Report</em>, May 2018. (<a href="https://arxiv.org/pdf/1805.11907.pdf" target="_blank">Arxiv PDF</a>), (<a href="https://www.irc-sphere.ac.uk/100-homes-sensors" target="_blank">Sensor Overview</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="caldnn.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> B Wang, L Tao, T Burghardt, M Mirmehdi.<strong> Calorific Expenditure Estimation using Deep Convolutional Network Features.</strong><em> Computer Vision for Active and Assisted Living Workshop (CV-AAL) at IEEE Winter Conference on Applications of Computer Vision (WACVW)</em>, March 2018. (<a href="https://doi.org/10.1109/WACVW.2018.00014" target="_blank">DOI:10.1109/WACVW.2018.00014</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ietsi.jpg" width="118px" style="float: left; margin-right:19px;"/> T Burghardt, RB Fisher, S Ravela (editors). <strong>Computer Vision for Animal Biometrics.</strong> <em> IET Computer Vision - Special Issue</em>, Vol 12, Issue 2, pp.119-120, March 2018. (<a href="http://dx.doi.org/10.1049/iet-cvi.2018.0019" target="_blank">DOI:10.1049/iet-cvi.2018.0019</a>), (<a href="http://digital-library.theiet.org/content/journals/iet-cvi/12/2" target="_blank">IET Online Version</a>), (<a href="http://digital-library.theiet.org/deliver/fulltext/iet-cvi/12/2/IET-CVI.2018.0019.pdf" target="_blank">Editorial PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ietcalorie.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock.<strong> Energy Expenditure Estimation using Visual and Inertial Sensors.</strong><em> IET Computer Vision, Special Section: Computer Vision in Healthcare and Assisted Living</em>, Vol 12, Issue 1, pp. 36-47, February 2018. (<a href="http://dx.doi.org/10.1049/iet-cvi.2017.0112" target="_blank">DOI:10.1049/iet-cvi.2017.0112</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iccvw.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> CA Brust, T Burghardt, M Groenenberg, C Kaeding, HS Kuehl, M Manguette, J Denzler.<strong> Towards Automated Visual Monitoring of Individual Gorillas in the Wild.</strong><em> Visual Wildlife Monitoring (VWM) Workshop at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 2820-2830, October 2017. (<a href="https://doi.org/10.1109/ICCVW.2017.333" target="_blank">DOI:10.1109/ICCVW.2017.333</a>), (<a href="gorilla2017.txt" target="_blank">Dataset Gorilla2017</a>), (<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w41/Brust_Towards_Automated_Visual_ICCV_2017_paper.pdf" target="_blank">CVF Version</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vwm2017.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/>W Andrew, C Greatwood, T Burghardt.<strong> Visual Localisation and Individual Identification of Holstein Friesian Cattle via Deep Learning.</strong><em> Visual Wildlife Monitoring (VWM) Workshop at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 2850-2859, October 2017. (<a href="https://doi.org/10.1109/ICCVW.2017.336" target="_blank">DOI:10.1109/ICCVW.2017.336</a>), (<a href="https://doi.org/10.5523/bris.2yizcfbkuv4352pzc32n54371r" target="_blank">Dataset FriesianCattle2017</a>), (<a href="https://doi.org/10.5523/bris.3owflku95bxsx24643cybxu3qh" target="_blank">Dataset AerialCattle2017</a>), (<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w41/Andrew_Visual_Localisation_and_ICCV_2017_paper.pdf" target="_blank">CVF Version</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="dagstuhl.jpg" width="118px" style="float: left; margin-right:19px;"/> G Camps-Valls, T Hickler, B Koenig-Ries (editors).<strong> Computer Science meets Ecology</strong> (Dagstuhl Seminar 17091). In: Dagstuhl Reports</em>, Vol 7, No 2, pp. 109-134, Leibniz-Zentrum fuer Informatik, September 2017. (<a href="http://dx.doi.org/10.4230/DagRep.7.2.109" target="_blank">DOI:10.4230/DagRep.7.2.109</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="kcf.jpg" width="118px" style="float: left; margin-right:19px;"/> M Camplani, A Paiment, M Mirmehdi, D Damen, S Hannuna, T Burghardt, L Tao.<strong> Multiple Human Tracking in RGB-D Data: A Survey.</strong> <em>IET Computer Vision.</em> Vol 11, No 4, pp. 265-285, ISSN: 1751-9632. June 2017. (<a href="http://dx.doi.org/10.1049/iet-cvi.2016.0178">DOI:10.1049/iet-cvi.2016.0178</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sharks.jpg" width="118px" style="float: left; margin-right:19px;"/> B Hughes, T Burghardt.<strong> Automated Visual Fin Identification of Individual Great White Sharks.</strong><em> International Journal of Computer Vision (IJCV)</em>, Vol 122, No 3, pp. 542-557, May 2017. (<a href="http://dx.doi.org/10.1007/s11263-016-0961-y" target="_blank">DOI:10.1007/s11263-016-0961-y</a>), (<a href="finscholl2456.txt" target="_blank">Dataset FinsScholl2456</a>)</td>
</tr><tr><td><hr/></td></tr>
 <tr>
   <td align="left" valign="top" width="100%" style=" text-align: justify;">
   <img src="ajp.jpg" width="118px" style="float: left; margin-right:19px;"/> AS Crunchant, M Egerer, A Loos, T Burghardt, K Zuberbuehler, K Corogenes, V Leinert, L Kulik, HS Kuehl.<strong> Automated Face Detection for Occurrence and Occupancy Estimation in Chimpanzees.</strong> <em>American Journal of Primatology.</em> Vol 79, Issue 3, ISSN: 1098-2345. March 2017. (<a href="http://dx.doi.org/10.1002/ajp.22627" target="_blank">DOI 10.1002/ajp.22627</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> PR Woznowski, A Burrows, T Diethe, X Fafoutis, J Hall, S Hannuna, M Camplani, N Twomey, M Kozlowski, B Tan, N Zhu, A Elsts, A Vafeas, A Paiement, L Tao, M Mirmehdi, T Burghardt, D Damen, P Flach, R Piechocki, I Craddock, G Oikonomou.<strong> SPHERE: A Sensor Platform for Healthcare in a Residential Environment.</strong> <em>Designing, Developing, and Facilitating Smart Cities: Urban Design to IoT Solutions.</em> (V Angelakis, E Tragos, HC Poehls, A Kapovits, A Bassi (eds.)), Springer, pp. 315-333, January 2017. (<a href="http://dx.doi.org/10.1007/978-3-319-44924-1_14">DOI:10.1007/978-3-319-44924-1_14</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="calorie.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock.<strong> Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at Home.</strong><em> Lecture Notes in Computer Science (LNCS)</em>, Vol 10116, pp. 239-251, November 2016. (<a href="https://doi.org/10.1007/978-3-319-54407-6">DOI:10.1007/978-3-319-54407-6</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="kcf.jpg" width="118px" style="float: left; margin-right:19px;"/> S Hannuna, M Camplani, J Hall, M Mirmehdi, D Damen, T Burghardt, A Paiment, L Tao.<strong> DS-KCF: A real-time tracker for RGB-D data. </strong><em>Journal of Real-Time Image Processing</em>, Vol 16, No 5, pp. 1439-1458, November 2016. (<a href="http://dx.doi.org/10.1007/s11554-016-0654-3" target="_blank">DOI 10.1007/s11554-016-0654-3</a>), (<a href="https://doi.org/10.5523/bris.f8amc39n5f751ekgp2kbws1hb" target="_blank">Rotational Dataset</a>), (<a href="https://doi.org/10.5523/bris.1mddmf1o6f54d18zgr2jvahs8p" target="_blank">Code Download</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> J Hall, M Camplani, S Hannuna, M Mirmehdi, L Tao, D Damen, T Burghardt, A Paiment.<strong> Designing a Video Monitoring System for AAL applications: The SPHERE Case Study.</strong><em> IET International Conference on Technologies for Active and Assisted Living (TechAAL).</em> October 2016. (<a href="http://dx.doi.org/10.1049/ic.2016.0061" target="_blank">DOI:10.1049/ic.2016.0061</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock.<strong> Real-time Estimation of Physical Activity Intensity for Daily Living.</strong><em> IET International Conference on Technologies for Active and Assisted Living (TechAAL).</em> October 2016. (<a href="http://dx.doi.org/10.1049/ic.2016.0060" target="_blank">DOI:10.1049/ic.2016.0060</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="icip2016.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> W Andrew, S Hannuna, N Campbell, T Burghardt.<strong> Automatic Individual Holstein Friesian Cattle Identification via Selective Local Coat Pattern Matching in RGB-D Imagery.</strong> <em>IEEE International Conference on Image Processing (ICIP)</em>, pp. 484-488, ISBN: 978-1-4673-9961-6, September 2016. (<a href="http://dx.doi.org/10.1109/ICIP.2016.7532404" target="_blank">DOI:10.1109/ICIP.2016.7532404</a>), (<a href=" http://dx.doi.org/10.5523/bris.wurzq71kfm561ljahbwjhx9n3" target="_blank">Dataset FriesianCattle2015</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> L Tao, A Paiment, D Damen, M Mirmehdi, S Hannuna, M Camplani, T Burghardt, I Craddock.<strong> A Comparative Study of Pose Representation and Dynamics Modelling for Online Motion Quality Assessment.</strong> <em>Computer Vision and Image Understanding</em>, Vol 148, pp. 136-152, Elsevier, May 2016. (<a href="http://authors.elsevier.com/sd/article/S107731421500260X" target="_blank">DOI:10.1016/j.cviu.2015.11.016</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="ehealth2015.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, S Hannuna, M Camplani, A Paiement, D Damen, M Mirmehdi, I Craddock.<strong> A Comparative Home Activity Monitoring Study using Visual and Inertial Sensors.</strong><em> IEEE 17th International Conference on eHealth Networking, Applications and Services. </em>pp. 644-647, October 2015. (<a href="https://dx.doi.org/10.1109/HealthCom.2015.7454583" target="_blank">DOI:10.1109/HealthCom.2015.7454583</a>), (<a target="_blank" href="https://doi.org/10.5523/bris.zs8naru512g01gn93wy76kp6q">Dataset SPHERE_H130</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="icip2015.jpg" width="118px" style="float: left; margin-right:19px;"/> D Gibson, T Burghardt, N Campbell, N Canagarajah. <strong>Towards Automating Visual In-Situ Monitoring of Crops Health</strong>. <em>IEEE International Conference on Image Processing (ICIP)</em>, pp. 3906 - 3910, September 2015. (<a href="http://dx.doi.org/10.1109/ICIP.2015.7351537" target="_blank">DOI:10.1109/ICIP.2015.7351537</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc2015.jpg" width="118px" style="float: left; margin-right:19px;"/> B Hughes, T Burghardt.<strong> Automated Identification of Individual Great White Sharks from Unrestricted Fin Imagery.</strong> <em>26th British Machine Vision Conference (BMVC)</em>, pp. 92.1-92.14, ISBN 1-901725-53-7, BMVA Press, September 2015. (<a href="https://dx.doi.org/10.5244/C.29.92" target="_blank">DOI:10.5244/C.29.92</a>), (<a href="http://www.bmva.org/bmvc/2015/papers/paper092/sup092.zip">Dataset FinsScholl2456</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="kcf.jpg" width="118px" style="float: left; margin-right:19px;"/> M Camplani, S Hannuna, D Damen, M Mirmehdi, A Paiement, T Burghardt, L Tao.<span id="ctl00_cph_SubmissionSummary_TitleText"><strong> Robust Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters</strong></span><strong>. </strong><em>26th British Machine Vision Conference (BMVC)</em>, pp. 145.1-145.11, ISBN 1-901725-53-7, BMVA Press, September 2015. (<a href="https://dx.doi.org/10.5244/C.29.145" target="_blank">DOI:10.5244/C.29.145</a>), (<a href="http://data.bris.ac.uk/data/dataset/16vbnj3im1ygi1sh0yd0mt4lp0">Code</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="mvab2015.jpg" width="118px" style="float: left; margin-right:19px;"/> B Hughes, T Burghardt.<strong> Affinity Matting for Pixel-accurate Fin Shape Recovery from Great White Shark Imagery.</strong> <em>Machine Vision of Animals and their Behaviour (MVAB), Workshop at BMVC</em>, pp. 8.1-8.8. BMVA Press, September 2015. (<a href="http://www.bmva.org/bmvc/2015/mvab/papers/paper008/index.html" target="_blank">DOI:10.5244/CW.29.MVAB.8</a>), (<a href="http://www.bmva.org/bmvc/2015/mvab/papers/paper008/sup008.zip">Dataset FinsScholl2456</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ijcv.jpg" width="118px" style="float: left; margin-right:19px;"/> T Burghardt, D Damen, W Mayol-Cuevas, M Mirmehdi (editors). <strong>Correspondence, Matching and Recognition</strong>. <em>International Journal of Computer Vision (IJCV)</em>, Vol 113, Issue 3, pp. 161-162, ISSN 0920-5691, Springer, June 2015. (<a href="http://link.springer.com/article/10.1007/s11263-015-0827-8" target="_blank">DOI:10.1007/s11263-015-0827-8</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> P Woznowski, F Fafoutis, T Song, S Hannuna, M Camplani, L Tao, A Paiement, E Mellios, M Haghighi, N Zhu, G Hilton, D Damen, T Burghardt, M Mirmehdi, R Piechocki, D Kaleshi, I Craddock. <strong>A Multi-modal Sensor Infrastructure for Healthcare in Residential Environment</strong>. <em>IEEE ICC Workshop on ICT-enabled Services and Technologies for eHealth and Ambient Assisted Living (ICCW)</em>, pp. 271-277, June 2015. (<a href="http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?arnumber=7247190">DOI:10.1109/ICCW.2015.7247190</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="visapp2015.jpg" width="118px" style="float: left; margin-right:19px;"/> L Palmer, T Burghardt. <strong>Contextual Saliency for Nonrigid Landmark Registration and
    Recognition of Natural Patterns</strong>. <em>International Conference on Computer Vision Theory and Applications (VISAPP)</em>, pp. 403-410, ISBN: <span id="ContentPlaceHolder1_PublicationsDetailsPage_PublicationsDetailsContent_ISBN">978-989-758-089-5</span>, March 2015.
    (<a href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005268604030410" target="_blank">DOI:10.5220/0005268604030410</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="tree2013.jpg" width="118px" style="float: left; margin-right:19px;"/> HS Kuehl, T Burghardt. <strong>Fractal Representation and Recognition for Animal Biometrics: A Reply to Jovani et al</strong>. <em>Trends in Ecology and Evolution, </em>Vol 28, No 9, pp. 500-501, September 2013. (<a href="http://dx.doi.org/10.1016/j.tree.2013.06.007" target="_blank">DOI:10.1016/j.tree.2013.06.007</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc2013.jpg" width="118px" style="float: left; margin-right:19px;"/> T Burghardt, D Damen, W Mayol-Cuevas, M Mirmehdi (editors). <strong>Proceedings of the 24th British Machine Vision Conference, BMVC2013</strong>. <em>British Machine Vision Association (BMVA)</em>, <a href="http://www.bmva.org/bmvc/2013/" target="_blank">ISBN 1-901725-49-9</a>, BMVA Press. September 2013.</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvw.jpg" width="118px" style="float: left; margin-right:19px;"/> RC Sandwell, A Loos, T Burghardt. <strong>Synthesising Unseen Image Conditions to Enhance Classification Accuracy for Sparse Datasets: Applied to Chimpanzee Face Recognition. </strong><em>British Machine Vision Workshop (BMVW)</em>, BMVA Press, September 2013. (BMVW: <a href="http://www.bmva.org/bmvc/2013/Workshop/0003.pdf" target="_blank">ISBN 1-901725-50-2</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="tree.jpg" width="118px" style="float: left; margin-right:19px;"/> HS Kuehl, T Burghardt. <strong>Animal Biometrics: Quantifying and Detecting Phenotypic Appearance</strong>. <em>Trends in Ecology and Evolution</em>, Vol 28, No 7, pp. 432-441, July 2013.<br />
    (<a href="http://dx.doi.org/10.1016/j.tree.2013.02.013" target="_blank">DOI:10.1016/j.tree.2013.02.013</a>)</td>
</tr><tr><td><hr/></td></tr>
</table>
<div align="center">For older papers please see <a href='https://scholar.google.com/citations?user=lqM244QAAAAJ&hl=en' target='_blank' title='Go to Google Scholar Profile'>Google Scholar</a>.<br/><br/><hr/>
<br/>(Links: 
          <a href="https://www.bris.ac.uk/mystudents" target="_blank">MyS</a>, 
          <a href="http://evision.apps.bris.ac.uk" target="_blank">eV</a>, 
          <a href="http://wwwa.fen.bris.ac.uk/2014-5/coms" target="_blank">S</a>, 
          <a href="https://www.ole.bris.ac.uk/webapps/bb-auth-provider-cas-bb_bb60/execute/casLogin?cmd=login&authProviderId=_122_1&redirectUrl=https%3A%2F%2Fwww.ole.bris.ac.uk">BB</a>, 
          <a href="http://www.bristol.ac.uk/timetables" target="_blank">TT</a>, 
          <a href="https://research-information.bris.ac.uk/pure" target="_blank">Pu</a>, 
          <a href="http://www.bristol.ac.uk/fec" target="_blank">fEC</a>, 
          <a href="http://encrypted.google.com" target="_blank">G</a>,
          <a href="https://uob.sharepoint.com/sites/staff" target="_blank">I</a>,
          <a href="https://uob.sharepoint.com/sites/engineering" target="_blank">E</a>, 
          <a href="https://webcentre.askadmissions.co.uk" target="_blank">A</a>)<br/><br/><br/>
</div>
</div>
<script> myResize(); myResize(); myResize();</script>
</body>
</html>
