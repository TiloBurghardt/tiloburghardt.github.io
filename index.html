<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-gb" xml:lang="en-gb" style="overflow-x:hidden;" onscroll = "myScroll()"; onresize="myResize()";>
<head>
    <title>Tilo Burghardt | University of Bristol | School of Computer Science</title>
    <base target="_top"></base>
    <meta name="description" content="Tilo Burghardt - Homepage"/>
    <meta name="ROBOTS" content="INDEX, FOLLOW"/>
    <meta name="Keywords" content="tilo burghardt, Tilo Burghardt, tilo, Tilo, burghardt, Burghardt, computer vision, animal biometrics, bristol, conservation, wildlife, imageomics, machine learning, artificial intelligence, drones"/>
    <meta HTTP-EQUIV="Keywords" CONTENT="tilo burghardt, tilo, burghardt, computer vision, animal biometrics, bristol"/>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
    <meta content="Tilo Burghardt, University of Bristol" name="DCTERMS.creator" />
    <meta content="en-GB" name="DCTERMS.language" />
    <meta content="2020-02-20" name="DCTERMS.modified" />
    <meta content="2013-07-15" name="DCTERMS.created" />
    <meta content="text/html" name="DCTERMS.format" />
    <meta content="University of Bristol" name="DCTERMS.publisher" />
    <meta content="http://www.bristol.ac.uk/university/web/terms-conditions.html" name="DCTERMS.rights" />
    <meta name="viewport" content="width=device-width, initial-scale=1;user-scalable=0;maximum-scale=1;">
    <link rel="icon" href="http://www.bristol.ac.uk/favicon.gif" type="image/gif" />
    <link rel="stylesheet" type="text/css" media="screen" href="uobcms_corporate.css" />
</head>


<style>

/* Position the image container (needed to position the left and right arrows) */
.container {
  position: relative;
}

/* Hide the images by default */
.mySlides {
  display: none;
  left: -9px;
}

</style>
<script>
 function goToUni()    { window.open("https://www.bristol.ac.uk"); }
 function getTitle()   { return "Homepage of Tilo Burghardt"; }
 function getTeach()   { return "Current Teaching in Computer Science"; }
 function getRes()     { return "Computer Vision for the Life Sciences"; }
 function getTQ()      { return "<br/><br/><b>TEACHING</b><hr/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td><font color='#FFFFFF'><b>What is our teaching about?</b><hr/>'[Our] learning experience encourages students to become independent thinkers, equipped to respond to the intellectual, social, and personal challenges they encounter throughout their lives and careers and become leaders in their chosen fields.' [UoB Website]</td></tr></table>"; }
 function getVision()  { return "<br/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td><font color='#FFFFFF'><b>What is Video Representation Learning?</b><hr/>'Video representation learning is a group of algorithmic techniques that allows for an automatic discovery of representations of semantically relevant features at different levels of abstraction from raw video data. This approach replaces manual feature selection and enables so-called end-to-end system engineering. In effect, features are learned together with higher level semantic task solutions. Note, that this subject is a research area within computer vision, which is in itself an interdisciplinary domain in computer science that focuses on generating a high-level understanding based on digital images, videos, and other structured visual data.'</td></tr></table>"; }
 function getSphere()  { return "<br/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td><font color='#FFFFFF'><b>What is the Sphere Project?</b><hr/>'SPHERE is a community of nearly 100 researchers [...] using a unique platform of sensors to quantify health-related behaviours over long periods to diagnose and help manage health and wellbeing conditions. The technology will aid early diagnosis, lifestyle change and the ability of patients to live at home while maintaining their privacy and independence. [...] SPHERE has been named a World Technology Award Winner in the Organisation: Health and Medicine Catagory by the World Technology Network ('The WTN') - a global community comprised of the most innovative people and organisations at the forefront of science, technology and related fields.' [see <a href='https://www.irc-sphere.ac.uk/' target='_blank'><font color='#FFFFFF'><u>SPHERE Website</u></font></a>]</font></td></tr></table>"; }
 function getAB()      { return "<br/><br/><b>RESEARCH</b><hr/><table cellspacing='28px' style='text-align: justify; background-color: #1e4f85'><tr><td><font color='#FFFFFF'><b>What is Animal Biometrics?</b><hr/>'Animal biometrics is an emerging field that develops quantified approaches for representing and detecting the phenotypic appearance of species, individuals, behaviors, and morphological traits. It operates at the intersection between pattern recognition, ecology, and information sciences, producing computerized systems for phenotypic measurement and interpretation. Animal biometrics can benefit a wide range of disciplines, including biogeography, population ecology, and behavioral research. (...) However, to advance animal biometrics will require integration of methodologies among the scientific disciplines involved. Such efforts will be worthwhile because the great potential of this approach rests with the formal abstraction of phenomics, to create tractable interfaces between different organizational levels of life.' [cited from <a href='http://dx.doi.org/10.1016/j.tree.2013.02.013' target='_blank'><font color='#FFFFFF'><u>TREE2013</u></font></a>]</font></td></tr></table>"; }
 function getAddress() { return "School of Computer Science<br/>University of Bristol<br/>MVB3.42, Woodland Rd<br/>BS8 1UB, UK<br/>&nbsp;<br/>+44 (0) 117 954 5298<br/>tilo (...AT...) cs.bris.ac.uk"}
 function getSummary() { return "<b>About</b><hr/>Dr Tilo Burghardt is a computer science academic who researches animal biometrics and computer vision methods for the life sciences. His work focuses on using artificial intelligence and robotics to support biodiversity, sustainability, and animal welfare. His interdisciplinary research links computing with ecology, conservation, taxonomics, healthcare, animal husbandry, and sustainable farming. He was one of the first researchers to monitor animals in their natural habitat via automated real-time computer vision methods in the early 2000s. Over the past two decades he made contributions to establishing animal biometrics as an emerging cross-discipline linked to imageomics and routed in pattern recognition and artificial intelligence. Tilo's enthusiasm for computer science and computer vision is reflected in his dedication to teaching the subject. He has received the University of Bristol 'Award for Education' for his educational contributions to the Engineering Faculty.<br/><br/><b>Brief Biography</b><hr/>Tilo graduated with Distinction in Media Computing (Bakk. Medien-Inf.) at Dresden University of Technology in Germany. Subsequently, he received an MSc in Advanced Computing and a PhD in Computer Vision from the University of Bristol in the United Kingdom. After initial post-doctoral research at the School of Physics, he was awarded a Fellowship of the Research Councils UK and tenure as a Lecturer, Senior Lecturer, and now Associate Professor in Computer Science. He is member of the Visual Information Laboratory and the Intelligent Systems Laboratory at the University of Bristol. He is currently School Education Director at the School of Computer Science at Bristol. Tilo is Associate Editor of IET Computer Vision. He is a Fellow of the Higher Education Academy (HEA) and a member of the German Academic Foundation (Studienstiftung des Deutschen Volkes). He has contributed to the British Machine Vision Association (BMVA) as chair of the 24th British Machine Vision Conference (BMVC). His profile is on <a href='https://uk.linkedin.com/in/tilo-burghardt-0a9a37340' target='_blank' title='Go to LinkedIn Profile'>LinkedIn</a> and his scientific papers are listed on this website and in full on <a href='https://scholar.google.com/citations?user=lqM244QAAAAJ&hl=en' target='_blank' title='Go to Google Scholar Profile'>Google Scholar</a>."}
 function getTeaching() { return "<b>Teaching Profile and Outreach</b><hr/>Tilo Burghardt is currently UG <a href='http://www.bristol.ac.uk/engineering/departments/computerscience/contact' target='_blank'>CS Programme Director</a> at Bristol University where he coordinates the curriculum for students. During his teaching career, he taught across more than 15 different teaching units, many of which he designed and evolved. He supervised more than 70 successful BSc, MSc, or MEng final year thesis projects. He regularly teaches various units at undergraduate and postgraduate level and leads several teaching teams as unit director. He has been passionate about innovating regarding the delivery and evolution of computer science higher education so that students enjoy a challenging, timely, and not least lasting learning experience. He pioneered innovative teaching tools at Bristol, including: the use of board games to teach programming, the use of multi-core hardware kits for team-based practical work on concurrency; the extensive use of dual lecturing to increase student engagement; and collaborations with professional partners, industry, and the arts inside and outside of the university to broaden the CS teaching horizon in project units and enhance learning outcomes. Tilo Burghardt is a member of both the <a href='https://www.bristol.ac.uk/engineering/research/eerg' target='_blank'>Engineering Education Research Group (EERG)</a> and the CS Student-Staff Liaison Committee (SSLC). He is a Fellow of the Higher Education Academy (HEA). He also significantly contributed to science outreach. Tilo Burghardt lead a multi-university exhibit at the <a href='https://royalsociety.org' target='_blank'>Royal Society Summer Science Exhibition</a>, lectured in the <a href='http://www.bristol.ac.uk/bilt/opportunities-and-celebration/best-of-bristol-lectures/best-of-bristol-lecturers' target='_blank'>'Best of Bristol'</a> public lecture series, and contributed to the <a href='https://www.imdb.com/title/tt1351033/fullcredits'>Royal Institution Christmas Lectures</a> on Digital Intelligence. His outreach activities produced features on mainstream <a href='https://www.imdb.com/title/tt9593920' target='_blank'>BBC National TV</a> and articles in the popular science press including the <a href='https://www.newscientist.com/article/2108232-auto-finprinting-identifies-individual-sharks-as-they-migrate' target=''>New Scientist</a> that highlight the importance of using computer vision to study and monitor animals and the natural world. Tilo Burghardt has been presented with multiple teaching awards by the Computer Science Student Society and the Dept of Computer Science. In 2018 he received the University of Bristol <a href='https://www.bristol.ac.uk/news/2018/june/teaching-awards.html' target='_blank'>'Award for Education'</a> in Engineering, an accolade of the institution for outstanding educational contributions."}
 function getResearch() {return "<b>General Research Profile</b><hr/>Tilo Burghardt co-authored more than 60 peer-reviewed publications across biometrics, computer vision, machine learning, robotics, and associated subjects. His publications have an i10-index above 34, an h-index above 25, and overall more than 2,000 citations according to <a href='' target='_blank'>Google Scholar</a>. He has published in leading computer vision, artificial intelligence, and robotics venues such as IJCV, CVPR, and IROS, but also in inter-disciplinary science journals such as TREE and Nature Communications. He has been principal investigator (PI) or co-investigator (Co-I) of multiple research grants awarded by governmental research councils such as UK EPSRC, MSCA, and by leading industry. He is a long-standing member of the <a href='https://vilab.blogs.bristol.ac.uk/people' target='_blank'>Visual Information Laboratory</a> and formerly the Intelligent Systems Laboratory at the University of Bristol. He is also part of the panel of supervisors for <a href='https://www.bristol.ac.uk/cdt/interactive-ai' target='_blank'>Interactive-AI CDT</a> and for <a href='https://www.farscope.bris.ac.uk/college-of-supervisors-2' target='_blank'>FARSCOPE Robotics</a> at Bristol. He regularly follows invitations to act as internal or external PhD assessor. He regularly serves on programme committees of international conferences and workshops. He chaired the <a href='http://www.bmva.org/bmvc/2013' target='_blank'>24th British Machine Vision Conference</a> (BMVC). He is a team member of the <a href='https://secure.wtn.net/summit-2016/__winners' target='_blank'>WTN World Technology Award-winning 'SPHERE' Project</a> reserach team, which focusses on data collection, fusion, as well as on the biometric and medical interpretation of large-scale sensor deployments in order to establish digital health capabilities in home environments. He co-invented a patented visual identification system for plastics (<a href='https://patents.google.com/patent/WO2012020263A1/en' target='_blank'>Patent WO2012020263A1</a>) based on physically unclonable functions. His main research contribution, however, relates to the field of visual biometrics for animals, an emerging cross-discipline where he contributed to fundamental terminology and approaches. He also pioneered and evolved various approaches in this field including the earliest prototype attempts of automated real-time video animal identification (2004) and behaviour recognition (2006), the first fully automated contour-based visual animal identification system (2017), and recently the first deep learning driven aerial robotic platform to perform animal biometrics fully autonomously (2019). His animal biometrics research actively contributes to national and international science and engineering collaborations such as <a href='https://www.4d-reef.eu/consortium' target='_blank'>REEF4D</a>, <a href='https://ammod.de/partner' target='_blank'>AMMOD</a>, and <a href='https://www.sdu.dk/en/forskning/sduuascenter/news/wilddrone-announcement' target='_blank'>WildDrone</a> to help fight the biodiversity crisis. He is also affiliated with the sustainable farming initiative <a href='http://www.bristol.ac.uk/vet-school/research/john-oldacre-centre' target='_blank'>'John Oldacre Centre'</a> and collaborates with the <a href='https://bristolzoo.org.uk/zoo-information/about-us/what-we-do' target='_blank'>Bristol Zoological Society</a> to facilitate animal welfare and conservation. He is actively engaged in <a href='https://www.nature.com/articles/s41467-022-27980-y'>subject+community building</a> and organises the <a href='https://camtrapai.github.io'>CamTrapAI Workshops</a> community. For a full citation and publication overview see <a href='https://scholar.google.com/citations?user=lqM244QAAAAJ&hl=en' target='_blank' title='Go to Google Scholar Profile'>Google Scholar</a>."}
</script>
<body onscroll = "myScroll()"; onresize="myResize()"; style="margin: 0; min-width: 1280px; max-width: 100%; background-color: #FFFFFF; background-size: cover; padding: 0px; overflow-x:hidden; width:100%;">

   <!--NAVIGATION-->
<div class="uob-mainnav-container clearfix" id="top">
  <div id="csnav">
  <ul class="width-master uob-mainnav" role="navigation" id="csmenu" style="width:100%; margin-left:0;">
     <li class="u-m-university"> <img style="cursor: pointer;" src="uob.png" height="35px" width="60px" onclick="javascript:goToUni()"/></li>
     <li class="u-m-university"> <a class="u-m-link" href="javascript:currentSlide(1)" tabindex="0"> Home </a></li>
     <li class="u-m-university"> <a class="u-m-link" href="javascript:currentSlide(2)" tabindex="0"> Research </a> </li>
     <li class="u-m-university"> <a class="u-m-link" href="javascript:currentSlide(3)" tabindex="0"> Teaching </a> </li>
     <li class="u-m-university"> <a class="u-m-link" id="linkTeam" href="#team" tabindex="0"> Team </a> </li>
     <li id="menupap" class="u-m-university"> <a class="u-m-link" href="#papers" onclick="javascript:currentSlide(1)" tabindex="0"> Papers+Event</a> </li>
  </ul>
  </div>
</div>

<div class="container">
  <img id="topup" src="top.png" onclick="javascript:goToTop()" style="cursor: pointer; width:32px; height:32px; margin-left:7px; position: absolute">

  <div class="mySlides">
    <div class="myFlip">
      <img src="introsimple.gif" style="width:100%; height:320px; position: absolute">
      <img src="intro.gif" style="width:100%; height:320px; position: absolute">
      <div style="position: absolute">
        <a href="https://research-information.bris.ac.uk/en/persons/tilo-burghardt" z-index: 999; target="_blank" title="Go to University Researcher Profile">
           <img src="Tilo_Burghardt.jpg" style="width:132px; margin-top:92px; margin-left:59px">
        </a>
        <div style="width:90%; margin-left:59px; margin-top:-254px; color: #FFFFFF; font-size: 27pt; font-family: 'Didot', serif;">
          <script>document.write(getTitle());</script>
        </div>
        <div style="width:90%; margin-left:220px; margin-top:16px; color: #FFFFFF; font-size: 12pt; font-family: 'Helvetica', sans-serif;">
          <script>document.write(getAddress());</script>
        </div>
        <a href="https://scholar.google.com/citations?user=lqM244QAAAAJ&hl=en" z-index: 999; target="_blank" title="Go to Google Scholar Profile">
          <img src="scholar.png" alt="Scholar" id="myScholar" style="width:48px; position: absolute; margin-top:-48px;" />
        </a>
        <a href="https://dblp.org/pid/157/3612.html" z-index: 999; target="_blank" title="Go to DBLP Profile">
          <img src="dblp.png" alt="DBLP" id="myDBLP" style="width:48px; position: absolute; margin-top:-118px;" />
        </a>
        <a href="https://paperswithcode.com/search?q=author%3ATilo+Burghardt" z-index: 999; target="_blank" title="Go to Papers with Code">
          <img src="pwd.png" alt="PWC" id="myPWC" style="width:44px; position: absolute; margin-top:-188px;" />
        </a>
      </div>
    </div>

    <div class="myFlip">
      <img src="introsimple.gif" style="width:100%; height:256px; position: absolute">
      <img src="intro.gif" style="width:100%; height:256px; position: absolute">
      <div style="position: absolute">
         <img src="Tilo_Burghardt.jpg" style="width:124px; margin-top:60px; margin-left:50px">
        <div style="position: absolute; margin-left:50px; margin-top:-219px; color: #FFFFFF; font-size: 16pt; font-family: 'Didot', serif;">
          <script>document.write(getTitle());</script>
        </div>
        <div style="margin-left:194px; margin-top:-176px; color: #FFFFFF; font-size: 11pt; font-family: 'Helvetica', sans-serif;">
          <script>document.write(getAddress());</script>
        </div>
        <a href="https://scholar.google.com/citations?user=lqM244QAAAAJ&hl=en" z-index: 999; target="_blank" title="Go to Google Scholar Profile">
          <img src="scholar.png" alt="Scholar" id="myScholarSmall" style="width:32px; position: absolute; margin-top:-74px;" />
        </a>
        <a href="https://dblp.org/pid/157/3612.html" z-index: 999; target="_blank" title="Go to DBLP Profile">
          <img src="dblp.png" alt="DBLP" id="myDBLPSmall" style="width:32px; position: absolute; margin-top:-116px;" />
        </a>
        <a href="https://paperswithcode.com/search?q=author%3ATilo+Burghardt" z-index: 999; target="_blank" title="Go to Papers with Code">
          <img src="pwd.png" alt="PWC" id="myPWCSmall" style="width:28px; position: absolute; margin-top:-153px;" />
        </a>
      </div>
    </div>

  </div>
  </div>

  <div class="mySlides">

    <div id="myResearch">
      <img src="introsimple.gif" style="width:100%; height:320px; position: absolute"><img src="intro.gif" style="width:100%; height:320px; position: absolute">
      <div style="position: absolute">
        <img src="research.gif" style="width:132px; margin-top:92px; margin-left:59px">
        <div style="width:70%; margin-left:59px; margin-top:-254px; color: #FFFFFF; font-size: 27pt; font-family: 'Didot', serif;">
          <script>document.write(getRes());</script>
        </div>
        <div id="myResearchHook"style="width:70%; margin-left:209px; margin-top:16px; color: #FFFFFF; font-size: 12pt; font-family: 'Helvetica', sans-serif;">
        </div><div style="margin-top:-53px;">&nbsp;</div>
    </div></div>

    <div id="myResearchSmall">
      <img src="introsimple.gif" style="width:100%; height:256px; position: absolute"><img src="intro.gif" style="width:100%; height:256px; position: absolute">
      <div style="position: absolute">
         <img src="research.gif" style="width:124px; margin-top:60px; margin-left:50px">
        <div style="position: absolute; margin-left:50px; margin-top:-219px; color: #FFFFFF; font-size: 16pt; font-family: 'Didot', serif;">
          <script>document.write(getRes());</script>
        </div>
        <div id="myResearchHookSmall" style="margin-left:194px; margin-top:-176px; color: #FFFFFF; font-size: 11pt; font-family: 'Helvetica', sans-serif;">
        </div><div style="margin-top:-50px;">&nbsp;</div>
    </div></div>

</div>
  <div class="mySlides">

    <div id="myTeaching">
      <img src="introsimple.gif" style="width:100%; height:320px; position: absolute"><img src="intro.gif" style="width:100%; height:320px; position: absolute">
      <div style="position: absolute">
        <img src="award.jpg" style="width:132px; margin-top:92px; margin-left:59px">
        <div style="width:100%; margin-left:59px; margin-top:-254px; color: #FFFFFF; font-size: 27pt; font-family: 'Didot', serif;">
          <script>document.write(getTeach());</script>
        </div>
        <div id="myTeachingHook" style="width:70%; margin-left:209px; margin-top:16px; color: #FFFFFF; font-size: 12pt; font-family: 'Helvetica', sans-serif;">
        </div>
    </div></div>

    <div id="myTeachingSmall">
      <img src="introsimple.gif" style="width:100%; height:256px; position: absolute"><img src="intro.gif" style="width:100%; height:256px; position: absolute">
      <div style="position: absolute">
         <img src="award.jpg" style="width:124px; margin-top:60px; margin-left:50px">
        <div style="position: absolute; margin-left:50px; margin-top:-219px; color: #FFFFFF; font-size: 16pt; font-family: 'Didot', serif;">
          <script>document.write(getTeach());</script>
        </div>
        <div id="myTeachingHookSmall" style="margin-left:194px; margin-top:-176px; color: #FFFFFF; font-size: 11pt; font-family: 'Helvetica', sans-serif;">
        </div><div style="margin-top:-24px;">&nbsp;</div>
    </div></div>

</div>

<div class="myBlock" id="myPC" style="margin-left:49px; margin-top:345px; text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
  <table width="100%" border="0" cellspacing="14">
    <tr>
      <td valign="top" style="background-color: #FFFFFF;">
        <div id="myLeft" width="70%" style="text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
          <script> document.write(getSummary()); </script>
          <a id="team"></a>
          <div id="myTeamHook">
          </div>
          <a id="AB"></a>
          <script> document.write(getAB()); </script>
          <div id="myABProjectsHook">
          </div>
          <a id="sphere"></a>
          <script> document.write(getSphere()); </script>
          <div id="mySPProjectsHook">
          </div>
          <a id="research"></a>
          <script> document.write(getResearch()); </script>
          <a id="teaching"></a>
          <script> document.write(getTQ()); </script>
          <script> document.write(getTeaching()); </script>
          <div id="myFinalHook">
          </div>
        </div>
      </td>
      <td valign="top">
        <div id="myMiddle"><font color="#FFFFFF">.......</font>
        </div>
      </td>
      <td valign="top" style="background-color: #FFFFFF;">
        <div id="myRight" width="30%" style="margin-top:-33px; text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
        </div>
      </td>
    </tr>
  </table>
</div>

<div class="myBlock" id="myMobile" style="margin-left:49px; margin-top:288px; text-align: justify; font-size: 13pt; font-family: 'Helvetica', sans-serif;">
  <script> document.write(getSummary()); </script>
  <a id="teamMobile"></a>
  <div id="myTeamHookSmall">
  </div>
  <a id="ABMobile"></a>
  <script> document.write(getAB()); </script>
  <div id="myABProjectsHookSmall">
  </div>
  <a id="sphereMobile"></a>
  <script> document.write(getSphere()); </script>
  <div id="mySPProjectsHookSmall">
  </div>
  <a id="researchMobile"></a>
  <script> document.write(getResearch()); </script>
  <a id="teachingMobile"></a>
  <script> document.write(getTQ()); </script>
  <script> document.write(getTeaching()); </script>
  <div id="myFinalHookSmall">
  </div>
  <a id="papers"></a>
</div>

<script>
var slideIndex = 1;
var stopRotation = 1;
var firstRun = 1;
showSlides(slideIndex);
function getNWidth() { return window.innerWidth * 0.57 - 160;}
function getScroll() { return (document.documentElement.scrollTop || document.body.scrollTop) - 30;}
function getSMWidth() { return window.innerWidth - 92;}
function getRWidth() { return window.innerWidth - 122;}
function getWidth() { return window.innerWidth - 110;}
function getLWidth() { return window.innerWidth * 0.57 - 160;}
function getReducedWidth() { return (window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth) -110;}

function goToTop() {
  document.documentElement.scrollTop = "0px";
  document.body.scrollTop = "0px";
}

function myScroll() {
    document.getElementById("topup").style.marginTop = getScroll()+"px";
}

function myResize() {
  var width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
  var i;
  var slides = document.getElementsByClassName("myFlip");
  var blocks = document.getElementsByClassName("myBlock");
  for (i = 0; i < slides.length; i++) { slides[i].style.display = "none"; }
  for (i = 0; i < blocks.length; i++) { blocks[i].style.display = "none"; }
  document.getElementById("topicSearch").style.display = "none";

  myScroll();

  if (width<1280) {
    slides[1].style.display = "block";
    blocks[1].style.display = "block";
    document.getElementById("topup").style.marginLeft = "7px";
    document.getElementById("pubstab").style.fontSize = "10pt";
    document.getElementById("linkTeam").href = "#teamMobile";
    document.getElementById("linkAB2").href = "#ABMobile";
    document.getElementById("linkSphere2").href = "#sphereMobile";
    document.getElementById("linkRes").href = "#researchMobile";
    document.getElementById("myResearchSmall").style.display = "block";
    document.getElementById("myResearch").style.display = "none";
    document.getElementById("linkTeach").href = "#teachingMobile";
    document.getElementById("myTeachingSmall").style.display = "block";
    document.getElementById("myTeaching").style.display = "none";
    document.getElementById("menupap").style.display = "block";
    document.getElementById("topicSearch").style.display = "none";
    document.getElementById("myMobile").style.width = getWidth()+"px";
    document.getElementById("myScholarSmall").style.marginLeft = getSMWidth()+"px";
    document.getElementById("myDBLPSmall").style.marginLeft = getSMWidth()+"px";
    document.getElementById("myPWCSmall").style.marginLeft = getSMWidth()+"px";
  }
  else {
    slides[0].style.display = "block";
    blocks[0].style.display = "block";
    document.getElementById("topup").style.marginLeft = "12px";
    document.getElementById("pubstab").style.fontSize = "12pt";
    document.getElementById("linkTeam").href = "#team";
    document.getElementById("linkAB2").href = "#AB";
    document.getElementById("linkSphere2").href = "#sphere";
    document.getElementById("linkRes").href = "#research";
    document.getElementById("myResearchSmall").style.display = "none";
    document.getElementById("topicSearch").style.display = "block";
    document.getElementById("myResearch").style.display = "block";
    document.getElementById("linkTeach").href = "#teaching";
    document.getElementById("myTeachingSmall").style.display = "none";
    document.getElementById("myTeaching").style.display = "block";
    document.getElementById("menupap").style.display = "none";
    document.getElementById("myPC").style.width = getWidth()+"px";
    document.getElementById("myLeft").style.width = getLWidth()+"px";
    document.getElementById("myScholar").style.marginLeft = getRWidth()+"px";
    document.getElementById("myDBLP").style.marginLeft = getRWidth()+"px";
    document.getElementById("myPWC").style.marginLeft = getRWidth()+"px";
  }

  if (firstRun>0) {
    firstRun--;
    var itm = document.getElementById("pubs");
    itm.style.display = "block";
    var cln = itm.cloneNode(true);
    document.getElementById("myMobile").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myRight").appendChild(cln);
    itm.remove();
    itm = document.getElementById("topteach");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myTeachingHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myTeachingHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("topres");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myResearchHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myResearchHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("ABprojects");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myABProjectsHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myABProjectsHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("SPprojects");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("mySPProjectsHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("mySPProjectsHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("Team");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myTeamHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myTeamHookSmall").appendChild(cln);
    itm.remove();
    itm = document.getElementById("FinalYear");
    itm.style.display = "block";
    cln = itm.cloneNode(true);
    document.getElementById("myFinalHook").appendChild(cln);
    cln = itm.cloneNode(true);
    document.getElementById("myFinalHookSmall").appendChild(cln);
    itm.remove();
  }
}

function currentSlide(n) {
  stopRotation = 1;
  showSlides(slideIndex = n);
}

function showSlides(n) {
  var i;
  var slides = document.getElementsByClassName("mySlides");
  if (n > slides.length) {slideIndex = 1}
  if (n < 1) {slideIndex = slides.length}
  for (i = 0; i < slides.length; i++) {
      slides[i].style.display = "none";
  }
  slides[slideIndex-1].style.display = "block";
  if (!stopRotation) slideIndex++;
  if (slideIndex > slides.length) {slideIndex = 1}
  if (slideIndex == 2) setTimeout(showSlides, 15000);
  else setTimeout(showSlides, 10000);
}
</script>

<div id="topteach" style="display: none; margin-left: 12px;color: #FFFFFF; font-family: 'Helvetica', sans-serif;">
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMS10016" title="Go to COMS10016 Unit Description" target="_blank"><font color="#FFFFFF"><u>COMS10016</u></font></a>: Imp &amp; Func Programming<br />
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMS10017" title="Go to COMS10017 Unit Description" target="_blank"><font color="#FFFFFF"><u>COMS10017</u></font></a>: OOP and Algorithms I<br />
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMS30043" title="Go to COMS30043 Unit Description" target="_blank"><font color="#FFFFFF"><u>COMS30043</u></font></a>: Team Project<br />
  <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=21%2F22&unitCode=COMSM0045" title="Go to COMSM0045 Unit Description"target="_blank"><font color="#FFFFFF"><u>COMSM0045</u></font></a>: Applied Deep Learning<br /><br />
  <a id="linkTeach" href="#teaching"><font color="#FFFFFF"><u>Go to Teaching Profile</u></font></a><br/>
  <a href=""><font color="#FFFFFF"><u>Go to Previous Teaching</u></font></a>      <!--      <a href=""><font color="#FFFFFF"><b>How to program well across languages?</b></font></a><br/>
  <a href=""><font color="#FFFFFF"><b>When team development meets computer games!</b></font></a><br/>
  <a href=""><font color="#FFFFFF"><b>Preparing for a career in Deep Learning?</b></font></a><br/> -->
</div>

<div id="topres" style="display: none; margin-left: 12px; color: #FFFFFF; font-family: 'Helvetica', sans-serif;">
  <table width="1009px"  border="0" cellspacing="0">
    <tr>
      <td align="left" valign="top" width="254px">
        <b>Recent Work Domains</b>:<br/>
        <a id="linkAB2" href="#AB" title="Go to Animal Biometrics" ><font color="#FFFFFF"><u>Animal Biometrics</u></font></a><br/>
        <a id="linkSphere2" href="#sphere" title="Go to the SPHERE Project" ><font color="#FFFFFF"><u>SPHERE Health Technology</u></font></a><br/>
        <a id="linkCV" href="#CV" title="Go to Computer Vision" ><font color="#FFFFFF"><u>Video Representation Learning</u></font></a><br/><br/>
        <a id="linkRes" href="#research"><font color="#FFFFFF"><u>Go to Research Profile</u></font></a><br/>
        <a href="#papers"><font color="#FFFFFF"><u>Go to Research Papers</u></font></a>
      </td>
      <td align="left" valign="top" width="600px">
      <div id="topicSearch"><font size="2pt">
        <table border="0" cellspacing="0"><tr>
        <td valign="top" width="100pt">
          BY ANIMAL<br/>
          <a href="#cattle" title="Go to Individual Friesian Cattle Identification"><font color="#FFFFFF"><u>Cattle</u></font></a><br/>
          <a href="#apes" title="Go to Great Ape Detection, Identification, and Behaviour Recognition" ><font color="#FFFFFF"><u>Chimpanzee</u></font></a><br/>
          <a href="#corals" title="Go to Deep Learning for Coral Analysis" ><font color="#FFFFFF"><u>Coral</u></font></a><br/>
          <a href="#tiny" title="Go to Enhanced Aerial Animal Detection of Elephants and other African Mammals" ><font color="#FFFFFF"><u>Elephant</u></font></a><br/>
          <a href="#forams" title="Go to Taxonomic Deep Learning for Microfossils" ><font color="#FFFFFF"><u>Foraminifer</u></font></a><br/>
          <a href="#apes" title="Go to Great Ape Detection, Identification, and Behaviour Recognition" ><font color="#FFFFFF"><u>Gorilla</u></font></a><br/>
          <a href="#sharks" title="Go to Visual Fin Identification of Great White Sharks" ><font color="#FFFFFF"><u>Shark</u></font></a>
        </td>
        <td valign="top" width="155pt">
          BY TOPIC<br/>
            <a href="#drone" title="Go to Autonomous Drones for Ecology and Farming" ><font color="#FFFFFF"><u>Autonomous Drones</u></font></a><br/>
            <a href="#tiny" title="Go to Enhanced Aerial Animal Detection of Elephants and other African Mammals" ><font color="#FFFFFF"><u>Aerial Detection</u></font></a><br/>
            <a href="#drone" title="Go to Autonomous Drones for Ecology and Farming"><font color="#FFFFFF"><u>Aerial Navigation</u></font></a><br/>
            <a href="#calorie" title="Go to Calorific Expenditure Estimation from Video" ><font color="#FFFFFF"><u>Calorie Estimation</u></font></a><br/>
            <a href="#cep" title="Go to Cycle Encoding Prediction (CEP) for Video Representation Learning" ><font color="#FFFFFF"><u>CEP Cycle Encoding</u></font></a><br/>            <a href="#cattle" title="Go to Individual Friesian Cattle Identification"><font color="#FFFFFF"><u>Individual Identification</u></font></a><br/>
            <a href="#video" title="Go to Context-Agnostic Meta-Learning"><font color="#FFFFFF"><u>Meta-Learning</u></font></a><br/>
        <td valign="top"><br/>
            <a href="#cattle" title="Go to Individual Friesian Cattle Identification"><font color="#FFFFFF"><u>Metric Deep Learning</u></font></a><br/>
            <a href="#occlusion" title="Go to Animal Detection in Challenging Scenarios" ><font color="#FFFFFF"><u>Occlusion Robustness</u></font></a><br/>
            <a href="#sharks" title="Go to Visual Fin Identification of Great White Sharks"><font color="#FFFFFF"><u>Outline Re-Identification</u></font></a><br/>
            <a href="#TRX" title="Go to Person Re-Identification"><font color="#FFFFFF"><u>Person Recognition</u></font></a><br/>
            <a href="#forams" title="Go to Taxonomic Deep Learning for Microfossils" ><font color="#FFFFFF"><u>Taxonomic Deep Learning</u></font></a><br/>
            <a href="#TRX" title="Go to Temporal-Relational CrossTransformers (TRX) for Video Representation Learning" ><font color="#FFFFFF"><u>TRX CrossTransformer</u></font></a><br/>
            <a href="#community" title="Go to Community Building in Interdisciplinary AI for Ecology, Sustainability, and Wildlife Conservation" ><font color="#FFFFFF"><u>Wildlife AI Community</u></font><br/>
        </td>
      </tr></table>
      </font></div>
</td>
</tr></table>
</div>

<div id="Team" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">
  <b>Current Research Team</b><hr/>
        <ul>
          <li><strong>R Laidlaw</strong> <em>(PhD student since 2023, w M Mirmehdi, D Morris, S Beery, T Berger-Wolf)</em></li>
          <li><strong>W Li</strong> <em>(PhD student since 2023, with M Mirmehdi)</em></li>
          <li><strong>P Yu</strong> <em>(PhD student since 2023, with N Campbell)</em></li>
          <li><strong>D Nguyen Ngoc</strong> <em>(PhD student since 2023, with M Mirmehdi, T Richardson)</em></li>
          <li><strong>A Sharma</strong> <em>(PhD student since 2021, with A Dowsey et al.)</em></li>
          <li><strong>S Zeng</strong> <em>(PhD student since 2021, with A Gambaruto)</em></li>
          <li><strong>O Brookes</strong> <em>(PhD student since 2020, with M Mirmehdi, H Kuehl)</em></li>
          <li><strong>M Xue</strong> <em>(PhD student since 2020, with M Mirmehdi)</em></li>
          <li><strong>L Bertini</strong> <em>(PhD student since 2020, with E Hendy, K Johnson)</em></li>
      </ul><br/>
      <b>Research Team Alumni</b><hr/>
      <ul>
          <li><strong>J Gao</strong> <em>(PhD 2019-2024)</em></li>
          <li><strong>T Karaderi</strong> <em>(MRes 2019-2024)</em></li>
          <li><strong>X Yang</strong> <em>(PhD 2018-2023)</em>, <a href="https://research-information.bris.ac.uk/files/369713653/main.pdf" target="_blank">PhD Thesis</a>, <a href="https://scholar.google.com/citations?hl=en&user=ElynpaEAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>A Montout</strong> <em>(PhD 2018-2023)</em>, <a href="https://research-information.bris.ac.uk/files/383648548/University_of_Bristol_Thesis_Axel.pdf" target="_blank">PhD Thesis</a></li>
          <li><strong>A Sikdar</strong> <em>(SPHERE RA 2022-2023)</em>, <a href="https://scholar.google.com/citations?hl=en&user=8R1eDrIAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>T Perrett</strong> <em>(SPHERE RA 2018-2022)</em>, <a href="https://scholar.google.com/citations?hl=en&user=eWFLwEUAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>A Masullo</strong> <em>(SPHERE RA 2017-2021)</em>, <a href="https://scholar.google.com/citations?user=J6GebsYAAAAJ&hl=en" target="_blank">GoogleScholar</a></li>
          <li><strong>W Andrew</strong> <em>(PhD/RA 2014-2020)</em>, <a href="https://research-information.bris.ac.uk/files/210500057/Final_Copy_2019_05_07_Andrew_W_PhD_Redacted.pdf" target="_blank">PhD Thesis</a>, <a href="https://scholar.google.com/citations?hl=en&user=C_cE0-sAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>P Hill</strong> <em>(RA, 2016-2018)</em>, <a href="https://scholar.google.com/citations?hl=en&user=luxEujEAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>P Anantrasirichai</strong> (RA, 2018)</em>, <a href="https://scholar.google.com/citations?user=40VEYswAAAAJ&hl=en" target="_blank">GoogleScholar</a></li>
          <li><strong>M Camplani</strong> <em>(SPHERE RA 2013-2017)</em>, <a href="https://scholar.google.com/citations?hl=en&user=7FrKNIIAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>S Hannuna</strong> <em>(SPHERE RA 2013-2017)</em>, <a href="https://scholar.google.com/citations?hl=en&user=MO5berYAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>L Tao</strong> <em>(SPHERE RA 2013-2017)</em>, <a href="https://scholar.google.com/citations?hl=en&user=8nKLPxwAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>A Paiment</strong> <em>(SPHERE RA 2013-2017)</em>, <a href="https://scholar.google.com/citations?user=LvwLwnQAAAAJ&hl=en" target="_blank">GoogleScholar</a></li>
          <li><strong>M Devereux</strong> <em>(RA 2015-2016)</em>, <a href="https://scholar.google.com/citations?hl=en&user=IxWadrQAAAAJ" target="_blank">GoogleScholar</a></li>
          <li><strong>B Hughes</strong> <em>(PhD 2010-2015)</em>, <a href="https://saveourseas.com/project-leader/benjamin-hughes" target="_blank">SaveOurSeas</a></li>
          <li><strong>R Sandwell</strong> <em>(PhD 2010-2015)</em></li>
          <li><strong>L Palmer</strong><em> (MSc by Research 2011-2014)</em>, <a href="http://mindvisionlabs.com/about" target="_blank">MindVisionLabs</a></li>
      </ul><br/>
  <b>Recent Events and Community Service</b><hr/>
        <ul>
          <li><b>The Bristol AI & Nature Week 2023:</b> (Lead Organizer)<br/> <a href="https://camtrapai.github.io/ai_nature_week.html" target="_blank">Bristol AI & Nature Week 2023 (BAINW, Mar 2023)</a></li>
          <li><b>CamTrapAI Workshop 2023:</b> (Organizing Committee)<br/> <a href="https://camtrapai.github.io" target="_blank">Camtrap Ecology Meets AI 2023 (CamTrapAI, 02/03/2023)</a></li>
          <li><b>ICPR Workshop 2022:</b> (Program Committee)<br/> <a href="http://homepages.inf.ed.ac.uk/rbf/vaib22.html" target="_blank">Workshop on Visual Observation and Analysis of Vertebrate And Insect Behavior (VAIB, 21/08/2022)</a></li>
          <li><b>CamTrapAI Workshop 2022:</b> (Organizing Committee)<br/> <a href="https://camtrapai.github.io" target="_blank">Camtrap Ecology Meets AI 2022 (CamTrapAI, Sept 2022)</a></li>
          <li><b>ICIAP Workshop 2022:</b> (Program Committee)<br/> <a href="https://lplf.github.io/" target="_blank">Learning in Precision Livestock Farming (LPLF, 23/05/2022)</a></li>
          <li><b>ICPR Workshop 2020:</b> (Program Committee)<br/> <a href="http://homepages.inf.ed.ac.uk/rbf/vaib20.html" target="_blank">Workshop on Visual Observation and Analysis of Vertebrate And Insect Behavior (VAIB, 11/01/2021)</a></li>
          <li><b>WACV Workshop 2020:</b> (Advisory Committee)<br/> <a href="https://sites.google.com/view/wacv2020animalreid" target="_blank">Workshop on Deep Learning Methods and Applications for Animal Re-Identification (DLMAAR, 01/03/2020)</a></li>
          <li><b>ICCV Workshop 2019:</b>  (Program Committee)<br/> <a href="https://cvwc2019.github.io" target="_blank">Workshop on Computer Vision for Wildlife Conservation (CVWC, 27/10/19)</a></li>
          <li><b>ICCV Workshop 2017:</b>  (Program Committee)<br/> <a href="https://www.iiitd.edu.in/~anands/ICCVw2017_VWM" target="_blank">Workshop on Visual Wildlife Monitoring (VWM, 29/10/19)</a></li>
          <li><b>Dagstuhl Seminar 2017:</b> (Seminar Member)<br/> <a href="http://dx.doi.org/10.4230/DagRep.7.2.109" target="_blank">Dagstuhl Seminar on Computer Science Meets Ecology (DAG, 2017)</a></li>
      </ul> 
</div>

<div id="FinalYear" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">
<b>Current Final Year Student Projects</b><hr/>
<ul>
        <li>Deep Learning for the Recognition of Individual Zebras and Giraffes via Deep 3D Fitting (MEng 2022, M Stennett)</li>
        <li>Real-world Smart Cattle Tracking Proof-of-Concept via Deep Learning (MEng 2022, B Dumitrescu)</li>
        <li>Deep Learning for the Accurate Video Detection of European Wildlife from Camera Traps (MEng 2022, O Suleiman)</li>
        <li>Pose Modelling of Great Apes using Deep Learning (MEng 2022, R George)</li>
        <li>Deep Learning for the Accurate Modelling of 3D Growth Bands from 3D X-Rays (MEng 2022, M Mohammed)</li>
        <li>Explainable GAN-based Synthesis of Microfossils (MEng 2022, D Salter)</li>
        <li>Individual Identification of Giraffes using Animal Biometrics (MEng 2022, H Kountouris)</li>
        <li>Coralite 3D Reconstruction using Deep Learning (BSc 2022, B Karaman)</li>
        <li>Computer Vision to Understand Predator-Prey Interactions at Sea (MSc 2023, A Gunning)</li>
        <li>Reliable Animal Detection in Underwater Footage (MSc 2023, M Yang)</li>
  </ul>
<!--
<br/><b>Previeous Student Projects</b><hr/>
<font size="2">
        <ul>Visual Recognition of Individual Giraffes via Deep Learning<br/>(BSc 2021, J Allen)</ul>
        <ul>Automatic Sky Replacement for Landscape Photographs<br/>(MSc 2021, X Huang)</ul>
        <ul>Deep Learning to Understand Coral Growth from 3D X-Rays<br/>(MEng 2021, C Uthaya-Shankar)</ul>
        <ul>Deep Learning for the Reconstruction of Coral from 3D X-Rays<br/>(MEng 2021, S Varcoe)</ul>
        <ul>Deep Learning for the Detection of European Wildlife from Camera Traps<br/>(MEng 2021, G Ioannou)</ul>
        <ul>Deep Learning for the High Quality Recognition of African Wildlife from Drone Imagery<br/>(MEng 2021, T Greenslade)</ul>
        <ul>Image Quality Classification of Manta Ray Imagery<br/>(MEng 2020L, B Fossett)</ul>
        <ul>Deep Volumetric Image Processing for Coral Analysis<br/>(MEng 2020L, A Rutterford)</ul>
        <ul>Great Ape Activity and Behaviour Recognition via Deep Learning<br/>(MEng 2020L, F Sakib)</ul>
        <ul>Deep Open Set ID of Individual Manta Rays<br/>(MEng 2020, D Scott)</ul>
        <ul>Deep End-to-end Great White Shark Fin Identification<br/>(MEng 2020, J Arneil)</ul>
        <ul>Spider Web Reconstruction using Deep Segmentation<br/>(MSc 2020, O Skeates)</ul>
        <ul>Identification of Animals in Drone Imagery<br/>(MSc, 2020, R Mapperson)</ul>
        <ul>Classifying Endless Forams<br/>(BSc 2020, J Ramaer, Reviewer M Mirmehdi)</ul>
        <ul>Auto-marking of Handwritten Linear Algebra Papers<br/>(MSc 2019, N Drake)</ul>
        <ul>Forest Canape Segmentation and Classification using RNNs<br/>(MSc 2019, I Myttas)</ul>
        <ul>Individual Gorilla Face Recognition using Deep Learning Techniques<br/>(MSc 2019, O Brookes)</ul>
        <ul>Cost Functions and Multi-Sample Learning for Deep Animal Identification<br/>(MSc 2019, X Zhang)</ul>
        <ul>Supervised vs Reward-based Learning for Search and Rescue<br/>(BSc 2019, JQ Ovalle)</ul>
        <ul>Shark Fin Segmentation and ID via Deconvolutional Networks<br/>(BSc 2019, L Zhang)</ul>
        <ul>Black and White Sport Footage Recolouration with Cycle GANs<br/>(BSc 2019, J Atton)</ul><hr/>
        <ul>Deep Learning for the Identification of Individual Manta Rays</ul>
        <ul>Deep Learning for Classifying Marine Calcareous Microfossils</ul>
        <ul>Individual Elephant Identification using DNN Biometrics</ul>
        <ul>Style Transfer via Cycle Loss using Deep Neural Networks</ul>
        <ul>Whiteboard2Website: Interactive Digital Visual Content Generation Tool</ul>
        <ul>Great Ape Detection by Motion Signature using CNNs</ul>
        <ul>CNN-based 3D Key Reconstruction from Photography</ul>
        <ul>From Spider Web Photos to Spider Web Simulations</ul>
        <ul>Ecological Prediction using Deconvolutional and Convolutional Neural Nets</ul>
        <ul>Iris Recognition using Low-cost Portable Devices</ul>
        <ul>Elephant Facial Biometrics using Convolutional Neural Networks</ul>
        <ul>Automatic Visual Detection of Speech and Pausing in Testimony Videos</ul>
        <ul>Classifying Marine Calcareous Microfossils</ul>
        <ul>Evaluating CNN Models for Animal Detection in Mobile Vision Applications</ul>
        <ul>Detecting Gorillas in Natural Images using CNNs</ul>
        <ul>LoD for Elephant-part Recognition using Hierarchical DPBMs</ul>
        <ul>CatIdentifier: Which cat is in the Photo?</ul>
        <ul>Music Score Interpretation from Sketches using a Native Mobile Platform</ul>
        <ul>Mammography Interpretation Tool using Computer Vision</ul>
        <ul>Insect Species Recognition via Combined Local and Global Features</ul>
        <ul>Enhancing AR Museum Guides Using Markerless Tracking and 3D Model Generation in a Web-Browser</ul>
        <ul>Visual Identification and Comparison of Bristol graffiti</ul>
        <ul>Object Specific Haar-like Features for Fast and Accurate Shark Fin Detection</ul>
        <ul>Salient Object Detection for Navigation of Mars-like Environments</ul>
</font>-->
</div>

<div id="ABprojects" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">

  <a id="cattle"></a>
  <b>Project: Individual Friesian Cattle Identification</b><br/><font size="2pt">(various pieces of work with J Gao, A Sharma, W Andrew, N Campbell, A Dowsey, C Greatwood, S Hannuna, S Mullan, and others in collaboration with the <a href="http://farscope.bris.ac.uk" target="_blank">Farscope CDT</a>, <a href="https://vilab.blogs.bristol.ac.uk" target="_blank">VILab</a>, and <a href="http://www.bristol.ac.uk/vetscience" target="_blank">BVS</a>)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="friesian.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> This line of work has been the first to apply visual deep learning to the task of automated individual bovine identification. Friesian cattle carry a unique black and white coat pattern. We have produced a highly accurate state-of-the-art recognition framework for this biometric entity in real-world farm environments. Methods such as metric learning and self-supervision can be utilised to improve performance and flexibility of application.<br/>
    <b><i>Keywords:</i></b> Sustainable Farming, Friesian Cattle, Metric Learning, Individual Identification, Coat Pattern Analysis, Self-Supervision, Long-term
Recurrent Convolutional Networks<br/>
    <b><i>Papers:</i></b>
         <a href="https://doi.org/10.1016/j.compag.2021.106133" target="_blank">Deep Metric Learning (CEA2021)</a>,
         <a href="https://arxiv.org/abs/2105.01938" target="_blank">Self-Supervision (CV4A2021)</a>,
         <a href="http://dx.doi.org/10.1109/IROS40897.2019.8968555" target="_blank">Autonomous Drones (IROS2019)</a>,
         <a href="https://doi.org/10.1109/ICCVW.2017.336" target="_blank">LRCN Deep Learning (ICCVW2017)</a>,
         <a href="http://dx.doi.org/10.1109/ICIP.2016.7532404" target="_blank">SIFT ID via RGB-D (ICIP2016)</a><br/>
    <b><i>Thesis:</i></b>
        <a href="https://research-information.bris.ac.uk/admin/files/210500057/Final_Copy_2019_05_07_Andrew_W_PhD_Redacted.pdf" target="_blank">Visual Biometric Processes for Collective Identification of Individual Friesian Cattle</a><br/>
    <b><i>Datasets:</i></b>
        <a href="https://doi.org/10.5523/bris.4vnrca7qw1642qlwxjadp87h7" target="_blank">Cows2021</a>,
        <a href="https://doi.org/10.5523/bris.10m32xl88x2b61zlkkgz3fml17" target="_blank">OpenCows2020</a>,
        <a href="https://doi.org/10.5523/bris.3owflku95bxsx24643cybxu3qh" target="_blank">AerialCattle2017</a>,
        <a href="https://doi.org/10.5523/bris.2yizcfbkuv4352pzc32n54371r" target="_blank">FriesianCattle2017</a>,
        <a href="https://doi.org/10.5523/bris.wurzq71kfm561ljahbwjhx9n3" target="_blank">FriesianCattle2015</a><br/>
    <b><i>Code:</i></b>
        <a href="https://github.com/CWOA/MetricLearningIdentification" target="_blank">Metric Learning for Friesian ID (GitHub)</a>,
        <a href="https://github.com/Wormgit/Cows2021" target="_blank">Self-Supervision for Friesian ID (GitHub)</a>
  </td>
  </tr></table>

  <a id="drone"></a>
  <b>Project: Autonomous Drones for Ecology and Farming</b><br/>
  <font size="2pt">(with W Andrew, C Greatwood, and others in collaboration with the <a href="http://farscope.bris.ac.uk" target="_blank">Farscope CDT</a>, <a href="https://vilab.blogs.bristol.ac.uk" target="_blank">VILab</a>, and <a href="http://www.bristol.ac.uk/vetscience" target="_blank">BVS</a>)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="drone.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> This project has delivered the first system to achieve autonomous aerial animal biometrics. We built a computationally-enhanced M100 UAV platform with an onboard deep learning inference system for integrated computer vision and navigation able to autonomously find and visually identify by coat pattern individual Holstein Friesian cattle in freely moving herds. It has been tested on a small herd in a real-world pasture setting and produced error-free identification. Our proof-of-concept system is a successful step towards autonomous biometric identification of individual animals from the air in open pasture environments for tag-less AI support in farming and ecology. <br/>
    <b><i>Keywords:</i></b> UAV Navigation, Autonomous Robotics, On-board Inference, Multi-Stream Architecture, Sustainable Farming, Friesian Cattle<br/>
    <b><i>Summary Video:</i></b>
        <a href="videodrone.html" target="_blank">IROS 2019 Video Summary</a>, <a href="videonav.html" target="_blank">IROS 2018 Video Summary</a><br/>
    <b><i>Papers:</i></b>
         <a href="http://dx.doi.org/10.1109/IROS40897.2019.8968555" target="_blank">Autonomous Drones (IROS2019)</a>,
         <a href="https://doi.org/10.1109/IROS.2018.8593751" target="_blank">Dynamic Aerial Navigation (IROS2018)</a>,
         <a href="https://doi.org/10.1109/ICCVW.2017.336" target="_blank">Deep Learning (ICCVW2017)</a><br/>
    <b><i>Datasets:</i></b>
        <a href="https://doi.org/10.5523/bris.10m32xl88x2b61zlkkgz3fml17" target="_blank">OpenCows2020</a>,
        <a href="https://doi.org/10.5523/bris.2zot65rxlmgqq23au92qwkaa3x" target="_blank">GTRF2018</a>,
        <a href="https://doi.org/10.5523/bris.3owflku95bxsx24643cybxu3qh" target="_blank">AerialCattle2017</a><br/>
    <b><i>Code:</i></b>
        <a href="https://doi.org/10.5523/bris.2zot65rxlmgqq23au92qwkaa3x" target="_blank">Grid-based Target Recovery Framework (GTRF)</a>
  </td>
  </tr></table>

  <a id="apes"></a>
  <b>Project: Great Ape Individual Identification and Behaviour Recognition</b><br/><font size="2pt">(various lines of work with O Brookes, F Sakib, M Mirmehdi, H Kuehl, CA Brust, M Groenenberg, C Kaeding, M Manguette, J Denzler, RC Sandwell, A Loos and others)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="apes.gif" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Automated monitoring of Great Apes from integrated camera trapping is a pillar for answering important research questions relating to their ecology and conservation. Gathering long-term data on the demographics, behaviours, group structure and social interactions are fundumental for understanding these charismatic creatures better. Here in Bristol we have built systems to detect Great Apes in challenging settings and individually identify them based on their facial features. We have also constructed the first systems that can automatically recognise great ape activities in video. Some facial recognition systems have been applied to thousands of camera trap videos in the wild. We are currently scaling up our work and build new international collaborations to further this line of work.<br/>
    <b><i>Keywords:</i></b> Behaviour Classification, Facial Recognition, Individual Identification, Dual-Stream Networks, Long-term
Recurrent Convolutional Networks, Spatio-Temporal Analysis, Wildlife Conservation<br/>
    <b><i>Papers:</i></b>
      <a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20fstb.pdf" target="_blank">Deep Behaviour Recognition (VAIB2021)</a>,
      <a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20obtb.pdf" target="_blank">Deep Face Recognition for Zoos (VAIB2021)</a>,
      <a href="https://doi.org/10.1109/ICCVW.2017.333" target="_blank">Deep Face Recognition in the Wild (ICCVW2017)</a>,
      <a href="http://dx.doi.org/10.1002/ajp.22627" target="_blank">Face Detection in CamTrap Data (AJP2017)</a>,
      <a href="http://www.bmva.org/bmvc/2013/Workshop/0003.pdf" target="_blank">Augmentation for Face Detection (BMVW2013)</a><br/>
    <b><i>Datasets:</i></b>
      <a href="https://doi.org/10.5523/bris.jf0859kboy8k2ufv60dqeb2t8" target="_blank">BristolGorillas2020 Videos and Annotations</a>,
      <a href="https://doi.org/10.5523/bris.jh6hrovynjik2ix2h7m6fdea3" target="_blank">PanAfrican2020 Annotations</a>,
      <a href="mpi2019.txt" target="_blank">PanAfrican2019 Videos</a>,
      <a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">PanAfrican2019 Annotations</a>,
      <a href="gorilla2017.txt" target="_blank">Gorilla2017 Videos</a><br/>
    <b><i>Code:</i></b>
      <a href="https://github.com/fznsakib/great-ape-behaviour-detector" target="_blank">Dual-Stream LRCN Behaviour Recognition (GitHub)</a>,
      <a href="https://github.com/obrookes/BristolGorillas2020" target="_blank">YOLO-based Gorilla Face Recognition (GitHub)</a>
  </td>
  </tr></table>

  <a id="occlusion"></a>
  <b>Project: Occlusion-robust Animal Detection in Challenging Scenarios</b>
  <br/><font size="2pt">(with X Yang and M Mirmehdi)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="jungle.gif" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Camera trap video data showing heavily occluded animals pose significant challenges to current computer vision methods. Yet, the analysis of biodiversity and animal population dynamics such as chimpanzees and gorillas around the globe often depends on such scenarios. We have started to address these problems using deep object detectors with techniques such as spatio-temporal feature blending which is a special form of attention-based learning. We have published our results along with all relevant key code sections.<br/>
    <b><i>Keywords:</i></b> Spatio-Temporal Feature Blending, Attention Model, ResNet-based Feature Pyramid Network<br/>
    <b><i>Paper:</i></b>
      <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVWC/Yang_Great_Ape_Detection_in_Challenging_Jungle_Camera_Trap_Footage_via_ICCVW_2019_paper.pdf" target="_blank">Challenging Great Ape Detection in Video (ICCVW2019)</a><br/>
    <b><i>Datasets:</i></b>
      <a href="mpi2019.txt" target="_blank">PanAfrican2019 Videos</a>,
      <a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">PanAfrican2019 Annotations</a><br/>
    <b><i>Code:</i></b>
      <a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">SCM and TCM Deep Detection Components</a>
  </td>
  </tr></table>

  <a id="tiny"></a>
  <b>Project: Enhanced Aerial Animal Detection of Elephants and other African Mammals</b>
  <br/><font size="2pt">(various lines of work with M Xue, T Greenslade, and M Mirmehdi)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="sr.gif" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Tiny animal visuals (a few dozen pixels) captured by high-flying aerial drones are, despite advances in deep learning, a seriously challenging application for current AI detection methods. However, census operations in wildlife parks and conservation efforts often rely on exactly such data. Our line of work has for the first time combined deep super-resolution networks with contextual metadata such as drone altitude to animal detection in order to address the issue. Our systems are evaluated on large, publically available datasets including <a href="https://zenodo.org/record/3234780" target="_blank">AED</a> and <a href="https://www.epfl.ch/labs/lasig/research/projects/savmap" target="_blank">SAVMAP</a> containing African elephants and African mammals, respectively.<br/>
    <b><i>Keywords:</i></b> Super-Resolution, Altitude Data, Holistic Attention Network<br/>
    <b><i>Paper:</i></b>
      <a href="https://arxiv.org/abs/2111.06830" target="_blank">Aerial Animal Surveillance with Super-Resolution and Altitude Data (WACVW2022)</a>
  </td>
  </tr></table>

  <a id="forams"></a>
  <b>Project: Taxonomic Deep Learning for Microfossils</b>
  <br/><font size="2pt">(with T Karaderi, AY Hsiang, J Ramaer, and DN Schmidt)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="forams.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> Identifying a species based on appearance of the phenotype can be challenging (even for seasoned taxonomists) if visual features are very similar - as is the case in some planktic microfossils. We have started to apply deep metric learning for the first time to the problem of classifying these planktic foraminifer shells on microscopic images. This species recognition task is an important information source and scientific pillar for reconstructing past climates. All foraminifer CNN recognition pipelines in the literature produce black-box classifiers that lack visualisation options for human experts and cannot be applied to open set problems. We have benchmarked metric learning against these pipelines, have produced the first scientific visualisation of the phenotypic planktic foraminifer morphology space, and demonstrated that metric learning can be used to cluster species unseen during training. Metric learning proves highly effective for this domain and can serve as an important tool towards expert-in-the-loop automation of microfossil identification. <br/>
    <b><i>Keywords:</i></b> Deep Metric Learning, Clustering, Planktic Foraminifer, Active Learning<br/>
    <b><i>Pre-print:</i></b>
      <a href="https://arxiv.org/abs/2112.09490" target="_blank">Microfossil Identification via Deep Metric Learning</a>
  </td>
  </tr></table>

    <a id="sharks"></a>
    <b>Project: Visual Fin Identification of Great White Sharks</b>
    <br/><font size="2pt"> (with B Hughes and M Scholl at the <a href="https://www.saveourseasmagazine.com/high-tech-fin-recognition" target="_blank">The SaveOurSeas Foundation</a>)</font><hr/>
    <table><tr>
    <td width="70%" style="text-align: justify;" valign="top">
      <img src="shark.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
      <b><i>Summary:</i></b> Dorsal fins of Great Whites as seen in photographs have an individually characteristic outline shape. This poses a fine-grained, multi-instance classification problem for flexible, fairly textureless and possibly partly occluded objects. Our biometric approach was the first to operate fully automatically on shark fins. It utilises classical machine learning and computer vision to identify individual animals. We exploit techniques such as ultrametric contour maps, combinatorial spectral fingerprinting, LNBNN classification, and random forest fin space construction. <a href="https://saveourseas.com/project-leader/benjamin-hughes/" target="_blank">B Hughes</a> has built further on this work and created real-world animal ID applications for the <a href="https://saveourseas.com" target="_blank">SaveOurSeas Foundation</a> and the <a href="https://www.mantatrust.org" target="_blank">Manta Trust</a>.<br/>
      <b><i>Keywords:</i></b> Great White Sharks, Contour Recognition, Individual Identification, Pattern Matching, Segmentation<br/>
      <b><i>Talks:</i></b>
        <a href="http://www.bmva.org/bmvc/2015/papers/paper092/video.m4v" target="_blank">BMVC 2015 Presentation of Early System</a>, 
        <a href="videoshark.html" target="_blank">Shark and Ray Symposium 2015 Presentation of Early System</a><br/>
      <b><i>Papers:</i></b> <a href="http://dx.doi.org/10.1007/s11263-016-0961-y" target="_blank">Full FinSpace Prototype (IJCV2017)</a>,
                         <a href="https://dx.doi.org/10.5244/C.29.92" target="_blank">LNBNN-based System (BMVC2015)</a>,
                         <a href="http://www.bmva.org/bmvc/2015/mvab/papers/paper008/index.html" target="_blank">Fin Segmentation (BMVCW2015)</a><br/>
      <b><i>Datasets:</i></b>
           <a href="finscholl2456.txt" target="_blank">FinsScholl2456</a>
    </td>
    </tr></table>

    <a id="corals"></a>
    <b>Project: Deep Learning for Coral Analysis</b>
    <br/><font size="2pt">(with L Bertini, EJ Hendy, KG Johnson, A Rutterford, and R Summerfield in collaboration with the <a href="https://www.4d-reef.eu/" target="_blank">4DReef</a> Marie Skłodowska-Curie ITN)</font><hr/>
    <table><tr>
    <td width="70%" style="text-align: justify;" valign="top">
      <img src="coral.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
      <b><i>Summary:</i></b> X-ray micro–computed tomography is increasingly used to record the skeletal structure of corals. We are developing artificial intelligence approaches to assist the expert identification in small colonies of massive Porites spanning decades. For example, annual skeletal banding unlocks information about growth and calcification critical for an understanding of ecology and conservation questions. We recommend the development of a community platform to share annotated images for AI and have started to put this into practice.<br/>
      <b><i>Keywords:</i></b> Deep Learning, UNet Architectures, Segmentation, Coral Ecology<br/>
      <b><i>Paper:</i></b>
          <a href="https://doi.org/10.1007/s42452-021-04912-x" target="_blank">Coral Skeletal Density-banding using Deep Learning (SNAS2022)</a><br/>
      <b><i>Code:</i></b>
          <a href="https://github.com/ainsleyrutterford/deep-learning-coral-analysis" target="_blank">UNet-based Banding Recognition (GitHub)</a>
    </td>
    </tr></table>

  <a id="community"></a>
  <b>Community Building: Interdisciplinary AI for Ecology, Sustainability, and Wildlife Conservation</b><br/><font size="2pt">(work across various aspects with international reserachers including H Kuehl, P Bodesheim, J Denzler, D Tuia, B Kellenberger, S Beery, BR Costelloe, S Zuffi, B Risse, A Mathis, MW Mathis, F van Langevelde, R Kays, H Klinck, M Wikelski, ID Couzin, G van Horn, MC Crofoot, CV Stewart, T Berger-Wolf, RB Fisher and others)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="sensors.jpg" border="1" width="35%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> We are part of a growing international community of engineers and computer scientists who believe that deep learning approaches can be integrated into ecological workflows to improve inputs for conservation and behaviour research. Data acquisition in animal ecology is rapidly accelerating building on inexpensive and readily available sensor data. This holds great potential for large-scale environmental monitoring and understanding. This approach will require decades of work and further close collaboration as well as cross-disciplinary education between the computer science and animal ecology communities. We will need to train a new generation of data scientists who work in ecology, conservation, biodiversity and sustainability sectors. Everything starts with coming together to meet this challenge - <a href="https://www.bristol.ac.uk/engineering/departments/computerscience/courses/postgraduate/" target="_blank">join us</a>!<br/>
    <b><i>Keywords:</i></b> International Community, Biodiversity, Sustainability, Conservation<br/>
    <b><i>Papers:</i></b>
         <a href="https://arxiv.org/abs/2110.12951" target="_blank">Seeing Biodiversity: Machine Learning for Conservation (2022)</a>,
         <a href="http://dx.doi.org/10.1049/iet-cvi.2018.0019" target="_blank">Computer Vision for Animal Biometrics (IETCV2018)</a>,
         <a href="http://dx.doi.org/10.4230/DagRep.7.2.109" target="_blank">Computer Science meets Ecology (DAGSTUHL2017)</a>,
         <a href="http://dx.doi.org/10.1016/j.tree.2013.02.013" target="_blank">Animal Biometrics (TREE2013)</a>
         <br/>
  </td>
  </tr></table>

</div>

<div id="SPprojects" style="display: none; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">

  <a id="calorie"></a>
  <b>Project: Calorific Expenditure Estimation from Video</b>
  <br/><font size="2pt">(various lines of work with A Masullo, L Tao, T Perrett, B Wang, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock and carried out as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
  <table><tr>
  <td width="70%" style="text-align: justify;" valign="top">
    <img src="cal.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
    <b><i>Summary:</i></b> This line of work has shown that the estimation of calorific expenditure from video and inertial sensors can be implemented reliably, even when applied in privacy-enhanced environments where RGB video is replaced with sihouette data only. We have experimented with classical machine learning and deep learning solutions, combining various input modalities from visual, depth, and inertial sensors. The work has been validated with a gold-standard gas exchange calorimeter, that is a portable metabolic measurement system of type COSMED K4b2.<br/>
    <b><i>Keywords:</i></b> Deep Learning, Calorific Expenditure Estimation, Motion Analysis, Inertial Sensors, RGB-D<br/>
    <b><i>Papers:</i></b>
      <a href="http://bmvc2018.org/contents/papers/0383.pdf" target="_blank">Inertial+Silhouette CalorieNet System (BMVC2018)</a>,
      <a href="https://doi.org/10.1109/WACVW.2018.00014" target="_blank">Deep Learning (WACVW2018)</a>,
      <a href="http://dx.doi.org/10.1049/iet-cvi.2017.0112" target="_blank">Inertial+Visual System (IETCV2018)</a>,
      <a href="https://doi.org/10.1007/978-3-319-54407-6" target="_blank">RGB-D Calorie Counter (LNCS2016)</a><br/>
    <b><i>Datasets:</i></b>
      <a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">SPHERE-Calorie</a><br/>
      <b><i>Code:</i></b>
        <a href="https://github.com/ale152/CaloriNet" target="_blank">CalorieNet (GitHub)</a>
  </td>
</tr></table>

<a id="TRX"></a>
<b>Project: Person Re-Identification via Deep Learning</b>
<br/><font size="2pt">(various lines of work with A Masullo, T Perrett, V Ponce-Lopez, Y Sun, D Damen, and M Mirmehdi as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="id.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> The identification of individuals from visual, distance, and inertial sensors is critical for enabling individualised services including health technology and assisted living settings. In these works we explored various deep learning techniques in order to re-identify individuals including more private settings where only silhouette and/or inertial data is available.<br/>
  <b><i>Keywords:</i></b> Deep Learning, Person Identification, Private Environments, Silhouette Data, Inertial Sensors, RGB-D<br/>
  <b><i>Papers:</i></b>
    <a href="https://www.doi.org/10.5220/0010202903280337" target="_blank">Multi-Sensory Fusion in Real-World Settings (VISAPP2021)</a>,
    <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVPM/Masullo_Who_Goes_There_Exploiting_Silhouettes_and_Wearable_Signals_for_Subject_ICCVW_2019_paper.pdf" target="_blank">Silhoutte and Wearable Re-ID (SENSORS2021)</a>,
    <a href="http://dx.doi.org/10.1007/978-3-030-30642-7_44" target="_blank">Guided DC-GANs for Re-ID (ICIAP2019)</a>,
    <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Ponce-Lopez_Semantically_Selective_Augmentation_for_Deep_Compact_Person_Re-Identification_ECCVW_2018_paper.pdf" target="_blank">Selective Augmentation for Re-ID (ICIAP2019)</a>
    <br/>
 </td>
</tr></table>

<a id="CV"></a>
<script> document.write(getVision());</script>

<a id="video"></a>
<b>Project: Context-Agnostic Meta-Learning</b>
<br/><font size="2pt">(with T Perrett, A Masullo, M Mirmehdi, D Damen as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="meta.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> In the literature, meta-learning approaches have addressed few-shot problems by finding initialisations suited for fine-tuning to target tasks. Yet, often there are additional properties within training data (the context), not relevant to the target task, which can act as a distractor to meta-learning. We address this oversight by incorporating a context-adversarial component into the meta-learning process. This produces an initialisation which is both context-agnostic and task-generalised. We evaluate our approach on three commonly used meta-learning algorithms and four case studies including calorie estimation data.</b> .<br/>
  <b><i>Keywords:</i></b> Meta-Learning, Context, Calorific Expenditure Estimation<br/>
  <b><i>Paper:</i></b>
    <a href="https://doi.org/10.1007/978-3-030-69538-5_5" target="_blank">Context-Agnostic Meta-Learning (ACCV2021)</a><br/>
  <b><i>Datasets:</i></b>
    <a href="https://tobyperrett.github.io/contextagnosticweb" target="_blank">Website with Splits for Mini-ImageNet and CUB</a>,
    <a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">SPHERE-Calorie</a><br/>
</td>
</tr></table>

<a id="TRX"></a>
<b>Project: Temporal-Relational CrossTransformers (TRX) for Video Representation Learning</b>
<br/><font size="2pt">(work with T Perrett, A Masullo, M Mirmehdi, D Damen as part of the <a href="https://www.irc-sphere.ac.uk/" target="_blank">SPHERE</a> project)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="trct.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> Temporal-Relational CrossTransformers (TRX) observe relevant sub-sequences of all support videos, rather than  class averages or single best matches. This approach achieved state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51, and UCF101 at publication. Importantly, the TRX method outperformed prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations.<br/>
  <b><i>Keywords:</i></b> Transformers, Contrastive Learning, Pre-Training, Action Recognition<br/>
  <b><i>Paper:</i></b>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Perrett_Temporal-Relational_CrossTransformers_for_Few-Shot_Action_Recognition_CVPR_2021_paper.pdf" target="_blank">Temporal-Relational Cross-Transformers (TRX) (CVPR2021)</a><br/>
  <b><i>Code:</i></b>
    <a href="https://github.com/tobyperrett/trx" target="_blank">Temporal-Relational CrossTransformers (GitHub)</a>
</td>
</tr></table>

<a id="cep"></a>
<b>Project: Cycle Encoding Prediction (CEP) for Video Representation Learning</b>
<br/><font size="2pt">(with X Yang and M Mirmehdi)</font><hr/>
<table><tr>
<td width="70%" style="text-align: justify;" valign="top">
  <img src="cep.jpg" border="1" width="35%" height="18%" style="float: left; margin-right:19px;">
  <b><i>Summary:</i></b> We show that learning video feature spaces in which temporal cycles are maximally predictable benefits action classification. In particular, we propose a novel learning approach termed Cycle Encoding Prediction (CEP) that is able to effectively represent high-level spatio-temporal structure of unlabelled video content. CEP builds a latent space wherein the concept of closed forward-backward as well as backward-forward temporal loops is approximately preserved. As a self-supervision signal, CEP leverages the bi-directional temporal coherence of the video stream and applies loss functions that encourage both temporal cycle closure as well as contrastive feature separation. We also show that using CEP as an add-on may improve the performance of existing architectures.<br/>
  <b><i>Keywords:</i></b> Deep Learning, Temporal Cycle Learning, Self-Supervision, Action Recognition<br/>
  <b><i>Paper:</i></b>
    <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0399.pdf" target="_blank">CEP Self-Supervision (BMVC2021)</a><br/>
  <b><i>Code:</i></b>
    <a href="https://github.com/youshyee/CEP" target="_blank">Cycle Encoding Prediction (GitHub)</a>
</td>
</tr></table>
</div>

<div id="pubs" style="width:100%; display: none; vertical-align: top; margin-top: 32px; text-align: left; font-family: 'Helvetica', sans-serif;">
<div style="vertical-align: top;">
<b>Current Key Events and Project Links</b><hr/>
<table id="pubstab" width="100%" border="0" cellspacing="10" style="text-align: justify; margin-top: -24px; font-family: 'Helvetica', sans-serif; font-size:12pt;" >
  <tr><td></td></tr>
  <tr><td> 
    <a href="https://camtrapai.github.io/ai_nature_week.html" alt="The Bristol AI and Nature Week" target="_blank"><img border="1" src="ainature.png" height="76px" /></a><b style="color:white">..</b>
    <a href="https://camtrapai.github.io" alt="Ecology meets AI Workshop" target="_blank"><img border="1" src="ecologyai.png" height="76px"/></a><b style="color:white">..</b>
    <a href="https://wilddrone.eu" alt="Wilddrone.eu Project" target="_blank"><img border="1" src="wilddrone.png" height="76px"/></a>
  </td></tr>
</table>

<b>Latest Papers and Publications</b><hr/>
<table id="pubstab" width="100%" border="0" cellspacing="10" style="text-align: justify; margin-top: -24px; font-family: 'Helvetica', sans-serif; font-size:12pt;" >
  <tr><td></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="PanAfFGBG.png" border="1" width="118px" style="float: left; margin-right:19px;"/> O Brookes, M Kukushkin, M Mirmehdi, C Stephens, P Dieguez, TC Hicks, S Jones, K Lee, MS McCarthy, A Meier, E Normand, EG Wessling, RM Wittig, K Langergraber, K Zuberbuehler, L Boesch, T Schmid, M Arandjelovic, H Kuehl, T Burghardt. <strong>The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition.</strong> Preprint. <em>39th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> 2025. (<a href="https://arxiv.org/pdf/2502.21201" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="panaf.png" border="1" width="118px" style="float: left; margin-right:19px;"/> AHH Chan, O Brookes, U Waldmann, H Naik, ID Couzin, M Mirmehdi, NA Houa, E Normand, C Boesch, L Boesch, M Arandjelovic, HS Kuehl, T Burghardt, F Kano. <strong>Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology.</strong> <em>38th IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)</em>, June 2025.</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="eVLM2025.png" border="1" width="118px" style="float: left; margin-right:19px;"/> C Zeng, Y Jiang, F Zhang, AM Gambaruto, T Burghardt. <strong>Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation.</strong> Preprint. <em></em> 2025. (<a href="https://arxiv.org/abs/2504.02351" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="wildlive2025.png" border="1" width="118px" style="float: left; margin-right:19px;"/> NN Dat, T Richardson, M Watson, K Meier, J Kline, S Reid, G Maalouf, D Hine, M Mirmehdi, T Burghardt. <strong>WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs.</strong> Preprint. <em>38th IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)</em>, June 2025. (<a href="https://arxiv.org/abs/2504.10165" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="MedPPP2025.png" border="1" width="118px" style="float: left; margin-right:19px;"/> C Zeng, D Smithard, AM Gambaruto, T Burghardt. <strong>Tuning Vision Foundation Model via Test-Time Prompt-Guided Training for VFSS Segmentations.</strong> Preprint. <em></em> 2025. (<a href="https://arxiv.org/pdf/2501.18474" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="CV4A2025.png" border="1" width="118px" style="float: left; margin-right:19px;"/> J Kline, S Stevens, G Maalouf, CR Saint-Jean, DN Ngoc, M Mirmehdi, D Guerin, T Burghardt, E Pastucha, B Costelloe, M Watson, T Richardson, UP Schultz Lundquist. <strong>MMLA:Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset.</strong> Preprint. <em>38th IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)</em>, June 2025. (<a href="https://arxiv.org/abs/2504.07744" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="multicow24.png" border="1" width="118px" style="float: left; margin-right:19px;"/> P Xu, T Burghardt, AW Dowsey, NW Campbell. <strong>Holstein-Friesian Re-Identification using Multiple Cameras and Self-Supervision on a Working Farm.</strong> Preprint. <em></em> 2025. (<a href="https://arxiv.org/abs/2410.12695" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="RBF2.png" border="1" width="118px" style="float: left; margin-right:19px;"/> C Zeng, T Burghardt, AM Gambaruto. <strong>Feature Mapping in Physics-Informed Neural Networks (PINNs).</strong> Preprint. <em></em> 2025. (<a href="https://arxiv.org/abs/2402.06955" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cat.png" border="1" width="118px" style="float: left; margin-right:19px;"/> A Montout, E Maniaki, T Burghardt, M Hezzell, E Blackwell, AW Dowsey. <strong>Accelerometer-derived Classifiers for Early Detection of Degenerative Joint Disease in Cats.</strong> In Press. <em>The Veterinary Journal</em>, 2025. (<a href="https://doi.org/10.1016/j.tvjl.2025.106352" target="_blank">DOI:10.1016/j.tvjl.2025.106352</a>), (<a href="https://www.biorxiv.org/content/biorxiv/early/2024/12/17/2024.12.13.628330.full.pdf" target="_blank">BioRxiv PDF</a>), (<a href="https://www.bristol.ac.uk/vet-school/research/projects/cats/" target="_blank">The Bristol Cats Study</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ubi.png" border="1" width="118px" style="float: left; margin-right:19px;"/> A Sharma, L Randewich, W Andrew, S Hannuna, N Campbell, S Mullan, AW Dowsey, M Smith, M Hansen, T Burghardt. <strong>Universal Bovine Identification via Depth Data and Deep Metric Learning.</strong> <em>Computers and Electronics in Agriculture</em>, Vol 229, pp. 109657, ISSN 0168-1699, Elsevier, February 2025. (<a href="https://doi.org/10.1016/j.compag.2024.109657" target="_blank">DOI:10.1016/j.compag.2024.109657</a>), (<a href="https://arxiv.org/abs/2404.00172" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="KABR.png" border="1" width="118px" style="float: left; margin-right:19px;"/> M Kholiavchenko, J Kline, M Kukushkin, O Brookes, S Stevens, I Duporge, A Sheets, RR Babu, N Banergji, E Campolongo, M Thompson, N Van Tiel, J Miliko, E Bessa, M Mirmehdi, T Schmid, T Berger-Wolf, DI Rubenstein, T Burghardt, CV Stewart. <strong>Deep Dive into KABR: A Dataset for Understanding Ungulate Behaviour from In-Situ Drone Video.</strong> <em>Multimedia Tools and Applications</em>, pp. 1-20, Springer, December 2024. (<a href="https://doi.org/10.1007/s11042-024-20512-4" target="_blank">DOI:10.1007/s11042-024-20512-4</a>), (<a href="https://drive.google.com/file/d/1gAsmIs6MVXYjaC03SE_1Pn46jkXceY2F/view" target="_blank">Preprint</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="CV4A2024.png" border="1" width="118px" style="float: left; margin-right:19px;"/> O Brookes, M Mirmehdi, H Kuhl, T Burghardt. <strong>ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition.</strong> <em>38th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)</em>, June 2024. (<a href="https://drive.google.com/file/d/1baTu4UJt-SuVacGzVejF8OL0qlWEmIxS/view" target="_blank">CVPR Workshop PDF</a>), (<a href="https://arxiv.org/abs/2404.08937" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="RBF.png" border="1" width="118px" style="float: left; margin-right:19px;"/> C Zeng, T Burghardt, AM Gambaruto. <strong>RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks.</strong> <em>12th International Conference on Learning Representations (ICLR) Workshop on Artificial Intelligence for Differential Equations (AI4DE) in Science</em>, pp. 1-22, May 2024. (<a href="https://openreview.net/attachment?id=NCr1AdxThs&name=pdf" target="_blank">ICLR Workshop PDF</a>), (<a href="https://arxiv.org/abs/2402.08367" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="panaf.png" border="1" width="118px" style="float: left; margin-right:19px;"/> O Brookes, M Mirmehdi, C Stephens, S Angedakin, K Corogenes, D Dowd, P Dieguez, TC Hicks, S Jones, K Lee, V Leinert, J Lapuente, MS McCarthy, A Meier, M Murai, E Normand, V Vergnes, EG Wessling, RM Wittig, K Langergraber, N Maldonado, X Yang, K Zuberbuhler, C Boesch, M Arandjelovic, H Kuhl, T Burghardt. <strong>PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition.</strong> <em>International Journal of Computer Vision (IJCV).</em> March 2024. (<a href="https://doi.org/10.1007/s11263-024-02003-z" target="_blank">DOI:10.1007/s11263-024-02003-z</a>), (<a href="https://arxiv.org/abs/2401.13554" target="_blank">Arxiv PDF</a>), (<a href="https://doi.org/10.5523/bris.1h73erszj3ckn2qjwm4sqmr2wt" target="_blank">Dataset DOI:10.5523/bris.1h73erszj3ckn2qjwm4sqmr2wt</a>), (<a href="https://obrookes.github.io/panaf.github.io" target="_blank">Website</a>), (<a href="https://data.bris.ac.uk/datasets/1h73erszj3ckn2qjwm4sqmr2wt" target="_blank">Dataset Explorer</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="visgen23.png" border="1" width="118px" style="float: left; margin-right:19px;"/> T Karaderi, T Burghardt, R Morard, DN Schmidt. <strong>Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species.</strong> <em>24th IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, pp. 7115-7125, January 2024. (<a href="https://openaccess.thecvf.com/content/WACV2024/papers/Karaderi_Deep_Visual-Genetic_Biometrics_for_Taxonomic_Classification_of_Rare_Species_WACV_2024_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/2305.06695" target="_blank">Arxiv PDF</a>), (<a href="http://endlessforams.org" target="_blank">Dataset</a>), (<a href="https://github.com/TayfunKaraderi/WACV-2024---Deep-Visual-Genetic-Biometrics-for-Taxonomic-Classification-of-Rare-Species" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="acc.png" border="1" width="118px" style="float: left; margin-right:19px;"/> A Montout, RS Bhamber, DS Lange, DZ Ndlovu, ER Morgan, CC Ioannou, TH Terrill, JA van Wyk, T Burghardt, AW Dowsey. <strong>Early Prediction of Declining Health in Small Ruminants with Accelerometers and Machine Learning.</strong> Preprint. 2024. (<a href="https://doi.org/10.1101/2020.08.03.234203" target="_blank">BioRxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="halu2022.jpg" border="1" width="118px" style="float: left; margin-right:19px;"/> A Masullo, T Perrett, T Burghardt, I Craddock, D Damen, M Mirmehdi. <strong>Inertial Hallucinations - When Wearable Inertial Devices Start Seeing Things.</strong> Preprint. 2024. (<a href="https://arxiv.org/abs/2207.06789" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ICIP23.jpg" border="1" width="118px" style="float: left; margin-right:19px;"/> C Zeng, X Yang, D Smithard, M Mirmehdi, AM Gambaruto, T Burghardt. <strong>Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation.</strong> <em>30th International Conference on Image Processing (ICIP)</em>, ISBN: 978-1-7281-9835-4, pp. 2470-2474, October 2023. (<a href="https://doi.org/10.1109/ICIP49359.2023.10222212" target="_blank">DOI:10.1109/ICIP49359.2023.10222212</a>), (<a href="https://arxiv.org/abs/2302.11325" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/SimonZeng7108/Video-SwinUNet" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="camtrapai23.jpg" border="1" width="118px" style="float: left; margin-right:19px;"/> K Zhang, M Yang, SDJ Lang, AM McInnes, RB Sherley, T Burghardt. <strong>Diving with Penguins: Detecting Penguins and their Prey in Animal-borne Underwater Videos via Deep Learning.</strong> <em>3rd International Workshop on Camera Traps, AI and Ecology</em>, pp. 1-4, September 2023. (<a href="https://camtrapai.github.io/" target="_blank">Workshop Website</a>), (<a href="https://arxiv.org/abs/2308.07267" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/Kejia928/PenguinProject" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cvpr23.jpg" width="118px" style="float: left; margin-right:19px;"/> T Perrett, S Sinha, T Burghardt, M Mirmehdi, D Damen. <strong>Use Your Head: Improving Long-Tail Video Recognition.</strong> <em>36th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2023. (<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Perrett_Use_Your_Head_Improving_Long-Tail_Video_Recognition_CVPR_2023_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/2304.01143" target="_blank">Arxiv PDF</a>), (<a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Perrett_Use_Your_Head_CVPR_2023_supplemental.pdf" target="_blank">Supplementary</a>), (<a href="https://github.com/tobyperrett/lmr-release" target="_blank">GitHub</a>), (<a href="https://www.youtube.com/watch?v=TXEMh99Ukmg" target="_blank">Video Summary</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="VISAPP2023.jpg" width="118px" style="float: left; margin-right:19px;"/> O Brookes, M Mirmehdi, H Kuehl, T Burghardt. <strong>Triple-stream Deep Metric Learning of Great Ape Behavioural Actions.</strong> <em>18th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP)</em>, Vol 5, pp. 294-302, February 2023. (<a href="https://doi.org/10.5220/0011798400003417" target="_blank">DOI:10.5220/0011798400003417</a>), (<a href="https://arxiv.org/abs/2301.02642" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="DCL2022.jpg" width="118px" style="float: left; margin-right:19px;"/> X Yang, T Burghardt, M Mirmehdi. <strong>Dynamic Curriculum Learning for Great Ape Detection in the Wild.</strong> <em>International Journal of Computer Vision (IJCV)</em>, January 2023. (<a href="https://doi.org/10.1007/s11263-023-01748-3" target="_blank">DOI:10.1007/s11263-023-01748-3</a>), (<a href="https://arxiv.org/abs/2205.00275" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/youshyee/DCLDet" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ICMV22.jpg" width="118px" style="float: left; margin-right:19px;"/> C Zeng, X Yang, M Mirmehdi, AM Gambaruto, T Burghardt. <strong>Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation.</strong> <em>Proceedings SPIE 12701, 15th International Conference on Machine Vision (ICMV)</em>, November 2022. <b><i>(Best Oral Presentation Award!)</i></b>  (<a href="https://doi.org/10.1117/12.2679409" target="_blank">DOI:10.1117/12.2679409</a>), (<a href="https://arxiv.org/abs/2208.08315" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/SimonZeng7108/Video-TransUNet" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vaib22.jpg" width="118px" style="float: left; margin-right:19px;"/> M Stennett, DI Rubenstein, T Burghardt. <strong>Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning.</strong><em> 26th IEEE/IAPR International Conference on Pattern Recognition (ICPR) Workshop on Visual Observation and Analysis of Vertebrate And Insect Behavior (VAIB)</em>, August 2022. (<a href="https://homepages.inf.ed.ac.uk/rbf/VAIB22PAPERS/vaib22ms.pdf" target="_blank">ICPR Workshop PDF</a>), (<a href="https://arxiv.org/abs/2206.02261" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/Lm0079/grevys-zebra-individual-identification" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="mi2022.jpg" width="118px" style="float: left; margin-right:19px;"/> T Karaderi, T Burghardt, AY Hsiang, J Ramaer, DN Schmidt. <strong>Visual Microfossil Identification via Deep Metric Learning.</strong> <em>3rd International Conference on Pattern Recognition and Artificial Intelligence (ICPRAI), Lecture Notes in Computer Science (LNCS), Vol 13363, pp. 34-46, </em>June 2022. (<a href="https://doi.org/10.1007/978-3-031-09037-0_4" target="_blank">DOI:10.1007/978-3-031-09037-0_4</a>), (<a href="https://arxiv.org/abs/2112.09490" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/TayfunKaraderi/ICPRAI-2022-Visual-Microfossil-Identification-via-Deep-Metric-Learning" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> T Perrett, A Masullo, D Damen, T Burghardt, I Craddock, M Mirmehdi. <strong>Personalised Energy Expenditure Estimation: A Visual Sensing Approach With Deep Learning.</strong> <em>JMIR Formative Research</em>, Vol 6, No 9, Paper e33606, Sept 2022. (<a href="https://doi.org/10.2196/33606" target="_blank">DOI:10.2196/33606</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="lplf22.jpg" width="118px" style="float: left; margin-right:19px;"/> J Gao, T Burghardt, NW Campbell. <strong>Label a Herd in Minutes: Individual Holstein-Friesian Cattle Identification.</strong> <em>21st International Conference on Image Analysis and Processing Workshop (ICIAPW) on Learning in Precision Livestock Farming (LPLF), Lecture Notes in Computer Science (LNCS), Vol 13374, pp. 384-396, </em>May 2022. (<a href="https://doi.org/10.1007/978-3-031-13324-4_33" target="_blank">DOI:10.1007/978-3-031-13324-4_33</a>), (<a href="http://arxiv.org/abs/2204.10905" target="_blank">Arxiv PDF</a>), (<a href="https://doi.org/10.5523/bris.4vnrca7qw1642qlwxjadp87h7" target="_blank">Dataset Cows2021</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vaib2020a.jpg" width="118px" style="float: left; margin-right:19px;"/>O Brookes, SI Gray, P Bennett, KV Burgess, FE Clark, E Roberts, T Burghardt. <strong>Evaluating Cognitive Enrichment for Zoo-Housed Gorillas Using Facial Recognition.</strong> <em>Frontiers in Veterinary Science, Animal Behavior and Welfare</em>, Vol 9, May 2022. (<a href="https://doi.org/10.3389/fvets.2022.886720" target="_blank">DOI:10.3389/fvets.2022.886720</a>), (<a href="https://www.frontiersin.org/articles/10.3389/fvets.2022.886720" target="_blank">ISSN 2297-1769</a>), (<a href="https://github.com/obrookes/BristolGorillas2020" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.jf0859kboy8k2ufv60dqeb2t8" target="_blank">Dataset BristolGorillas2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="nc2022.jpg" width="118px" style="float: left; margin-right:19px;"/>D Tuia, B Kellenberger, S Beery, BR Costelloe, S Zuffi, B Risse, A Mathis, MW Mathis, F van Langevelde, T Burghardt, R Kays, H Klinck, M Wikelski, ID Couzin, G van Horn, MC Crofoot, CV Stewart, T Berger-Wolf. <strong>Perspectives in Machine Learning for Wildlife Conservation.</strong> <em>Nature Communications</em>, Vol 13, Issue 1, No 792. February 2022. <a href='https://www.nature.com/collections/dgiidcjfdh' target='_blank'><b><i>(Top 25 Life Sciences Article of the Year 2022)</i></b></a> (<a href="https://www.nature.com/articles/s41467-022-27980-y" target="_blank">DOI:10.1038/s41467-022-27980-y</a>), (<a href="https://arxiv.org/abs/2110.12951" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="snap22.jpg" width="118px" height="118px" style="float: left; margin-right:19px;"/> A Rutterford, L Bertini, EJ Hendy, KG Johnson, R Summerfield, T Burghardt. <strong>Towards the Analysis of Coral Skeletal Density-banding using Deep Learning.</strong> <em>Springer Nature Applied Sciences</em>, Vol 4, Issue 2, No 38, January 2022. (<a href="https://doi.org/10.1007/s42452-021-04912-x" target="_blank">DOI:10.1007/s42452-021-04912-x</a>), (<a href="https://static-content.springer.com/esm/art%3A10.1007%2Fs42452-021-04912-x/MediaObjects/42452_2021_4912_MOESM1_ESM.docx" target="_blank">Supplementary</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="wacvw22.jpg" width="118px" height="118px" style="float: left; margin-right:19px;"/> M Xue, T Greenslade, M Mirmehdi, T Burghardt. <strong>Small or Far Away? Exploiting Deep Super-Resolution and Altitude Data for Aerial Animal Surveillance.</strong> <em>Real-World Surveillance: Applications and Challenges Workshop (RWS) at IEEE Winter Conference on Applications of Computer Vision (WACVW)</em>, pp. 509-519, January 2022. (<a href="https://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Xue_Small_or_Far_Away_Exploiting_Deep_Super-Resolution_and_Altitude_Data_WACVW_2022_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/2111.06830" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/mowen111/salt" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc21.jpg" width="118px" height="80px" style="float: left; margin-right:19px;"/> X Yang, M Mirmehdi, T Burghardt. <strong>Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning.</strong> <em>Proc. 32nd British Machine Vision Conference (BMVC)</em>, 399, BMVA Press, November 2021. (<a href="https://www.bmvc2021-virtualconference.com/assets/papers/0399.pdf" target="_blank">BMVA Version</a>), (<a href="https://arxiv.org/abs/2010.07217" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/youshyee/CEP" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cvpr.jpg" width="118px" style="float: left; margin-right:19px;"/> T Perrett, A Masullo, T Burghardt, M Mirmehdi, D Damen. <strong>Temporal-Relational CrossTransformers for Few-Shot Action Recognition.</strong> <em>Proc. 34th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 475-484, June 2021. (<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Perrett_Temporal-Relational_CrossTransformers_for_Few-Shot_Action_Recognition_CVPR_2021_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/2101.06184v3" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/tobyperrett/trx" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cvprw.jpg" width="118px" style="float: left; margin-right:19px;"/> J Gao, T Burghardt, W Andrew, AW Dowsey, NW Campbell. <strong>Towards Self-Supervision for Video Identification of Individual Holstein-Friesian Cattle: The Cows2021 Dataset.</strong> <em>34th IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)</em>, June 2021. (<a href="https://arxiv.org/abs/2105.01938" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/Wormgit/Cows2021" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.4vnrca7qw1642qlwxjadp87h7" target="_blank">Dataset Cows2021</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cea.jpg" width="118px" style="float: left; margin-right:19px;"/> W Andrew, J Gao, S Mullan, N Campbell, AW Dowsey, T Burghardt. <strong>Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning.</strong> <em>Computers and Electronics in Agriculture</em>, Vol 185, June 2021. (<a href="https://doi.org/10.1016/j.compag.2021.106133" target="_blank">DOI:10.1016/j.compag.2021.106133</a>), (<a href="https://arxiv.org/abs/2006.09205" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/CWOA/MetricLearningIdentification" target="_blank">GitHub</a>), (<a href="https://data.bris.ac.uk/data/dataset/10m32xl88x2b61zlkkgz3fml17" target="_blank">Dataset OpenCows2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi. <strong>No Need for a Lab: Towards Multi-Sensory Fusion for Ambient Assisted Living in Real-World Living Homes.</strong> <em> 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP)</em>, February 2021. (<a href="https://www.doi.org/10.5220/0010202903280337" target="_blank">DOI:10.5220/0010202903280337</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vaib2020b.jpg" width="118px" style="float: left; margin-right:19px;"/> F Sakib, T Burghardt. <strong>Visual Recognition of Great Ape Behaviours in the Wild.</strong> <em>25th IEEE/IAPR International Conference on Pattern Recognition (ICPR) Workshop on Visual Observation and Analysis of Vertebrate And Insect Behavior (VAIB)</em>, January 2021. (<a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20fstb.pdf" target="_blank">ICPR Workshop PDF</a>), (<a href="mpi2019.txt" target="_blank">Dataset PanAfrican2019 Video</a>), (<a href="https://arxiv.org/abs/2011.10759" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/fznsakib/great-ape-behaviour-detector" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.jh6hrovynjik2ix2h7m6fdea3" target="_blank">Dataset PanAfrican2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vaib2020a.jpg" width="118px" style="float: left; margin-right:19px;"/> O Brookes, T Burghardt. <strong>A Dataset and Application for Facial Recognition of Individual Gorillas in Zoo Environments.</strong> <em>25th IEEE/IAPR International Conference on Pattern Recognition (ICPR) Workshop on Visual Observation and Analysis of Vertebrate And Insect Behavior (VAIB)</em>, January 2021. (<a href="http://homepages.inf.ed.ac.uk/rbf/VAIB20PAPERS/vaib20obtb.pdf" target="_blank">ICPR Workshop PDF</a>), (<a href="https://arxiv.org/abs/2012.04689" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/obrookes/BristolGorillas2020" target="_blank">GitHub</a>), (<a href="https://doi.org/10.5523/bris.jf0859kboy8k2ufv60dqeb2t8" target="_blank">Dataset BristolGorillas2020</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="accv.jpg" width="118px" style="float: left; margin-right:19px;"/> T Perrett, A Masullo, T Burghardt, M Mirmehdi, D Damen. <strong>Meta-Learning with Context-Agnostic Initialisations.</strong> <em>15th Asian Conference on Computer Vision (ACCV)</em>, Lecture Notes in Computer Science (LNCS), Vol 12625, pp. 70-86, November 2020. (<a href="https://doi.org/10.1007/978-3-030-69538-5_5" target="_blank">DOI:10.1007/978-3-030-69538-5_5</a>), (<a href="https://arxiv.org/abs/2007.14658" target="_blank">Arxiv PDF</a>), (<a href="https://openaccess.thecvf.com/content/ACCV2020/html/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.html" target="_blank">CVF Version</a>), (<a href="https://tobyperrett.github.io/contextagnosticweb" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sen2020.jpg" width="118px" style="float: left; margin-right:19px;"/> A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi. <strong>Person Re-ID by Fusion of Video Silhouettes and Wearable Signals for Home Monitoring Applications.</strong> <em> Sensors</em>, 20(9), 2576, May 2020. (<a href="http://dx.doi.org/10.3390/s20092576" target="_blank">DOI:10.3390/s20092576</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iros2019.jpg" width="118px" style="float: left; margin-right:19px;"/> W Andrew, C Greatwood, T Burghardt. <strong>Aerial Animal Biometrics: Individual Friesian Cattle Recovery and Visual Identification via an Autonomous UAV with Onboard Deep Inference.</strong><em> 32nd IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, pp. 237-243, November 2019. (<a href="http://dx.doi.org/10.1109/IROS40897.2019.8968555" target="_blank">DOI:10.1109/IROS40897.2019.8968555</a>), (<a href="https://arxiv.org/abs/1907.05310" target="_blank">Arxiv PDF</a>), (<a href="http://openaccess.thecvf.com/content_WACVW_2020/papers/w2/Andrew_Fusing_Animal_Biometrics_with_Autonomous_Robotics_Drone-based_Search_and_Individual_WACVW_2020_paper.pdf" target="_blank">CVF Extended Abstract at WACVW2020</a>), (<a href="https://vimeo.com/391197619" target="_blank">Video Summary</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="cvpm2019.jpg" width="118px" style="float: left; margin-right:19px;"/> A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi.<strong> Who Goes There? Exploiting Silhouettes and Wearable Signals for Subject Identification in Multi-Person Environments.</strong><em> 2nd International Workshop on Computer Vision for Physiological Measurement (CVPM) at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 1599-1607, October 2019. (<a href="http://dx.doi.org/10.1109/ICCVW.2019.00199" target="_blank">DOI:10.1109/ICCVW.2019.00199</a>), (<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVPM/Masullo_Who_Goes_There_Exploiting_Silhouettes_and_Wearable_Signals_for_Subject_ICCVW_2019_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iccv2019.jpg" width="118px" style="float: left; margin-right:19px;"/> X Yang, M Mirmehdi, T Burghardt.<strong> Great Ape Detection in Challenging Jungle Camera Trap Footage via Attention-Based Spatial and Temporal Feature Blending.</strong><em> Computer Vision for Wildlife Conservation (CVWC) Workshop at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 255-262, October 2019. (<a href="http://dx.doi.org/10.1109/ICCVW.2019.00034" target="_blank">DOI:10.1109/ICCVW.2019.00034</a>), (<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVWC/Yang_Great_Ape_Detection_in_Challenging_Jungle_Camera_Trap_Footage_via_ICCVW_2019_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/abs/1908.11240" target="_blank">Arxiv PDF</a>), (<a href="mpi2019.txt" target="_blank">Dataset PanAfrican2019 Video</a>), (<a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">Dataset PanAfrican2019 Annotations and Code</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iciap2019.jpg" width="118px" style="float: left; margin-right:19px;"/> V Ponce-Lopez, T Burghardt, Y Sun, S Hannuna, D Damen, M Mirmehdi.<strong> Deep Compact Person Re-Identification with Distractor Synthesis via Guided DC-GANs.</strong><em> 20th International Conference on Image Analysis and Processing (ICIAP)</em>,  pp. 488-498, September 2019. (<a href="http://dx.doi.org/10.1007/978-3-030-30642-7_44" target="_blank">DOI:10.1007/978-3-030-30642-7_44</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bits.jpg" width="118px" style="float: left; margin-right:19px;"/> J Green, T Burghardt, E Oswald.<strong> Not a Free Lunch but a Cheap Lunch: Experimental Results for Training Many Neural Nets.</strong><em> IACR Cryptology ePrint Archive</em>, paper 1068, September 2019. (<a href="https://eprint.iacr.org/2019/1068.pdf" target="_blank">ePrint PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iciar19.jpg" width="118px" style="float: left; margin-right:19px;"/> A Massulo, T Burghardt, T Perrett, D Damen, M Mirmehdi.<strong> Sit-to-Stand Analysis in the Wild using Silhouettes for Longitudinal Health Monitoring.</strong><em> 16th International Conference on Image Analysis and Recognition (ICIAR)</em>, pp. 175-185, August 2019.(<a href="http://dx.doi.org/10.1007/978-3-030-27272-2_15" target="_blank">DOI:10.1007/978-3-030-27272-2_15</a>), (<a href="https://arxiv.org/abs/1910.01370" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="gcpr.jpg" width="118px" style="float: left; margin-right:19px;"/> R Smith, T Burghardt.<strong> DeepKey: Towards End-to-End Physical Key Replication From a Single Photograph.</strong><em> 40th German Conference on Pattern Recognition (GCPR)</em>, pp. 487-502, October 2018. (<a href="https://doi.org/10.1007/978-3-030-12939-2_34" target="_blank">DOI:10.1007/978-3-030-12939-2_34</a>), (<a href="https://arxiv.org/pdf/1811.01405" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iros18.jpg" width="118px" style="float: left; margin-right:19px;"/> W Andrew, C Greatwood, T Burghardt.<strong> Deep Learning for Exploration and Recovery of Uncharted and Dynamic Targets from UAV-like Vision.</strong><em> 31st IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, pp. 1124-1131, October 2018. (<a href="https://doi.org/10.1109/IROS.2018.8593751" target="_blank">DOI:10.1109/IROS.2018.8593751</a>), (<a href="https://ieeexplore.ieee.org/document/8593751" target="_blank">IEEE Version</a>), (<a href="https://doi.org/10.5523/bris.2zot65rxlmgqq23au92qwkaa3x" target="_blank">Dataset GTRF2018</a>), (<a href="https://vimeo.com/280747562" target="_blank">Video Summary</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc18.jpg" width="118px" style="float: left; margin-right:19px;"/> A Massulo, T Burghardt, D Damen, S Hannuna, V Ponce-Lopez, M Mirmehdi.<strong> CaloriNet: From Silhouettes to Calorie Estimation in Private Environments.</strong><em> 29th British Machine Vision Conference (BMVC)</em>, 108, BMVA Press, September 2018. (<a href="http://bmvc2018.org/contents/papers/0383.pdf" target="_blank">BMVA Version</a>), (<a href="https://arxiv.org/pdf/1806.08152" target="_blank">Arxiv PDF</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>), (<a href="https://github.com/ale152/CaloriNet" target="_blank">GitHub</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="piceccv.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> V Ponce-Lopez, T Burghardt, S Hannuna, D Damen, A Masullo, M Mirmehdi.<strong> Semantically Selective Augmentation for Deep Compact Person Re-Identification.</strong><em> Person in Context (PIC) Workshop at European Conference of Computer Vision (ECCVW)</em>, pp. 551-561, September 2018. (<a href="https://doi.org/10.1007/978-3-030-11012-3_41" target="_blank">DOI:10.1007/978-3-030-11012-3_41</a>), (<a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Ponce-Lopez_Semantically_Selective_Augmentation_for_Deep_Compact_Person_Re-Identification_ECCVW_2018_paper.pdf" target="_blank">CVF Version</a>), (<a href="https://arxiv.org/pdf/1806.04074" target="_blank">Arxiv PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> A Elsts, T Burghardt, D Byrne, D Damen, X Fafoutis, S Hannuna, V Ponce-Lopez, A Masullo, M Mirmehdi, G Oikonomou, R Piechocki, E Tonkin, A Vafeas, P Woznowski, I Craddock.<strong> A Guide to the SPHERE 100 Homes Study Dataset.</strong><em> Technical Report</em>, May 2018. (<a href="https://arxiv.org/pdf/1805.11907.pdf" target="_blank">Arxiv PDF</a>), (<a href="https://www.irc-sphere.ac.uk/100-homes-sensors" target="_blank">Sensor Overview</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="caldnn.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> B Wang, L Tao, T Burghardt, M Mirmehdi.<strong> Calorific Expenditure Estimation using Deep Convolutional Network Features.</strong><em> Computer Vision for Active and Assisted Living Workshop (CV-AAL) at IEEE Winter Conference on Applications of Computer Vision (WACVW)</em>, March 2018. (<a href="https://doi.org/10.1109/WACVW.2018.00014" target="_blank">DOI:10.1109/WACVW.2018.00014</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ietsi.jpg" width="118px" style="float: left; margin-right:19px;"/> T Burghardt, RB Fisher, S Ravela (editors). <strong>Computer Vision for Animal Biometrics.</strong> <em> IET Computer Vision - Special Issue</em>, Vol 12, Issue 2, pp.119-120, March 2018. (<a href="http://dx.doi.org/10.1049/iet-cvi.2018.0019" target="_blank">DOI:10.1049/iet-cvi.2018.0019</a>), (<a href="http://digital-library.theiet.org/content/journals/iet-cvi/12/2" target="_blank">IET Online Version</a>), (<a href="http://digital-library.theiet.org/deliver/fulltext/iet-cvi/12/2/IET-CVI.2018.0019.pdf" target="_blank">Editorial PDF</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ietcalorie.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock.<strong> Energy Expenditure Estimation using Visual and Inertial Sensors.</strong><em> IET Computer Vision, Special Section: Computer Vision in Healthcare and Assisted Living</em>, Vol 12, Issue 1, pp. 36-47, February 2018. (<a href="http://dx.doi.org/10.1049/iet-cvi.2017.0112" target="_blank">DOI:10.1049/iet-cvi.2017.0112</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="iccvw.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> CA Brust, T Burghardt, M Groenenberg, C Kaeding, HS Kuehl, M Manguette, J Denzler.<strong> Towards Automated Visual Monitoring of Individual Gorillas in the Wild.</strong><em> Visual Wildlife Monitoring (VWM) Workshop at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 2820-2830, October 2017. (<a href="https://doi.org/10.1109/ICCVW.2017.333" target="_blank">DOI:10.1109/ICCVW.2017.333</a>), (<a href="gorilla2017.txt" target="_blank">Dataset Gorilla2017</a>), (<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w41/Brust_Towards_Automated_Visual_ICCV_2017_paper.pdf" target="_blank">CVF Version</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="vwm2017.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/>W Andrew, C Greatwood, T Burghardt.<strong> Visual Localisation and Individual Identification of Holstein Friesian Cattle via Deep Learning.</strong><em> Visual Wildlife Monitoring (VWM) Workshop at IEEE International Conference of Computer Vision (ICCVW),</em> pp. 2850-2859, October 2017. (<a href="https://doi.org/10.1109/ICCVW.2017.336" target="_blank">DOI:10.1109/ICCVW.2017.336</a>), (<a href="https://doi.org/10.5523/bris.2yizcfbkuv4352pzc32n54371r" target="_blank">Dataset FriesianCattle2017</a>), (<a href="https://doi.org/10.5523/bris.3owflku95bxsx24643cybxu3qh" target="_blank">Dataset AerialCattle2017</a>), (<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w41/Andrew_Visual_Localisation_and_ICCV_2017_paper.pdf" target="_blank">CVF Version</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="dagstuhl.jpg" width="118px" style="float: left; margin-right:19px;"/> G Camps-Valls, T Hickler, B Koenig-Ries (editors).<strong> Computer Science meets Ecology</strong> (Dagstuhl Seminar 17091). In: Dagstuhl Reports</em>, Vol 7, No 2, pp. 109-134, Leibniz-Zentrum fuer Informatik, September 2017. (<a href="http://dx.doi.org/10.4230/DagRep.7.2.109" target="_blank">DOI:10.4230/DagRep.7.2.109</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="kcf.jpg" width="118px" style="float: left; margin-right:19px;"/> M Camplani, A Paiment, M Mirmehdi, D Damen, S Hannuna, T Burghardt, L Tao.<strong> Multiple Human Tracking in RGB-D Data: A Survey.</strong> <em>IET Computer Vision.</em> Vol 11, No 4, pp. 265-285, ISSN: 1751-9632. June 2017. (<a href="http://dx.doi.org/10.1049/iet-cvi.2016.0178">DOI:10.1049/iet-cvi.2016.0178</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sharks.jpg" width="118px" style="float: left; margin-right:19px;"/> B Hughes, T Burghardt.<strong> Automated Visual Fin Identification of Individual Great White Sharks.</strong><em> International Journal of Computer Vision (IJCV)</em>, Vol 122, No 3, pp. 542-557, May 2017. (<a href="http://dx.doi.org/10.1007/s11263-016-0961-y" target="_blank">DOI:10.1007/s11263-016-0961-y</a>), (<a href="finscholl2456.txt" target="_blank">Dataset FinsScholl2456</a>)</td>
</tr><tr><td><hr/></td></tr>
 <tr>
   <td align="left" valign="top" width="100%" style=" text-align: justify;">
   <img src="ajp.jpg" width="118px" style="float: left; margin-right:19px;"/> AS Crunchant, M Egerer, A Loos, T Burghardt, K Zuberbuehler, K Corogenes, V Leinert, L Kulik, HS Kuehl.<strong> Automated Face Detection for Occurrence and Occupancy Estimation in Chimpanzees.</strong> <em>American Journal of Primatology.</em> Vol 79, Issue 3, ISSN: 1098-2345. March 2017. (<a href="http://dx.doi.org/10.1002/ajp.22627" target="_blank">DOI 10.1002/ajp.22627</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> PR Woznowski, A Burrows, T Diethe, X Fafoutis, J Hall, S Hannuna, M Camplani, N Twomey, M Kozlowski, B Tan, N Zhu, A Elsts, A Vafeas, A Paiement, L Tao, M Mirmehdi, T Burghardt, D Damen, P Flach, R Piechocki, I Craddock, G Oikonomou.<strong> SPHERE: A Sensor Platform for Healthcare in a Residential Environment.</strong> <em>Designing, Developing, and Facilitating Smart Cities: Urban Design to IoT Solutions.</em> (V Angelakis, E Tragos, HC Poehls, A Kapovits, A Bassi (eds.)), Springer, pp. 315-333, January 2017. (<a href="http://dx.doi.org/10.1007/978-3-319-44924-1_14">DOI:10.1007/978-3-319-44924-1_14</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="calorie.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock.<strong> Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at Home.</strong><em> Lecture Notes in Computer Science (LNCS)</em>, Vol 10116, pp. 239-251, November 2016. (<a href="https://doi.org/10.1007/978-3-319-54407-6">DOI:10.1007/978-3-319-54407-6</a>), (<a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset SPHERE-Calorie</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="kcf.jpg" width="118px" style="float: left; margin-right:19px;"/> S Hannuna, M Camplani, J Hall, M Mirmehdi, D Damen, T Burghardt, A Paiment, L Tao.<strong> DS-KCF: A real-time tracker for RGB-D data. </strong><em>Journal of Real-Time Image Processing</em>, Vol 16, No 5, pp. 1439-1458, November 2016. (<a href="http://dx.doi.org/10.1007/s11554-016-0654-3" target="_blank">DOI 10.1007/s11554-016-0654-3</a>), (<a href="https://doi.org/10.5523/bris.f8amc39n5f751ekgp2kbws1hb" target="_blank">Rotational Dataset</a>), (<a href="https://doi.org/10.5523/bris.1mddmf1o6f54d18zgr2jvahs8p" target="_blank">Code Download</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> J Hall, M Camplani, S Hannuna, M Mirmehdi, L Tao, D Damen, T Burghardt, A Paiment.<strong> Designing a Video Monitoring System for AAL applications: The SPHERE Case Study.</strong><em> IET International Conference on Technologies for Active and Assisted Living (TechAAL).</em> October 2016. (<a href="http://dx.doi.org/10.1049/ic.2016.0061" target="_blank">DOI:10.1049/ic.2016.0061</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, M Mirmehdi, D Damen, A Cooper, M Camplani, S Hannuna, A Paiment, I Craddock.<strong> Real-time Estimation of Physical Activity Intensity for Daily Living.</strong><em> IET International Conference on Technologies for Active and Assisted Living (TechAAL).</em> October 2016. (<a href="http://dx.doi.org/10.1049/ic.2016.0060" target="_blank">DOI:10.1049/ic.2016.0060</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="icip2016.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> W Andrew, S Hannuna, N Campbell, T Burghardt.<strong> Automatic Individual Holstein Friesian Cattle Identification via Selective Local Coat Pattern Matching in RGB-D Imagery.</strong> <em>IEEE International Conference on Image Processing (ICIP)</em>, pp. 484-488, ISBN: 978-1-4673-9961-6, September 2016. (<a href="http://dx.doi.org/10.1109/ICIP.2016.7532404" target="_blank">DOI:10.1109/ICIP.2016.7532404</a>), (<a href=" http://dx.doi.org/10.5523/bris.wurzq71kfm561ljahbwjhx9n3" target="_blank">Dataset FriesianCattle2015</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> L Tao, A Paiment, D Damen, M Mirmehdi, S Hannuna, M Camplani, T Burghardt, I Craddock.<strong> A Comparative Study of Pose Representation and Dynamics Modelling for Online Motion Quality Assessment.</strong> <em>Computer Vision and Image Understanding</em>, Vol 148, pp. 136-152, Elsevier, May 2016. (<a href="http://authors.elsevier.com/sd/article/S107731421500260X" target="_blank">DOI:10.1016/j.cviu.2015.11.016</a>)</td>
</tr><tr><td><hr/></td></tr>
  <tr>
    <td align="left" valign="top" width="100%" style=" text-align: justify;">
    <img src="ehealth2015.jpg" width="118px" height="100px" style="float: left; margin-right:19px;"/> L Tao, T Burghardt, S Hannuna, M Camplani, A Paiement, D Damen, M Mirmehdi, I Craddock.<strong> A Comparative Home Activity Monitoring Study using Visual and Inertial Sensors.</strong><em> IEEE 17th International Conference on eHealth Networking, Applications and Services. </em>pp. 644-647, October 2015. (<a href="https://dx.doi.org/10.1109/HealthCom.2015.7454583" target="_blank">DOI:10.1109/HealthCom.2015.7454583</a>), (<a target="_blank" href="https://doi.org/10.5523/bris.zs8naru512g01gn93wy76kp6q">Dataset SPHERE_H130</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="icip2015.jpg" width="118px" style="float: left; margin-right:19px;"/> D Gibson, T Burghardt, N Campbell, N Canagarajah. <strong>Towards Automating Visual In-Situ Monitoring of Crops Health</strong>. <em>IEEE International Conference on Image Processing (ICIP)</em>, pp. 3906 - 3910, September 2015. (<a href="http://dx.doi.org/10.1109/ICIP.2015.7351537" target="_blank">DOI:10.1109/ICIP.2015.7351537</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc2015.jpg" width="118px" style="float: left; margin-right:19px;"/> B Hughes, T Burghardt.<strong> Automated Identification of Individual Great White Sharks from Unrestricted Fin Imagery.</strong> <em>26th British Machine Vision Conference (BMVC)</em>, pp. 92.1-92.14, ISBN 1-901725-53-7, BMVA Press, September 2015. (<a href="https://dx.doi.org/10.5244/C.29.92" target="_blank">DOI:10.5244/C.29.92</a>), (<a href="http://www.bmva.org/bmvc/2015/papers/paper092/sup092.zip">Dataset FinsScholl2456</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="kcf.jpg" width="118px" style="float: left; margin-right:19px;"/> M Camplani, S Hannuna, D Damen, M Mirmehdi, A Paiement, T Burghardt, L Tao.<span id="ctl00_cph_SubmissionSummary_TitleText"><strong> Robust Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters</strong></span><strong>. </strong><em>26th British Machine Vision Conference (BMVC)</em>, pp. 145.1-145.11, ISBN 1-901725-53-7, BMVA Press, September 2015. (<a href="https://dx.doi.org/10.5244/C.29.145" target="_blank">DOI:10.5244/C.29.145</a>), (<a href="http://data.bris.ac.uk/data/dataset/16vbnj3im1ygi1sh0yd0mt4lp0">Code</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="mvab2015.jpg" width="118px" style="float: left; margin-right:19px;"/> B Hughes, T Burghardt.<strong> Affinity Matting for Pixel-accurate Fin Shape Recovery from Great White Shark Imagery.</strong> <em>Machine Vision of Animals and their Behaviour (MVAB), Workshop at BMVC</em>, pp. 8.1-8.8. BMVA Press, September 2015. (<a href="http://www.bmva.org/bmvc/2015/mvab/papers/paper008/index.html" target="_blank">DOI:10.5244/CW.29.MVAB.8</a>), (<a href="http://www.bmva.org/bmvc/2015/mvab/papers/paper008/sup008.zip">Dataset FinsScholl2456</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="ijcv.jpg" width="118px" style="float: left; margin-right:19px;"/> T Burghardt, D Damen, W Mayol-Cuevas, M Mirmehdi (editors). <strong>Correspondence, Matching and Recognition</strong>. <em>International Journal of Computer Vision (IJCV)</em>, Vol 113, Issue 3, pp. 161-162, ISSN 0920-5691, Springer, June 2015. (<a href="http://link.springer.com/article/10.1007/s11263-015-0827-8" target="_blank">DOI:10.1007/s11263-015-0827-8</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="sphere.jpg" width="118px" style="float: left; margin-right:19px;"/> P Woznowski, F Fafoutis, T Song, S Hannuna, M Camplani, L Tao, A Paiement, E Mellios, M Haghighi, N Zhu, G Hilton, D Damen, T Burghardt, M Mirmehdi, R Piechocki, D Kaleshi, I Craddock. <strong>A Multi-modal Sensor Infrastructure for Healthcare in Residential Environment</strong>. <em>IEEE ICC Workshop on ICT-enabled Services and Technologies for eHealth and Ambient Assisted Living (ICCW)</em>, pp. 271-277, June 2015. (<a href="http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?arnumber=7247190">DOI:10.1109/ICCW.2015.7247190</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="visapp2015.jpg" width="118px" style="float: left; margin-right:19px;"/> L Palmer, T Burghardt. <strong>Contextual Saliency for Nonrigid Landmark Registration and
    Recognition of Natural Patterns</strong>. <em>International Conference on Computer Vision Theory and Applications (VISAPP)</em>, pp. 403-410, ISBN: <span id="ContentPlaceHolder1_PublicationsDetailsPage_PublicationsDetailsContent_ISBN">978-989-758-089-5</span>, March 2015.
    (<a href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005268604030410" target="_blank">DOI:10.5220/0005268604030410</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="tree2013.jpg" width="118px" style="float: left; margin-right:19px;"/> HS Kuehl, T Burghardt. <strong>Fractal Representation and Recognition for Animal Biometrics: A Reply to Jovani et al</strong>. <em>Trends in Ecology and Evolution, </em>Vol 28, No 9, pp. 500-501, September 2013. (<a href="http://dx.doi.org/10.1016/j.tree.2013.06.007" target="_blank">DOI:10.1016/j.tree.2013.06.007</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvc2013.jpg" width="118px" style="float: left; margin-right:19px;"/> T Burghardt, D Damen, W Mayol-Cuevas, M Mirmehdi (editors). <strong>Proceedings of the 24th British Machine Vision Conference, BMVC2013</strong>. <em>British Machine Vision Association (BMVA)</em>, <a href="http://www.bmva.org/bmvc/2013/" target="_blank">ISBN 1-901725-49-9</a>, BMVA Press. September 2013. (Recorded Talks: <a href="http://videolectures.net/bmvc2013_bristol" target="_blank">VideoLectures.net</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="bmvw.jpg" width="118px" style="float: left; margin-right:19px;"/> RC Sandwell, A Loos, T Burghardt. <strong>Synthesising Unseen Image Conditions to Enhance Classification Accuracy for Sparse Datasets: Applied to Chimpanzee Face Recognition. </strong><em>British Machine Vision Workshop (BMVW)</em>, BMVA Press, September 2013. (BMVW: <a href="http://www.bmva.org/bmvc/2013/Workshop/0003.pdf" target="_blank">ISBN 1-901725-50-2</a>)</td>
</tr><tr><td><hr/></td></tr>
<tr>
  <td align="left" valign="top" width="100%" style=" text-align: justify;">
  <img src="tree.jpg" width="118px" style="float: left; margin-right:19px;"/> HS Kuehl, T Burghardt. <strong>Animal Biometrics: Quantifying and Detecting Phenotypic Appearance</strong>. <em>Trends in Ecology and Evolution</em>, Vol 28, No 7, pp. 432-441, July 2013.<br />
    (<a href="http://dx.doi.org/10.1016/j.tree.2013.02.013" target="_blank">DOI:10.1016/j.tree.2013.02.013</a>)</td>
</tr><tr><td><hr/></td></tr>
</table>
<div align="center">For older papers please see <a href='https://scholar.google.com/citations?user=lqM244QAAAAJ&hl=en' target='_blank' title='Go to Google Scholar Profile'>Google Scholar</a>.<br/><br/><hr/>
<br/>(Links: 
          <a href="https://www.bris.ac.uk/mystudents" target="_blank">MyS</a>, 
          <a href="http://evision.apps.bris.ac.uk" target="_blank">eV</a>, 
          <a href="http://wwwa.fen.bris.ac.uk/2014-5/coms" target="_blank">S</a>, 
          <a href="https://www.ole.bris.ac.uk/webapps/bb-auth-provider-cas-bb_bb60/execute/casLogin?cmd=login&authProviderId=_122_1&redirectUrl=https%3A%2F%2Fwww.ole.bris.ac.uk">BB</a>, 
          <a href="http://www.bristol.ac.uk/timetables" target="_blank">TT</a>, 
          <a href="https://research-information.bris.ac.uk/pure" target="_blank">Pu</a>, 
          <a href="http://www.bristol.ac.uk/fec" target="_blank">fEC</a>, 
          <a href="http://encrypted.google.com" target="_blank">G</a>,
          <a href="https://uob.sharepoint.com/sites/staff" target="_blank">I</a>,
          <a href="https://uob.sharepoint.com/sites/engineering" target="_blank">E</a>, 
          <a href="https://webcentre.askadmissions.co.uk" target="_blank">A</a>)<br/><hr/><br/>

<br/><b>Website Terms of Use</b><hr/><br/>
<table id="pubstab" width="100%" border="0" cellspacing="0" style="text-align: justify; margin-top: -24px; font-family: 'Helvetica', sans-serif; font-size:12pt;" >
<tr><td>See <a href='https://www.bristol.ac.uk/style-guides/web/policies/legal/terms' target='_blank'>University of Bristol Website Terms of Use</a></td></tr>
</table><br/><br/>
</div>
</div>
<script> myResize(); myResize(); myResize();</script>
</body>
</html>
